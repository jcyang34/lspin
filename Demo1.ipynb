{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "import os\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm,colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from model import convertToOneHot, DataSet_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear synthetic data generation\n",
    "\n",
    "Group 1: $X$ ~ $N(1,0.5)$,  $Y = -2X_1 + X_2 - 0.5X_3$\n",
    "\n",
    "Group 2: $X$ ~ $N(-1,0.5)$, $Y = -0.5X_3 + X_4 - 2X_5$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(34)\n",
    "\n",
    "Xs1 = np.random.normal(loc=1,scale=0.5,size=(300,5))\n",
    "Ys1 = -2*Xs1[:,0]+1*Xs1[:,1]-0.5*Xs1[:,2]\n",
    "\n",
    "Xs2 = np.random.normal(loc=-1,scale=0.5,size=(300,5))\n",
    "Ys2 = -0.5*Xs2[:,2]+1*Xs2[:,3]-2*Xs2[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.concatenate((Xs1,Xs2),axis=0)\n",
    "Y_data = np.concatenate((Ys1.reshape(-1,1),Ys2.reshape(-1,1)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data = Y_data-Y_data.min()\n",
    "Y_data=Y_data/Y_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ground truth group label of each sample\n",
    "case_labels = np.concatenate((np.array([1]*300),np.array([2]*300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data = np.concatenate((Y_data,case_labels.reshape(-1,1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% for validation, 10% for test \n",
    "X_train,X_remain,yc_train,yc_remain = train_test_split(X_data,Y_data,train_size=0.8,shuffle=True,random_state=34)\n",
    "X_valid,X_test,yc_valid,yc_test = train_test_split(X_remain,yc_remain,train_size=0.5,shuffle=True,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 10 samples used for training\n",
    "X_train,_,yc_train,_ = train_test_split(X_train,yc_train,train_size=10,shuffle=True,random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sizes:\n",
      "10 60 60\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample sizes:\")\n",
    "print(X_train.shape[0],X_valid.shape[0],X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = yc_train[:,0].reshape(-1,1)\n",
    "y_valid = yc_valid[:,0].reshape(-1,1)\n",
    "y_test = yc_test[:,0].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = yc_train[:,1]\n",
    "valid_label = yc_valid[:,1]\n",
    "test_label= yc_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2.0: 6, 1.0: 4})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2.0: 29, 1.0: 31})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataSet_meta(**{'_data':X_train, '_labels':y_train,'_meta':y_train,\n",
    "                '_valid_data':X_valid, '_valid_labels':y_valid,'_valid_meta':y_valid,\n",
    "                '_test_data':X_test, '_test_labels':y_test,'_test_meta':y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference ground truth feature matrix (training/test)\n",
    "ref_feat_mat_train = np.array([[1,1,1,0,0] if label == 1 else [0,0,1,1,1] for label in train_label])\n",
    "ref_feat_mat_test = np.array([[1,1,1,0,0] if label == 1 else [0,0,1,1,1] for label in test_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LLSPIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter specification\n",
    "model_params = {     \n",
    "    \"input_node\" : X_train.shape[1],       # input dimension for the prediction network\n",
    "    \"hidden_layers_node\" : [100,100,10,1], # number of nodes for each hidden layer of the prediction net\n",
    "    \"output_node\" : 1,                     # number of nodes for the output layer of the prediction net\n",
    "    \"feature_selection\" : True,            # if using the gating net\n",
    "    \"gating_net_hidden_layers_node\": [10], # number of nodes for each hidden layer of the gating net\n",
    "    \"display_step\" : 500                   # number of epochs to output info\n",
    "}\n",
    "model_params['activation_pred']= 'none' # linear prediction\n",
    "model_params['activation_gating'] = 'tanh'\n",
    "\n",
    "model_params['batch_normalization'] = 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'batch_size':X_train.shape[0]\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective function for optuna hyper-parameter optimization\n",
    "def llspin_objective(trial):  \n",
    "    global model\n",
    "    \n",
    "    # hyper-parameter to optimize: lambda, learning rate, number of epochs\n",
    "    model_params['lam'] = trial.suggest_loguniform('lam',1e-3,1e-2)\n",
    "    training_params['lr'] = trial.suggest_loguniform('learning_rate', 1e-2, 2e-1)\n",
    "    training_params['num_epoch'] = trial.suggest_categorical('num_epoch', [2000,5000,10000,15000])\n",
    "\n",
    "    # specify the model with these parameters and train the model\n",
    "    model = Model(**model_params)\n",
    "    train_acces, train_losses, val_acces, val_losses = model.train(dataset=dataset,**training_params)\n",
    "\n",
    "    print(\"In trial:---------------------\")\n",
    "    val_prediction = model.test(X_valid)[0]\n",
    "    mse = mean_squared_error(y_valid.reshape(-1),val_prediction.reshape(-1))\n",
    "    print(\"validation mse: {}\".format(mse))\n",
    "    \n",
    "    loss= mse\n",
    "            \n",
    "    return loss\n",
    "        \n",
    "def callback(study,trial):\n",
    "    global best_model\n",
    "    if study.best_trial == trial:\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:20:20,861]\u001b[0m A new study created in memory with name: no-name-231f7775-9e79-4436-8604-34bb9f5953d2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.019199613 valid loss= 0.014349099\n",
      "Epoch: 1000 train loss=0.018299948 valid loss= 0.013251478\n",
      "Epoch: 1500 train loss=0.018981043 valid loss= 0.012919768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:20:41,968]\u001b[0m Trial 0 finished with value: 0.005639222691545177 and parameters: {'lam': 0.00976368643084559, 'learning_rate': 0.0320124089168281, 'num_epoch': 2000}. Best is trial 0 with value: 0.005639222691545177.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 train loss=0.024905063 valid loss= 0.013935876\n",
      "Optimization Finished!\n",
      "test loss: 0.01478402130305767, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.005639222691545177\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.024651777 valid loss= 0.008461524\n",
      "Epoch: 1000 train loss=0.007127225 valid loss= 0.008032663\n",
      "Epoch: 1500 train loss=0.006645962 valid loss= 0.007861067\n",
      "Epoch: 2000 train loss=0.013712140 valid loss= 0.007685911\n",
      "Epoch: 2500 train loss=0.013006258 valid loss= 0.007303611\n",
      "Epoch: 3000 train loss=0.008046509 valid loss= 0.007826992\n",
      "Epoch: 3500 train loss=0.009211547 valid loss= 0.006861527\n",
      "Epoch: 4000 train loss=0.004579989 valid loss= 0.006721389\n",
      "Epoch: 4500 train loss=0.005859715 valid loss= 0.006620081\n",
      "Epoch: 5000 train loss=0.004934237 valid loss= 0.006428248\n",
      "Epoch: 5500 train loss=0.003499446 valid loss= 0.006333012\n",
      "Epoch: 6000 train loss=0.005556284 valid loss= 0.006069766\n",
      "Epoch: 6500 train loss=0.005926464 valid loss= 0.005879606\n",
      "Epoch: 7000 train loss=0.007505193 valid loss= 0.005638471\n",
      "Epoch: 7500 train loss=0.005147415 valid loss= 0.005353034\n",
      "Epoch: 8000 train loss=0.005505805 valid loss= 0.005313960\n",
      "Epoch: 8500 train loss=0.005765205 valid loss= 0.005195373\n",
      "Epoch: 9000 train loss=0.005784405 valid loss= 0.005066554\n",
      "Epoch: 9500 train loss=0.003552839 valid loss= 0.005628864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:22:19,917]\u001b[0m Trial 1 finished with value: 0.003529721426480185 and parameters: {'lam': 0.0018503755211600663, 'learning_rate': 0.026064922715766485, 'num_epoch': 10000}. Best is trial 1 with value: 0.003529721426480185.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10000 train loss=0.003265945 valid loss= 0.005195721\n",
      "Optimization Finished!\n",
      "test loss: 0.005266119260340929, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.003529721426480185\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.016604269 valid loss= 0.008435335\n",
      "Epoch: 1000 train loss=0.010373626 valid loss= 0.007585984\n",
      "Epoch: 1500 train loss=0.008601937 valid loss= 0.007588248\n",
      "Epoch: 2000 train loss=0.008194674 valid loss= 0.007450633\n",
      "Epoch: 2500 train loss=0.007638324 valid loss= 0.007638639\n",
      "Epoch: 3000 train loss=0.008222405 valid loss= 0.007040053\n",
      "Epoch: 3500 train loss=0.009674870 valid loss= 0.006922674\n",
      "Epoch: 4000 train loss=0.005358696 valid loss= 0.007328531\n",
      "Epoch: 4500 train loss=0.009904248 valid loss= 0.006832212\n",
      "Epoch: 5000 train loss=0.004833036 valid loss= 0.006587625\n",
      "Epoch: 5500 train loss=0.005995873 valid loss= 0.005847696\n",
      "Epoch: 6000 train loss=0.005611592 valid loss= 0.006292931\n",
      "Epoch: 6500 train loss=0.007397601 valid loss= 0.005366785\n",
      "Epoch: 7000 train loss=0.006683512 valid loss= 0.005250023\n",
      "Epoch: 7500 train loss=0.005528262 valid loss= 0.005598771\n",
      "Epoch: 8000 train loss=0.005553978 valid loss= 0.004722361\n",
      "Epoch: 8500 train loss=0.005779709 valid loss= 0.004845795\n",
      "Epoch: 9000 train loss=0.002525348 valid loss= 0.004228726\n",
      "Epoch: 9500 train loss=0.006087701 valid loss= 0.004040804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:23:56,427]\u001b[0m Trial 2 finished with value: 0.002505764457827466 and parameters: {'lam': 0.0015878770588951567, 'learning_rate': 0.02799719973388292, 'num_epoch': 10000}. Best is trial 2 with value: 0.002505764457827466.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10000 train loss=0.004826409 valid loss= 0.003782789\n",
      "Optimization Finished!\n",
      "test loss: 0.003950268495827913, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002505764457827466\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.015926231 valid loss= 0.013816646\n",
      "Epoch: 1000 train loss=0.012191494 valid loss= 0.010602374\n",
      "Epoch: 1500 train loss=0.014433151 valid loss= 0.010391161\n",
      "Epoch: 2000 train loss=0.008959000 valid loss= 0.010296751\n",
      "Epoch: 2500 train loss=0.009011566 valid loss= 0.010846651\n",
      "Epoch: 3000 train loss=0.008208347 valid loss= 0.010457069\n",
      "Epoch: 3500 train loss=0.007084726 valid loss= 0.010091584\n",
      "Epoch: 4000 train loss=0.012326405 valid loss= 0.009947291\n",
      "Epoch: 4500 train loss=0.008750997 valid loss= 0.010000524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:24:46,299]\u001b[0m Trial 3 finished with value: 0.004325573666880858 and parameters: {'lam': 0.009399983493436131, 'learning_rate': 0.11872534816754728, 'num_epoch': 5000}. Best is trial 2 with value: 0.002505764457827466.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.007929506 valid loss= 0.010068219\n",
      "Optimization Finished!\n",
      "test loss: 0.010199256241321564, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.004325573666880858\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.018594807 valid loss= 0.009608831\n",
      "Epoch: 1000 train loss=0.011996957 valid loss= 0.008956482\n",
      "Epoch: 1500 train loss=0.018249685 valid loss= 0.008811019\n",
      "Epoch: 2000 train loss=0.016814891 valid loss= 0.008528588\n",
      "Epoch: 2500 train loss=0.010693766 valid loss= 0.008615874\n",
      "Epoch: 3000 train loss=0.015547372 valid loss= 0.007839699\n",
      "Epoch: 3500 train loss=0.008855030 valid loss= 0.008493962\n",
      "Epoch: 4000 train loss=0.008670651 valid loss= 0.007480389\n",
      "Epoch: 4500 train loss=0.005281859 valid loss= 0.008418596\n",
      "Epoch: 5000 train loss=0.013199024 valid loss= 0.008433678\n",
      "Epoch: 5500 train loss=0.010321228 valid loss= 0.007744010\n",
      "Epoch: 6000 train loss=0.007995393 valid loss= 0.007409426\n",
      "Epoch: 6500 train loss=0.010843451 valid loss= 0.006654294\n",
      "Epoch: 7000 train loss=0.006628444 valid loss= 0.006546571\n",
      "Epoch: 7500 train loss=0.007961747 valid loss= 0.006444152\n",
      "Epoch: 8000 train loss=0.005589850 valid loss= 0.005984173\n",
      "Epoch: 8500 train loss=0.009229124 valid loss= 0.006146994\n",
      "Epoch: 9000 train loss=0.005036855 valid loss= 0.005610847\n",
      "Epoch: 9500 train loss=0.006355981 valid loss= 0.005401490\n",
      "Epoch: 10000 train loss=0.005890930 valid loss= 0.005390029\n",
      "Epoch: 10500 train loss=0.003383556 valid loss= 0.005110395\n",
      "Epoch: 11000 train loss=0.005776521 valid loss= 0.005154520\n",
      "Epoch: 11500 train loss=0.006353054 valid loss= 0.004904062\n",
      "Epoch: 12000 train loss=0.004075391 valid loss= 0.005193052\n",
      "Epoch: 12500 train loss=0.003257517 valid loss= 0.005128182\n",
      "Epoch: 13000 train loss=0.009209315 valid loss= 0.005172186\n",
      "Epoch: 13500 train loss=0.004589884 valid loss= 0.005373367\n",
      "Epoch: 14000 train loss=0.005272793 valid loss= 0.005021737\n",
      "Epoch: 14500 train loss=0.004643691 valid loss= 0.005429887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:27:12,411]\u001b[0m Trial 4 finished with value: 0.0038582939170120896 and parameters: {'lam': 0.002280414670384256, 'learning_rate': 0.019835751546484506, 'num_epoch': 15000}. Best is trial 2 with value: 0.002505764457827466.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.003547115 valid loss= 0.005548670\n",
      "Optimization Finished!\n",
      "test loss: 0.005675830878317356, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0038582939170120896\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.026117910 valid loss= 0.014746122\n",
      "Epoch: 1000 train loss=0.012349450 valid loss= 0.014890019\n",
      "Epoch: 1500 train loss=0.011446623 valid loss= 0.012851140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:27:33,867]\u001b[0m Trial 5 finished with value: 0.003920789104563355 and parameters: {'lam': 0.008761010834960906, 'learning_rate': 0.0640420520500171, 'num_epoch': 2000}. Best is trial 2 with value: 0.002505764457827466.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 train loss=0.010681633 valid loss= 0.010645153\n",
      "Optimization Finished!\n",
      "test loss: 0.010874215513467789, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.003920789104563355\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.015547818 valid loss= 0.010715184\n",
      "Epoch: 1000 train loss=0.013691332 valid loss= 0.010132811\n",
      "Epoch: 1500 train loss=0.015674392 valid loss= 0.010044904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:27:55,210]\u001b[0m Trial 6 finished with value: 0.0068103231666390525 and parameters: {'lam': 0.003552463032161061, 'learning_rate': 0.02272728124992357, 'num_epoch': 2000}. Best is trial 2 with value: 0.002505764457827466.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 train loss=0.012709934 valid loss= 0.009930237\n",
      "Optimization Finished!\n",
      "test loss: 0.012165794149041176, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0068103231666390525\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.011754628 valid loss= 0.008300214\n",
      "Epoch: 1000 train loss=0.009094238 valid loss= 0.007877668\n",
      "Epoch: 1500 train loss=0.008705718 valid loss= 0.007849361\n",
      "Epoch: 2000 train loss=0.014591634 valid loss= 0.008352211\n",
      "Epoch: 2500 train loss=0.011238880 valid loss= 0.008027209\n",
      "Epoch: 3000 train loss=0.007111190 valid loss= 0.008149890\n",
      "Epoch: 3500 train loss=0.006258218 valid loss= 0.007758787\n",
      "Epoch: 4000 train loss=0.006375549 valid loss= 0.007407881\n",
      "Epoch: 4500 train loss=0.013159290 valid loss= 0.007895548\n",
      "Epoch: 5000 train loss=0.006855285 valid loss= 0.007696294\n",
      "Epoch: 5500 train loss=0.008253125 valid loss= 0.007295419\n",
      "Epoch: 6000 train loss=0.005449088 valid loss= 0.008061192\n",
      "Epoch: 6500 train loss=0.008655818 valid loss= 0.007994399\n",
      "Epoch: 7000 train loss=0.011350315 valid loss= 0.007486699\n",
      "Epoch: 7500 train loss=0.008686691 valid loss= 0.007641760\n",
      "Epoch: 8000 train loss=0.011027509 valid loss= 0.008324340\n",
      "Epoch: 8500 train loss=0.003758863 valid loss= 0.007345634\n",
      "Epoch: 9000 train loss=0.006031770 valid loss= 0.007569117\n",
      "Epoch: 9500 train loss=0.008474304 valid loss= 0.007500649\n",
      "Epoch: 10000 train loss=0.008306828 valid loss= 0.007581821\n",
      "Epoch: 10500 train loss=0.008368483 valid loss= 0.007316167\n",
      "Epoch: 11000 train loss=0.004611341 valid loss= 0.007453029\n",
      "Epoch: 11500 train loss=0.008322757 valid loss= 0.008258383\n",
      "Epoch: 12000 train loss=0.007875623 valid loss= 0.007306106\n",
      "Epoch: 12500 train loss=0.006736200 valid loss= 0.007532609\n",
      "Epoch: 13000 train loss=0.009433013 valid loss= 0.007475778\n",
      "Epoch: 13500 train loss=0.007768608 valid loss= 0.007850101\n",
      "Epoch: 14000 train loss=0.006745242 valid loss= 0.007314491\n",
      "Epoch: 14500 train loss=0.003859948 valid loss= 0.007165956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:30:19,892]\u001b[0m Trial 7 finished with value: 0.005736672047566734 and parameters: {'lam': 0.0014336496195466997, 'learning_rate': 0.012352657127979932, 'num_epoch': 15000}. Best is trial 2 with value: 0.002505764457827466.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.007626234 valid loss= 0.007006505\n",
      "Optimization Finished!\n",
      "test loss: 0.006598735228180885, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.005736672047566734\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.025691304 valid loss= 0.015167283\n",
      "Epoch: 1000 train loss=0.023994733 valid loss= 0.014489707\n",
      "Epoch: 1500 train loss=0.020816974 valid loss= 0.013888052\n",
      "Epoch: 2000 train loss=0.018975846 valid loss= 0.013462206\n",
      "Epoch: 2500 train loss=0.018224929 valid loss= 0.012480568\n",
      "Epoch: 3000 train loss=0.011714961 valid loss= 0.010896111\n",
      "Epoch: 3500 train loss=0.009394361 valid loss= 0.010122527\n",
      "Epoch: 4000 train loss=0.011147059 valid loss= 0.009842014\n",
      "Epoch: 4500 train loss=0.014282737 valid loss= 0.009682556\n",
      "Epoch: 5000 train loss=0.009056992 valid loss= 0.009790011\n",
      "Epoch: 5500 train loss=0.014135309 valid loss= 0.009335763\n",
      "Epoch: 6000 train loss=0.015748007 valid loss= 0.009277957\n",
      "Epoch: 6500 train loss=0.011795518 valid loss= 0.009181453\n",
      "Epoch: 7000 train loss=0.009164179 valid loss= 0.009496996\n",
      "Epoch: 7500 train loss=0.011861972 valid loss= 0.009025559\n",
      "Epoch: 8000 train loss=0.007246458 valid loss= 0.008767350\n",
      "Epoch: 8500 train loss=0.009650003 valid loss= 0.008906325\n",
      "Epoch: 9000 train loss=0.011117465 valid loss= 0.008904878\n",
      "Epoch: 9500 train loss=0.008657904 valid loss= 0.008592912\n",
      "Epoch: 10000 train loss=0.010657670 valid loss= 0.008621311\n",
      "Epoch: 10500 train loss=0.006730761 valid loss= 0.008291718\n",
      "Epoch: 11000 train loss=0.007218333 valid loss= 0.008488651\n",
      "Epoch: 11500 train loss=0.008133594 valid loss= 0.008285468\n",
      "Epoch: 12000 train loss=0.011093821 valid loss= 0.008095587\n",
      "Epoch: 12500 train loss=0.009423564 valid loss= 0.007698706\n",
      "Epoch: 13000 train loss=0.006763084 valid loss= 0.007815615\n",
      "Epoch: 13500 train loss=0.009071655 valid loss= 0.007803879\n",
      "Epoch: 14000 train loss=0.008102728 valid loss= 0.007697104\n",
      "Epoch: 14500 train loss=0.011049752 valid loss= 0.007835131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:32:47,308]\u001b[0m Trial 8 finished with value: 0.0020552688292528884 and parameters: {'lam': 0.009487439861077389, 'learning_rate': 0.030995029182938123, 'num_epoch': 15000}. Best is trial 8 with value: 0.0020552688292528884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.007475052 valid loss= 0.007784557\n",
      "Optimization Finished!\n",
      "test loss: 0.0074676829390227795, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0020552688292528884\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.016302435 valid loss= 0.008237323\n",
      "Epoch: 1000 train loss=0.014989380 valid loss= 0.007706359\n",
      "Epoch: 1500 train loss=0.008566581 valid loss= 0.007512718\n",
      "Epoch: 2000 train loss=0.004233418 valid loss= 0.007124386\n",
      "Epoch: 2500 train loss=0.013569580 valid loss= 0.005970894\n",
      "Epoch: 3000 train loss=0.006678679 valid loss= 0.006002773\n",
      "Epoch: 3500 train loss=0.009474367 valid loss= 0.005645868\n",
      "Epoch: 4000 train loss=0.002310268 valid loss= 0.005600758\n",
      "Epoch: 4500 train loss=0.008961802 valid loss= 0.004606818\n",
      "Epoch: 5000 train loss=0.007016646 valid loss= 0.004273815\n",
      "Epoch: 5500 train loss=0.012681609 valid loss= 0.003818060\n",
      "Epoch: 6000 train loss=0.008963226 valid loss= 0.003770268\n",
      "Epoch: 6500 train loss=0.003469034 valid loss= 0.003726908\n",
      "Epoch: 7000 train loss=0.007265555 valid loss= 0.003941303\n",
      "Epoch: 7500 train loss=0.003438444 valid loss= 0.003946623\n",
      "Epoch: 8000 train loss=0.004466892 valid loss= 0.003824615\n",
      "Epoch: 8500 train loss=0.008902391 valid loss= 0.003638092\n",
      "Epoch: 9000 train loss=0.002547054 valid loss= 0.003822710\n",
      "Epoch: 9500 train loss=0.005362682 valid loss= 0.003807251\n",
      "Epoch: 10000 train loss=0.005362948 valid loss= 0.003932363\n",
      "Epoch: 10500 train loss=0.010192009 valid loss= 0.003954636\n",
      "Epoch: 11000 train loss=0.006094007 valid loss= 0.003884389\n",
      "Epoch: 11500 train loss=0.003912819 valid loss= 0.004134040\n",
      "Epoch: 12000 train loss=0.003670396 valid loss= 0.003935666\n",
      "Epoch: 12500 train loss=0.004449332 valid loss= 0.004069644\n",
      "Epoch: 13000 train loss=0.004578985 valid loss= 0.004073091\n",
      "Epoch: 13500 train loss=0.006198436 valid loss= 0.004082050\n",
      "Epoch: 14000 train loss=0.004817853 valid loss= 0.004194776\n",
      "Epoch: 14500 train loss=0.003452317 valid loss= 0.004002415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:35:10,766]\u001b[0m Trial 9 finished with value: 0.0028008236305818914 and parameters: {'lam': 0.001553727200218501, 'learning_rate': 0.024335435426449954, 'num_epoch': 15000}. Best is trial 8 with value: 0.0020552688292528884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.003879391 valid loss= 0.003984013\n",
      "Optimization Finished!\n",
      "test loss: 0.004499207716435194, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0028008236305818914\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.012384645 valid loss= 0.011740530\n",
      "Epoch: 1000 train loss=0.011251997 valid loss= 0.012361117\n",
      "Epoch: 1500 train loss=0.015822753 valid loss= 0.012495501\n",
      "Epoch: 2000 train loss=0.009927279 valid loss= 0.012896901\n",
      "Epoch: 2500 train loss=0.012879648 valid loss= 0.011836415\n",
      "Epoch: 3000 train loss=0.006140682 valid loss= 0.011474397\n",
      "Epoch: 3500 train loss=0.010885141 valid loss= 0.010400383\n",
      "Epoch: 4000 train loss=0.009752354 valid loss= 0.010728915\n",
      "Epoch: 4500 train loss=0.005989781 valid loss= 0.009495259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:36:00,324]\u001b[0m Trial 10 finished with value: 0.006025218616318714 and parameters: {'lam': 0.005311215554357117, 'learning_rate': 0.05736376209267282, 'num_epoch': 5000}. Best is trial 8 with value: 0.0020552688292528884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.009454729 valid loss= 0.010336101\n",
      "Optimization Finished!\n",
      "test loss: 0.009951401501893997, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.006025218616318714\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.018473174 valid loss= 0.007923791\n",
      "Epoch: 1000 train loss=0.017470745 valid loss= 0.007530995\n",
      "Epoch: 1500 train loss=0.011854138 valid loss= 0.007497827\n",
      "Epoch: 2000 train loss=0.024091605 valid loss= 0.007364745\n",
      "Epoch: 2500 train loss=0.017291160 valid loss= 0.007203855\n",
      "Epoch: 3000 train loss=0.012302106 valid loss= 0.006916594\n",
      "Epoch: 3500 train loss=0.009726957 valid loss= 0.006820249\n",
      "Epoch: 4000 train loss=0.021045690 valid loss= 0.006774708\n",
      "Epoch: 4500 train loss=0.012584156 valid loss= 0.006711405\n",
      "Epoch: 5000 train loss=0.009977190 valid loss= 0.006605555\n",
      "Epoch: 5500 train loss=0.008489206 valid loss= 0.006416754\n",
      "Epoch: 6000 train loss=0.019152135 valid loss= 0.006352305\n",
      "Epoch: 6500 train loss=0.009704054 valid loss= 0.006728464\n",
      "Epoch: 7000 train loss=0.004659695 valid loss= 0.006335857\n",
      "Epoch: 7500 train loss=0.006568822 valid loss= 0.006587976\n",
      "Epoch: 8000 train loss=0.012185957 valid loss= 0.006385969\n",
      "Epoch: 8500 train loss=0.006214219 valid loss= 0.006287167\n",
      "Epoch: 9000 train loss=0.009924571 valid loss= 0.006038283\n",
      "Epoch: 9500 train loss=0.004488992 valid loss= 0.005912863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:37:38,538]\u001b[0m Trial 11 finished with value: 0.005016395722700979 and parameters: {'lam': 0.0011391091582322957, 'learning_rate': 0.010040556322182259, 'num_epoch': 10000}. Best is trial 8 with value: 0.0020552688292528884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10000 train loss=0.006740719 valid loss= 0.006012139\n",
      "Optimization Finished!\n",
      "test loss: 0.00624422449618578, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.005016395722700979\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.013805656 valid loss= 0.008572465\n",
      "Epoch: 1000 train loss=0.016447563 valid loss= 0.007634606\n",
      "Epoch: 1500 train loss=0.007146989 valid loss= 0.007118715\n",
      "Epoch: 2000 train loss=0.014100665 valid loss= 0.005590839\n",
      "Epoch: 2500 train loss=0.008677365 valid loss= 0.005250604\n",
      "Epoch: 3000 train loss=0.004124018 valid loss= 0.005167339\n",
      "Epoch: 3500 train loss=0.003466857 valid loss= 0.005600278\n",
      "Epoch: 4000 train loss=0.003585803 valid loss= 0.005268050\n",
      "Epoch: 4500 train loss=0.007784759 valid loss= 0.005058754\n",
      "Epoch: 5000 train loss=0.004249450 valid loss= 0.005216352\n",
      "Epoch: 5500 train loss=0.005377300 valid loss= 0.004969540\n",
      "Epoch: 6000 train loss=0.007855222 valid loss= 0.004846795\n",
      "Epoch: 6500 train loss=0.004097870 valid loss= 0.005001709\n",
      "Epoch: 7000 train loss=0.007282553 valid loss= 0.004900427\n",
      "Epoch: 7500 train loss=0.003260211 valid loss= 0.004914553\n",
      "Epoch: 8000 train loss=0.003817789 valid loss= 0.004920641\n",
      "Epoch: 8500 train loss=0.003090451 valid loss= 0.004839409\n",
      "Epoch: 9000 train loss=0.003496035 valid loss= 0.004683612\n",
      "Epoch: 9500 train loss=0.003693318 valid loss= 0.004608179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:39:17,375]\u001b[0m Trial 12 finished with value: 0.0027598389851334498 and parameters: {'lam': 0.003163453442066962, 'learning_rate': 0.038752034357938885, 'num_epoch': 10000}. Best is trial 8 with value: 0.0020552688292528884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10000 train loss=0.007706853 valid loss= 0.004857872\n",
      "Optimization Finished!\n",
      "test loss: 0.004580171313136816, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0027598389851334498\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.014119195 valid loss= 0.011743631\n",
      "Epoch: 1000 train loss=0.008969542 valid loss= 0.011147453\n",
      "Epoch: 1500 train loss=0.005952419 valid loss= 0.011337452\n",
      "Epoch: 2000 train loss=0.008160462 valid loss= 0.010423305\n",
      "Epoch: 2500 train loss=0.012810803 valid loss= 0.010213468\n",
      "Epoch: 3000 train loss=0.006605437 valid loss= 0.009824554\n",
      "Epoch: 3500 train loss=0.005438136 valid loss= 0.009240935\n",
      "Epoch: 4000 train loss=0.006431568 valid loss= 0.009461473\n",
      "Epoch: 4500 train loss=0.021043014 valid loss= 0.007553545\n",
      "Epoch: 5000 train loss=0.003906779 valid loss= 0.006766297\n",
      "Epoch: 5500 train loss=0.007609342 valid loss= 0.006153926\n",
      "Epoch: 6000 train loss=0.004212425 valid loss= 0.005957434\n",
      "Epoch: 6500 train loss=0.003338923 valid loss= 0.005856100\n",
      "Epoch: 7000 train loss=0.004468343 valid loss= 0.005683383\n",
      "Epoch: 7500 train loss=0.003873316 valid loss= 0.005344601\n",
      "Epoch: 8000 train loss=0.006296479 valid loss= 0.005439141\n",
      "Epoch: 8500 train loss=0.004050942 valid loss= 0.005526278\n",
      "Epoch: 9000 train loss=0.006905213 valid loss= 0.005010983\n",
      "Epoch: 9500 train loss=0.005829821 valid loss= 0.005450021\n",
      "Epoch: 10000 train loss=0.004172536 valid loss= 0.005157744\n",
      "Epoch: 10500 train loss=0.003969260 valid loss= 0.005446244\n",
      "Epoch: 11000 train loss=0.005670715 valid loss= 0.005134909\n",
      "Epoch: 11500 train loss=0.006168301 valid loss= 0.005508817\n",
      "Epoch: 12000 train loss=0.004850513 valid loss= 0.005229753\n",
      "Epoch: 12500 train loss=0.006593787 valid loss= 0.005566555\n",
      "Epoch: 13000 train loss=0.004216339 valid loss= 0.005283460\n",
      "Epoch: 13500 train loss=0.003514761 valid loss= 0.005023845\n",
      "Epoch: 14000 train loss=0.004357545 valid loss= 0.004786206\n",
      "Epoch: 14500 train loss=0.004644524 valid loss= 0.005211253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:41:43,993]\u001b[0m Trial 13 finished with value: 0.002278732912322766 and parameters: {'lam': 0.0048028778394179185, 'learning_rate': 0.08649382256713707, 'num_epoch': 15000}. Best is trial 8 with value: 0.0020552688292528884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.004597039 valid loss= 0.005131766\n",
      "Optimization Finished!\n",
      "test loss: 0.004909774288535118, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002278732912322766\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.013620736 valid loss= 0.010592081\n",
      "Epoch: 1000 train loss=0.011332199 valid loss= 0.008304361\n",
      "Epoch: 1500 train loss=0.006612793 valid loss= 0.007018578\n",
      "Epoch: 2000 train loss=0.008715365 valid loss= 0.006834486\n",
      "Epoch: 2500 train loss=0.005999931 valid loss= 0.007065854\n",
      "Epoch: 3000 train loss=0.004669023 valid loss= 0.006858072\n",
      "Epoch: 3500 train loss=0.010844226 valid loss= 0.006519024\n",
      "Epoch: 4000 train loss=0.009278221 valid loss= 0.006345048\n",
      "Epoch: 4500 train loss=0.006516357 valid loss= 0.006140301\n",
      "Epoch: 5000 train loss=0.004293240 valid loss= 0.006449474\n",
      "Epoch: 5500 train loss=0.003872446 valid loss= 0.005934830\n",
      "Epoch: 6000 train loss=0.004504785 valid loss= 0.005756903\n",
      "Epoch: 6500 train loss=0.004573619 valid loss= 0.005710589\n",
      "Epoch: 7000 train loss=0.004753074 valid loss= 0.005660695\n",
      "Epoch: 7500 train loss=0.007065194 valid loss= 0.005957763\n",
      "Epoch: 8000 train loss=0.007574437 valid loss= 0.006064430\n",
      "Epoch: 8500 train loss=0.004138991 valid loss= 0.005573054\n",
      "Epoch: 9000 train loss=0.005983055 valid loss= 0.005880683\n",
      "Epoch: 9500 train loss=0.004448482 valid loss= 0.005830003\n",
      "Epoch: 10000 train loss=0.004106990 valid loss= 0.005944903\n",
      "Epoch: 10500 train loss=0.005587944 valid loss= 0.005671737\n",
      "Epoch: 11000 train loss=0.004036365 valid loss= 0.005884519\n",
      "Epoch: 11500 train loss=0.004971889 valid loss= 0.005700492\n",
      "Epoch: 12000 train loss=0.005357737 valid loss= 0.005468331\n",
      "Epoch: 12500 train loss=0.004552889 valid loss= 0.005839924\n",
      "Epoch: 13000 train loss=0.007120940 valid loss= 0.005623837\n",
      "Epoch: 13500 train loss=0.004267995 valid loss= 0.005522827\n",
      "Epoch: 14000 train loss=0.007379625 valid loss= 0.005647532\n",
      "Epoch: 14500 train loss=0.005055630 valid loss= 0.005589648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:44:12,042]\u001b[0m Trial 14 finished with value: 0.0023826255264126008 and parameters: {'lam': 0.005687511640516466, 'learning_rate': 0.1225918244925349, 'num_epoch': 15000}. Best is trial 8 with value: 0.0020552688292528884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.009558793 valid loss= 0.005713193\n",
      "Optimization Finished!\n",
      "test loss: 0.005439539905637503, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0023826255264126008\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.012348983 valid loss= 0.009663019\n",
      "Epoch: 1000 train loss=0.004729598 valid loss= 0.007510608\n",
      "Epoch: 1500 train loss=0.007685121 valid loss= 0.007028232\n",
      "Epoch: 2000 train loss=0.004432524 valid loss= 0.006122957\n",
      "Epoch: 2500 train loss=0.005640271 valid loss= 0.005932366\n",
      "Epoch: 3000 train loss=0.004100299 valid loss= 0.005930931\n",
      "Epoch: 3500 train loss=0.004974986 valid loss= 0.006139151\n",
      "Epoch: 4000 train loss=0.003992824 valid loss= 0.006003256\n",
      "Epoch: 4500 train loss=0.005600871 valid loss= 0.005718571\n",
      "Epoch: 5000 train loss=0.006543048 valid loss= 0.005836674\n",
      "Epoch: 5500 train loss=0.005199686 valid loss= 0.006163474\n",
      "Epoch: 6000 train loss=0.004621949 valid loss= 0.005758461\n",
      "Epoch: 6500 train loss=0.003998705 valid loss= 0.006167964\n",
      "Epoch: 7000 train loss=0.007197513 valid loss= 0.005816347\n",
      "Epoch: 7500 train loss=0.007091827 valid loss= 0.005776841\n",
      "Epoch: 8000 train loss=0.006967190 valid loss= 0.005710247\n",
      "Epoch: 8500 train loss=0.004581882 valid loss= 0.005767307\n",
      "Epoch: 9000 train loss=0.003961535 valid loss= 0.005869882\n",
      "Epoch: 9500 train loss=0.004027360 valid loss= 0.005852053\n",
      "Epoch: 10000 train loss=0.004500058 valid loss= 0.005484489\n",
      "Epoch: 10500 train loss=0.004871928 valid loss= 0.005780018\n",
      "Epoch: 11000 train loss=0.003795058 valid loss= 0.005638623\n",
      "Epoch: 11500 train loss=0.006896698 valid loss= 0.005777867\n",
      "Epoch: 12000 train loss=0.005401267 valid loss= 0.005731158\n",
      "Epoch: 12500 train loss=0.003881263 valid loss= 0.005871717\n",
      "Epoch: 13000 train loss=0.004033949 valid loss= 0.006095922\n",
      "Epoch: 13500 train loss=0.004841215 valid loss= 0.005917831\n",
      "Epoch: 14000 train loss=0.009531757 valid loss= 0.005967304\n",
      "Epoch: 14500 train loss=0.004131731 valid loss= 0.005862065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:46:37,951]\u001b[0m Trial 15 finished with value: 0.002156972493338465 and parameters: {'lam': 0.006024998771965476, 'learning_rate': 0.1879790793434074, 'num_epoch': 15000}. Best is trial 8 with value: 0.0020552688292528884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.003884946 valid loss= 0.005652872\n",
      "Optimization Finished!\n",
      "test loss: 0.0054588401690125465, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002156972493338465\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.012664422 valid loss= 0.011611437\n",
      "Epoch: 1000 train loss=0.017754629 valid loss= 0.010974564\n",
      "Epoch: 1500 train loss=0.009454953 valid loss= 0.009932226\n",
      "Epoch: 2000 train loss=0.027567711 valid loss= 0.009038283\n",
      "Epoch: 2500 train loss=0.010167108 valid loss= 0.008432524\n",
      "Epoch: 3000 train loss=0.006720756 valid loss= 0.008380682\n",
      "Epoch: 3500 train loss=0.006957921 valid loss= 0.008384150\n",
      "Epoch: 4000 train loss=0.010070449 valid loss= 0.008683551\n",
      "Epoch: 4500 train loss=0.008857344 valid loss= 0.008531934\n",
      "Epoch: 5000 train loss=0.007353094 valid loss= 0.008825377\n",
      "Epoch: 5500 train loss=0.006609284 valid loss= 0.008662320\n",
      "Epoch: 6000 train loss=0.007601583 valid loss= 0.008821165\n",
      "Epoch: 6500 train loss=0.010339156 valid loss= 0.008594931\n",
      "Epoch: 7000 train loss=0.008236375 valid loss= 0.008715821\n",
      "Epoch: 7500 train loss=0.005972723 valid loss= 0.009093831\n",
      "Epoch: 8000 train loss=0.006016039 valid loss= 0.009036966\n",
      "Epoch: 8500 train loss=0.009760763 valid loss= 0.008773766\n",
      "Epoch: 9000 train loss=0.006386178 valid loss= 0.008590328\n",
      "Epoch: 9500 train loss=0.006420388 valid loss= 0.008849057\n",
      "Epoch: 10000 train loss=0.006609662 valid loss= 0.008965783\n",
      "Epoch: 10500 train loss=0.006604821 valid loss= 0.008959590\n",
      "Epoch: 11000 train loss=0.007613011 valid loss= 0.008673655\n",
      "Epoch: 11500 train loss=0.006310992 valid loss= 0.008686587\n",
      "Epoch: 12000 train loss=0.007735109 valid loss= 0.008549295\n",
      "Epoch: 12500 train loss=0.010437391 valid loss= 0.008792231\n",
      "Epoch: 13000 train loss=0.012460901 valid loss= 0.008444883\n",
      "Epoch: 13500 train loss=0.005978574 valid loss= 0.008796543\n",
      "Epoch: 14000 train loss=0.006805951 valid loss= 0.008315071\n",
      "Epoch: 14500 train loss=0.006424608 valid loss= 0.008537805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:49:03,983]\u001b[0m Trial 16 finished with value: 0.00414928586605962 and parameters: {'lam': 0.007237755227911754, 'learning_rate': 0.04934921758323269, 'num_epoch': 15000}. Best is trial 8 with value: 0.0020552688292528884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.007068089 valid loss= 0.008587660\n",
      "Optimization Finished!\n",
      "test loss: 0.008538863621652126, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.00414928586605962\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.017979104 valid loss= 0.009765003\n",
      "Epoch: 1000 train loss=0.006244401 valid loss= 0.006491535\n",
      "Epoch: 1500 train loss=0.004748368 valid loss= 0.005620504\n",
      "Epoch: 2000 train loss=0.008779209 valid loss= 0.005311439\n",
      "Epoch: 2500 train loss=0.005248347 valid loss= 0.005053348\n",
      "Epoch: 3000 train loss=0.003643277 valid loss= 0.005375593\n",
      "Epoch: 3500 train loss=0.005931168 valid loss= 0.004986197\n",
      "Epoch: 4000 train loss=0.006509338 valid loss= 0.004374080\n",
      "Epoch: 4500 train loss=0.006047685 valid loss= 0.004885972\n",
      "Epoch: 5000 train loss=0.007183233 valid loss= 0.004892654\n",
      "Epoch: 5500 train loss=0.005021241 valid loss= 0.005313436\n",
      "Epoch: 6000 train loss=0.004924322 valid loss= 0.004649193\n",
      "Epoch: 6500 train loss=0.003455240 valid loss= 0.005266941\n",
      "Epoch: 7000 train loss=0.003571504 valid loss= 0.004827916\n",
      "Epoch: 7500 train loss=0.003265139 valid loss= 0.004285852\n",
      "Epoch: 8000 train loss=0.004629270 valid loss= 0.004278945\n",
      "Epoch: 8500 train loss=0.004910861 valid loss= 0.004970474\n",
      "Epoch: 9000 train loss=0.005221481 valid loss= 0.004098287\n",
      "Epoch: 9500 train loss=0.005371982 valid loss= 0.004253475\n",
      "Epoch: 10000 train loss=0.004242362 valid loss= 0.004744282\n",
      "Epoch: 10500 train loss=0.003552197 valid loss= 0.004784490\n",
      "Epoch: 11000 train loss=0.004418868 valid loss= 0.005455888\n",
      "Epoch: 11500 train loss=0.004038758 valid loss= 0.004979195\n",
      "Epoch: 12000 train loss=0.004426948 valid loss= 0.003996922\n",
      "Epoch: 12500 train loss=0.003228849 valid loss= 0.004683306\n",
      "Epoch: 13000 train loss=0.003961845 valid loss= 0.004844758\n",
      "Epoch: 13500 train loss=0.004133093 valid loss= 0.004706180\n",
      "Epoch: 14000 train loss=0.003688619 valid loss= 0.005063802\n",
      "Epoch: 14500 train loss=0.003002361 valid loss= 0.004857031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:51:29,354]\u001b[0m Trial 17 finished with value: 0.0022296549555165936 and parameters: {'lam': 0.004097335534015765, 'learning_rate': 0.16690511238852843, 'num_epoch': 15000}. Best is trial 8 with value: 0.0020552688292528884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.003501724 valid loss= 0.004876586\n",
      "Optimization Finished!\n",
      "test loss: 0.004887956194579601, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0022296549555165936\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.013054175 valid loss= 0.012235066\n",
      "Epoch: 1000 train loss=0.013494435 valid loss= 0.011483297\n",
      "Epoch: 1500 train loss=0.015952677 valid loss= 0.010757246\n",
      "Epoch: 2000 train loss=0.016619559 valid loss= 0.010174665\n",
      "Epoch: 2500 train loss=0.020996792 valid loss= 0.010244759\n",
      "Epoch: 3000 train loss=0.014426891 valid loss= 0.009695059\n",
      "Epoch: 3500 train loss=0.007461815 valid loss= 0.009763247\n",
      "Epoch: 4000 train loss=0.010044305 valid loss= 0.009347591\n",
      "Epoch: 4500 train loss=0.017147236 valid loss= 0.008848006\n",
      "Epoch: 5000 train loss=0.007862199 valid loss= 0.008490168\n",
      "Epoch: 5500 train loss=0.008271158 valid loss= 0.007859105\n",
      "Epoch: 6000 train loss=0.009571459 valid loss= 0.008134141\n",
      "Epoch: 6500 train loss=0.007608525 valid loss= 0.007706231\n",
      "Epoch: 7000 train loss=0.018806409 valid loss= 0.007689208\n",
      "Epoch: 7500 train loss=0.016387805 valid loss= 0.007457439\n",
      "Epoch: 8000 train loss=0.007137356 valid loss= 0.007518774\n",
      "Epoch: 8500 train loss=0.009130120 valid loss= 0.007701521\n",
      "Epoch: 9000 train loss=0.015904717 valid loss= 0.007660138\n",
      "Epoch: 9500 train loss=0.007855406 valid loss= 0.007574365\n",
      "Epoch: 10000 train loss=0.009496210 valid loss= 0.007779637\n",
      "Epoch: 10500 train loss=0.005981487 valid loss= 0.007591403\n",
      "Epoch: 11000 train loss=0.008116387 valid loss= 0.007599495\n",
      "Epoch: 11500 train loss=0.009105705 valid loss= 0.007438307\n",
      "Epoch: 12000 train loss=0.006905114 valid loss= 0.007449416\n",
      "Epoch: 12500 train loss=0.005746770 valid loss= 0.007067149\n",
      "Epoch: 13000 train loss=0.009217773 valid loss= 0.007432183\n",
      "Epoch: 13500 train loss=0.013066721 valid loss= 0.006946682\n",
      "Epoch: 14000 train loss=0.005469547 valid loss= 0.007696338\n",
      "Epoch: 14500 train loss=0.006950774 valid loss= 0.007306532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:53:55,455]\u001b[0m Trial 18 finished with value: 0.0023201418523358715 and parameters: {'lam': 0.00688028567521341, 'learning_rate': 0.0153894705166207, 'num_epoch': 15000}. Best is trial 8 with value: 0.0020552688292528884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.011274176 valid loss= 0.007048293\n",
      "Optimization Finished!\n",
      "test loss: 0.006793521344661713, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0023201418523358715\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.007953524 valid loss= 0.008531602\n",
      "Epoch: 1000 train loss=0.008113540 valid loss= 0.008540146\n",
      "Epoch: 1500 train loss=0.005753884 valid loss= 0.008138999\n",
      "Epoch: 2000 train loss=0.006179729 valid loss= 0.007153749\n",
      "Epoch: 2500 train loss=0.006859669 valid loss= 0.006662113\n",
      "Epoch: 3000 train loss=0.013933437 valid loss= 0.006965524\n",
      "Epoch: 3500 train loss=0.005050764 valid loss= 0.006850909\n",
      "Epoch: 4000 train loss=0.005121225 valid loss= 0.006802810\n",
      "Epoch: 4500 train loss=0.006213802 valid loss= 0.006882377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:54:45,022]\u001b[0m Trial 19 finished with value: 0.002275253406076098 and parameters: {'lam': 0.007149430764889204, 'learning_rate': 0.18902021461865795, 'num_epoch': 5000}. Best is trial 8 with value: 0.0020552688292528884.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.005297500 valid loss= 0.006508999\n",
      "Optimization Finished!\n",
      "test loss: 0.006266502197831869, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002275253406076098\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.013251441 valid loss= 0.009790006\n",
      "Epoch: 1000 train loss=0.011848223 valid loss= 0.008114240\n",
      "Epoch: 1500 train loss=0.006381647 valid loss= 0.007255204\n",
      "Epoch: 2000 train loss=0.007126063 valid loss= 0.005329410\n",
      "Epoch: 2500 train loss=0.004912111 valid loss= 0.004307284\n",
      "Epoch: 3000 train loss=0.004131252 valid loss= 0.003399443\n",
      "Epoch: 3500 train loss=0.004347038 valid loss= 0.003220928\n",
      "Epoch: 4000 train loss=0.004723764 valid loss= 0.003603285\n",
      "Epoch: 4500 train loss=0.003642793 valid loss= 0.003363895\n",
      "Epoch: 5000 train loss=0.004035263 valid loss= 0.003662228\n",
      "Epoch: 5500 train loss=0.002849145 valid loss= 0.003234416\n",
      "Epoch: 6000 train loss=0.001882573 valid loss= 0.003335768\n",
      "Epoch: 6500 train loss=0.005391199 valid loss= 0.003311573\n",
      "Epoch: 7000 train loss=0.003113378 valid loss= 0.002953765\n",
      "Epoch: 7500 train loss=0.004365137 valid loss= 0.003032205\n",
      "Epoch: 8000 train loss=0.006457224 valid loss= 0.002689488\n",
      "Epoch: 8500 train loss=0.003184155 valid loss= 0.002653552\n",
      "Epoch: 9000 train loss=0.003658332 valid loss= 0.002547942\n",
      "Epoch: 9500 train loss=0.003241960 valid loss= 0.002093308\n",
      "Epoch: 10000 train loss=0.004698900 valid loss= 0.002489082\n",
      "Epoch: 10500 train loss=0.005125600 valid loss= 0.002322489\n",
      "Epoch: 11000 train loss=0.002167206 valid loss= 0.002092815\n",
      "Epoch: 11500 train loss=0.002446501 valid loss= 0.002351094\n",
      "Epoch: 12000 train loss=0.007619474 valid loss= 0.001957040\n",
      "Epoch: 12500 train loss=0.003022338 valid loss= 0.002336080\n",
      "Epoch: 13000 train loss=0.002614031 valid loss= 0.002051581\n",
      "Epoch: 13500 train loss=0.001942420 valid loss= 0.002233410\n",
      "Epoch: 14000 train loss=0.001737693 valid loss= 0.002298259\n",
      "Epoch: 14500 train loss=0.003373646 valid loss= 0.002065120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:57:10,756]\u001b[0m Trial 20 finished with value: 0.0002924818646676488 and parameters: {'lam': 0.002292293155230627, 'learning_rate': 0.09214842664633631, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.002115719 valid loss= 0.001844452\n",
      "Optimization Finished!\n",
      "test loss: 0.0018103569746017456, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0002924818646676488\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.021099634 valid loss= 0.007948466\n",
      "Epoch: 1000 train loss=0.006118246 valid loss= 0.007114742\n",
      "Epoch: 1500 train loss=0.008607402 valid loss= 0.004321745\n",
      "Epoch: 2000 train loss=0.005139918 valid loss= 0.003852424\n",
      "Epoch: 2500 train loss=0.010693357 valid loss= 0.003865075\n",
      "Epoch: 3000 train loss=0.002729586 valid loss= 0.004492206\n",
      "Epoch: 3500 train loss=0.002952207 valid loss= 0.003911588\n",
      "Epoch: 4000 train loss=0.004006739 valid loss= 0.003959005\n",
      "Epoch: 4500 train loss=0.011778045 valid loss= 0.004198262\n",
      "Epoch: 5000 train loss=0.009338134 valid loss= 0.003916681\n",
      "Epoch: 5500 train loss=0.003043428 valid loss= 0.003263961\n",
      "Epoch: 6000 train loss=0.002563131 valid loss= 0.003455022\n",
      "Epoch: 6500 train loss=0.007871557 valid loss= 0.003419607\n",
      "Epoch: 7000 train loss=0.002695003 valid loss= 0.003014999\n",
      "Epoch: 7500 train loss=0.003232381 valid loss= 0.003477050\n",
      "Epoch: 8000 train loss=0.003192188 valid loss= 0.003210513\n",
      "Epoch: 8500 train loss=0.005562139 valid loss= 0.003213064\n",
      "Epoch: 9000 train loss=0.006604224 valid loss= 0.003196439\n",
      "Epoch: 9500 train loss=0.002549026 valid loss= 0.002932519\n",
      "Epoch: 10000 train loss=0.002056434 valid loss= 0.002677110\n",
      "Epoch: 10500 train loss=0.004888045 valid loss= 0.002652626\n",
      "Epoch: 11000 train loss=0.003891538 valid loss= 0.002764847\n",
      "Epoch: 11500 train loss=0.002315029 valid loss= 0.002768059\n",
      "Epoch: 12000 train loss=0.005888415 valid loss= 0.003063963\n",
      "Epoch: 12500 train loss=0.002215894 valid loss= 0.002377731\n",
      "Epoch: 13000 train loss=0.003509337 valid loss= 0.002778689\n",
      "Epoch: 13500 train loss=0.005893371 valid loss= 0.002163161\n",
      "Epoch: 14000 train loss=0.002755938 valid loss= 0.002664970\n",
      "Epoch: 14500 train loss=0.002139876 valid loss= 0.002414472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 04:59:36,881]\u001b[0m Trial 21 finished with value: 0.0007588288429506677 and parameters: {'lam': 0.002751104636134569, 'learning_rate': 0.08592594302338136, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.004803315 valid loss= 0.002619209\n",
      "Optimization Finished!\n",
      "test loss: 0.0025692940689623356, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0007588288429506677\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.014019719 valid loss= 0.007960748\n",
      "Epoch: 1000 train loss=0.009627106 valid loss= 0.006652255\n",
      "Epoch: 1500 train loss=0.017844327 valid loss= 0.004534280\n",
      "Epoch: 2000 train loss=0.004366710 valid loss= 0.004366750\n",
      "Epoch: 2500 train loss=0.004422338 valid loss= 0.004395275\n",
      "Epoch: 3000 train loss=0.003243648 valid loss= 0.004354035\n",
      "Epoch: 3500 train loss=0.004362565 valid loss= 0.004455052\n",
      "Epoch: 4000 train loss=0.003243355 valid loss= 0.004050418\n",
      "Epoch: 4500 train loss=0.006405751 valid loss= 0.004141083\n",
      "Epoch: 5000 train loss=0.002091532 valid loss= 0.004312077\n",
      "Epoch: 5500 train loss=0.002439606 valid loss= 0.003849226\n",
      "Epoch: 6000 train loss=0.001885779 valid loss= 0.004001124\n",
      "Epoch: 6500 train loss=0.004115498 valid loss= 0.004251068\n",
      "Epoch: 7000 train loss=0.001902908 valid loss= 0.004101611\n",
      "Epoch: 7500 train loss=0.003814431 valid loss= 0.004016555\n",
      "Epoch: 8000 train loss=0.003493239 valid loss= 0.003848573\n",
      "Epoch: 8500 train loss=0.001859632 valid loss= 0.003614206\n",
      "Epoch: 9000 train loss=0.004339585 valid loss= 0.004010552\n",
      "Epoch: 9500 train loss=0.001896842 valid loss= 0.004095469\n",
      "Epoch: 10000 train loss=0.003719298 valid loss= 0.004118713\n",
      "Epoch: 10500 train loss=0.002220461 valid loss= 0.003702586\n",
      "Epoch: 11000 train loss=0.003649072 valid loss= 0.003612814\n",
      "Epoch: 11500 train loss=0.003049150 valid loss= 0.004077493\n",
      "Epoch: 12000 train loss=0.003419903 valid loss= 0.003693672\n",
      "Epoch: 12500 train loss=0.002832292 valid loss= 0.003691960\n",
      "Epoch: 13000 train loss=0.004054640 valid loss= 0.003918328\n",
      "Epoch: 13500 train loss=0.007559042 valid loss= 0.003492648\n",
      "Epoch: 14000 train loss=0.004696410 valid loss= 0.003637442\n",
      "Epoch: 14500 train loss=0.002427839 valid loss= 0.003801685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:02:04,076]\u001b[0m Trial 22 finished with value: 0.002315713205680379 and parameters: {'lam': 0.002497417979130209, 'learning_rate': 0.0782986584599797, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.003371041 valid loss= 0.003807863\n",
      "Optimization Finished!\n",
      "test loss: 0.003568533807992935, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002315713205680379\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.016776597 valid loss= 0.009546671\n",
      "Epoch: 1000 train loss=0.011740960 valid loss= 0.007767168\n",
      "Epoch: 1500 train loss=0.008115681 valid loss= 0.004803021\n",
      "Epoch: 2000 train loss=0.003198315 valid loss= 0.004091864\n",
      "Epoch: 2500 train loss=0.004367473 valid loss= 0.004645795\n",
      "Epoch: 3000 train loss=0.005492105 valid loss= 0.004719227\n",
      "Epoch: 3500 train loss=0.007073808 valid loss= 0.005234104\n",
      "Epoch: 4000 train loss=0.003922055 valid loss= 0.005378455\n",
      "Epoch: 4500 train loss=0.005381867 valid loss= 0.005437402\n",
      "Epoch: 5000 train loss=0.010796642 valid loss= 0.005149607\n",
      "Epoch: 5500 train loss=0.003593496 valid loss= 0.004659642\n",
      "Epoch: 6000 train loss=0.002752701 valid loss= 0.004710597\n",
      "Epoch: 6500 train loss=0.010752429 valid loss= 0.004166143\n",
      "Epoch: 7000 train loss=0.002896611 valid loss= 0.004342791\n",
      "Epoch: 7500 train loss=0.002686383 valid loss= 0.004453506\n",
      "Epoch: 8000 train loss=0.002837169 valid loss= 0.004034791\n",
      "Epoch: 8500 train loss=0.002468825 valid loss= 0.004210749\n",
      "Epoch: 9000 train loss=0.002777833 valid loss= 0.004249403\n",
      "Epoch: 9500 train loss=0.002734333 valid loss= 0.003952659\n",
      "Epoch: 10000 train loss=0.003820601 valid loss= 0.004019351\n",
      "Epoch: 10500 train loss=0.004910932 valid loss= 0.003859149\n",
      "Epoch: 11000 train loss=0.007127213 valid loss= 0.004244519\n",
      "Epoch: 11500 train loss=0.002664899 valid loss= 0.004009414\n",
      "Epoch: 12000 train loss=0.005333019 valid loss= 0.003854092\n",
      "Epoch: 12500 train loss=0.003806858 valid loss= 0.004088260\n",
      "Epoch: 13000 train loss=0.002613024 valid loss= 0.003997375\n",
      "Epoch: 13500 train loss=0.003431224 valid loss= 0.004452285\n",
      "Epoch: 14000 train loss=0.002334164 valid loss= 0.003647060\n",
      "Epoch: 14500 train loss=0.005713549 valid loss= 0.004126498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:04:30,460]\u001b[0m Trial 23 finished with value: 0.0022578294363761677 and parameters: {'lam': 0.0025705151618824005, 'learning_rate': 0.11510892169365486, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.002301065 valid loss= 0.004004650\n",
      "Optimization Finished!\n",
      "test loss: 0.003855331800878048, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0022578294363761677\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.016969459 valid loss= 0.008494799\n",
      "Epoch: 1000 train loss=0.010712699 valid loss= 0.007371914\n",
      "Epoch: 1500 train loss=0.010378660 valid loss= 0.007172680\n",
      "Epoch: 2000 train loss=0.009237086 valid loss= 0.007156043\n",
      "Epoch: 2500 train loss=0.012855428 valid loss= 0.007062869\n",
      "Epoch: 3000 train loss=0.019304339 valid loss= 0.005552229\n",
      "Epoch: 3500 train loss=0.006539260 valid loss= 0.005071019\n",
      "Epoch: 4000 train loss=0.007984434 valid loss= 0.004718664\n",
      "Epoch: 4500 train loss=0.002853595 valid loss= 0.003840676\n",
      "Epoch: 5000 train loss=0.005825882 valid loss= 0.003724694\n",
      "Epoch: 5500 train loss=0.003151813 valid loss= 0.003456107\n",
      "Epoch: 6000 train loss=0.004499429 valid loss= 0.003577254\n",
      "Epoch: 6500 train loss=0.006203860 valid loss= 0.003308225\n",
      "Epoch: 7000 train loss=0.003121468 valid loss= 0.003545527\n",
      "Epoch: 7500 train loss=0.002420581 valid loss= 0.003667704\n",
      "Epoch: 8000 train loss=0.003300467 valid loss= 0.003700880\n",
      "Epoch: 8500 train loss=0.005027368 valid loss= 0.003571643\n",
      "Epoch: 9000 train loss=0.002288176 valid loss= 0.003523958\n",
      "Epoch: 9500 train loss=0.005204054 valid loss= 0.004031991\n",
      "Epoch: 10000 train loss=0.005395729 valid loss= 0.003755116\n",
      "Epoch: 10500 train loss=0.003615061 valid loss= 0.003574128\n",
      "Epoch: 11000 train loss=0.003363725 valid loss= 0.003747172\n",
      "Epoch: 11500 train loss=0.003031512 valid loss= 0.003572370\n",
      "Epoch: 12000 train loss=0.004295785 valid loss= 0.003699140\n",
      "Epoch: 12500 train loss=0.003302315 valid loss= 0.003637660\n",
      "Epoch: 13000 train loss=0.003428795 valid loss= 0.003545135\n",
      "Epoch: 13500 train loss=0.006981822 valid loss= 0.003479437\n",
      "Epoch: 14000 train loss=0.003013898 valid loss= 0.003778832\n",
      "Epoch: 14500 train loss=0.005472197 valid loss= 0.003687299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:06:55,842]\u001b[0m Trial 24 finished with value: 0.002160667497964756 and parameters: {'lam': 0.002083712113389387, 'learning_rate': 0.04123868376766286, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.002686200 valid loss= 0.003656594\n",
      "Optimization Finished!\n",
      "test loss: 0.0034131789579987526, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002160667497964756\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.010410964 valid loss= 0.007674138\n",
      "Epoch: 1000 train loss=0.015247028 valid loss= 0.007039030\n",
      "Epoch: 1500 train loss=0.004977827 valid loss= 0.005249175\n",
      "Epoch: 2000 train loss=0.003439931 valid loss= 0.005040380\n",
      "Epoch: 2500 train loss=0.006232762 valid loss= 0.005272581\n",
      "Epoch: 3000 train loss=0.004129419 valid loss= 0.004838355\n",
      "Epoch: 3500 train loss=0.008421872 valid loss= 0.004917467\n",
      "Epoch: 4000 train loss=0.008215660 valid loss= 0.005140652\n",
      "Epoch: 4500 train loss=0.011325254 valid loss= 0.004918304\n",
      "Epoch: 5000 train loss=0.003517390 valid loss= 0.005160093\n",
      "Epoch: 5500 train loss=0.004585782 valid loss= 0.004879781\n",
      "Epoch: 6000 train loss=0.002290274 valid loss= 0.004878101\n",
      "Epoch: 6500 train loss=0.004101548 valid loss= 0.004493844\n",
      "Epoch: 7000 train loss=0.005683177 valid loss= 0.005091156\n",
      "Epoch: 7500 train loss=0.004774658 valid loss= 0.004690366\n",
      "Epoch: 8000 train loss=0.006604135 valid loss= 0.004406352\n",
      "Epoch: 8500 train loss=0.002896021 valid loss= 0.004408346\n",
      "Epoch: 9000 train loss=0.002794787 valid loss= 0.004341514\n",
      "Epoch: 9500 train loss=0.006808852 valid loss= 0.004183212\n",
      "Epoch: 10000 train loss=0.003855763 valid loss= 0.004115402\n",
      "Epoch: 10500 train loss=0.003143181 valid loss= 0.004249855\n",
      "Epoch: 11000 train loss=0.006025593 valid loss= 0.004060284\n",
      "Epoch: 11500 train loss=0.005980021 valid loss= 0.004096441\n",
      "Epoch: 12000 train loss=0.002302772 valid loss= 0.004086724\n",
      "Epoch: 12500 train loss=0.002363791 valid loss= 0.004098583\n",
      "Epoch: 13000 train loss=0.004351447 valid loss= 0.004259896\n",
      "Epoch: 13500 train loss=0.002594086 valid loss= 0.003966316\n",
      "Epoch: 14000 train loss=0.002041814 valid loss= 0.003985139\n",
      "Epoch: 14500 train loss=0.002154868 valid loss= 0.003856629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:09:24,321]\u001b[0m Trial 25 finished with value: 0.002393255612079863 and parameters: {'lam': 0.0028652633147628758, 'learning_rate': 0.07637752426808309, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.003643901 valid loss= 0.004125746\n",
      "Optimization Finished!\n",
      "test loss: 0.0038874384481459856, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002393255612079863\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.015342175 valid loss= 0.009559542\n",
      "Epoch: 1000 train loss=0.009920016 valid loss= 0.009328254\n",
      "Epoch: 1500 train loss=0.011252632 valid loss= 0.008299798\n",
      "Epoch: 2000 train loss=0.007786541 valid loss= 0.006692502\n",
      "Epoch: 2500 train loss=0.003739072 valid loss= 0.005822474\n",
      "Epoch: 3000 train loss=0.006338723 valid loss= 0.005181953\n",
      "Epoch: 3500 train loss=0.004307351 valid loss= 0.005165363\n",
      "Epoch: 4000 train loss=0.004461458 valid loss= 0.005102260\n",
      "Epoch: 4500 train loss=0.004170232 valid loss= 0.005232207\n",
      "Epoch: 5000 train loss=0.005095166 valid loss= 0.005254141\n",
      "Epoch: 5500 train loss=0.004508418 valid loss= 0.005294350\n",
      "Epoch: 6000 train loss=0.012013863 valid loss= 0.004712919\n",
      "Epoch: 6500 train loss=0.002736704 valid loss= 0.005044082\n",
      "Epoch: 7000 train loss=0.006948959 valid loss= 0.004795114\n",
      "Epoch: 7500 train loss=0.004000136 valid loss= 0.004858359\n",
      "Epoch: 8000 train loss=0.002895696 valid loss= 0.004833376\n",
      "Epoch: 8500 train loss=0.004575674 valid loss= 0.004681100\n",
      "Epoch: 9000 train loss=0.003857124 valid loss= 0.005100942\n",
      "Epoch: 9500 train loss=0.004257098 valid loss= 0.004902358\n",
      "Epoch: 10000 train loss=0.002870239 valid loss= 0.004709654\n",
      "Epoch: 10500 train loss=0.002618779 valid loss= 0.004516984\n",
      "Epoch: 11000 train loss=0.005699970 valid loss= 0.004705925\n",
      "Epoch: 11500 train loss=0.004540935 valid loss= 0.004681773\n",
      "Epoch: 12000 train loss=0.002645522 valid loss= 0.004483784\n",
      "Epoch: 12500 train loss=0.002888444 valid loss= 0.004648917\n",
      "Epoch: 13000 train loss=0.003813580 valid loss= 0.004835591\n",
      "Epoch: 13500 train loss=0.003737423 valid loss= 0.004675679\n",
      "Epoch: 14000 train loss=0.002776752 valid loss= 0.004542240\n",
      "Epoch: 14500 train loss=0.008621201 valid loss= 0.004656229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:11:51,090]\u001b[0m Trial 26 finished with value: 0.002157083802823554 and parameters: {'lam': 0.0038298220370913596, 'learning_rate': 0.09563756313209221, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.002676400 valid loss= 0.004423404\n",
      "Optimization Finished!\n",
      "test loss: 0.004257244989275932, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002157083802823554\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.009887820 valid loss= 0.009508171\n",
      "Epoch: 1000 train loss=0.011216423 valid loss= 0.009978408\n",
      "Epoch: 1500 train loss=0.006040249 valid loss= 0.008687245\n",
      "Epoch: 2000 train loss=0.007160773 valid loss= 0.008415176\n",
      "Epoch: 2500 train loss=0.004154616 valid loss= 0.008240996\n",
      "Epoch: 3000 train loss=0.007431848 valid loss= 0.007631902\n",
      "Epoch: 3500 train loss=0.004709042 valid loss= 0.007405854\n",
      "Epoch: 4000 train loss=0.004444334 valid loss= 0.007452092\n",
      "Epoch: 4500 train loss=0.004087697 valid loss= 0.006998935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:12:40,576]\u001b[0m Trial 27 finished with value: 0.00553533255546764 and parameters: {'lam': 0.001979661201509275, 'learning_rate': 0.060399502984679394, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.004373020 valid loss= 0.007179925\n",
      "Optimization Finished!\n",
      "test loss: 0.007606192026287317, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.00553533255546764\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.014340432 valid loss= 0.009880141\n",
      "Epoch: 1000 train loss=0.011035620 valid loss= 0.010032510\n",
      "Epoch: 1500 train loss=0.006045135 valid loss= 0.009914850\n",
      "Epoch: 2000 train loss=0.006936460 valid loss= 0.008428673\n",
      "Epoch: 2500 train loss=0.003942866 valid loss= 0.007050968\n",
      "Epoch: 3000 train loss=0.004954560 valid loss= 0.006576213\n",
      "Epoch: 3500 train loss=0.004644596 valid loss= 0.006576679\n",
      "Epoch: 4000 train loss=0.008698093 valid loss= 0.006083556\n",
      "Epoch: 4500 train loss=0.003802248 valid loss= 0.005668462\n",
      "Epoch: 5000 train loss=0.005050919 valid loss= 0.005730215\n",
      "Epoch: 5500 train loss=0.003884054 valid loss= 0.005272888\n",
      "Epoch: 6000 train loss=0.005332786 valid loss= 0.005424825\n",
      "Epoch: 6500 train loss=0.003802279 valid loss= 0.005188616\n",
      "Epoch: 7000 train loss=0.004016421 valid loss= 0.005338323\n",
      "Epoch: 7500 train loss=0.004011556 valid loss= 0.004935621\n",
      "Epoch: 8000 train loss=0.004210034 valid loss= 0.005077438\n",
      "Epoch: 8500 train loss=0.003473797 valid loss= 0.005085015\n",
      "Epoch: 9000 train loss=0.003959895 valid loss= 0.004686821\n",
      "Epoch: 9500 train loss=0.003481169 valid loss= 0.005080194\n",
      "Epoch: 10000 train loss=0.004762316 valid loss= 0.005048186\n",
      "Epoch: 10500 train loss=0.003472618 valid loss= 0.005110691\n",
      "Epoch: 11000 train loss=0.004610891 valid loss= 0.005005431\n",
      "Epoch: 11500 train loss=0.003488423 valid loss= 0.004860374\n",
      "Epoch: 12000 train loss=0.003317549 valid loss= 0.005073906\n",
      "Epoch: 12500 train loss=0.003257463 valid loss= 0.004661145\n",
      "Epoch: 13000 train loss=0.006676474 valid loss= 0.004813368\n",
      "Epoch: 13500 train loss=0.005062558 valid loss= 0.004911723\n",
      "Epoch: 14000 train loss=0.005822748 valid loss= 0.004826560\n",
      "Epoch: 14500 train loss=0.003266735 valid loss= 0.004717647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:15:06,688]\u001b[0m Trial 28 finished with value: 0.0020633140335526118 and parameters: {'lam': 0.00451297533870014, 'learning_rate': 0.14254692451830034, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.004107280 valid loss= 0.004705795\n",
      "Optimization Finished!\n",
      "test loss: 0.0046318816021084785, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0020633140335526118\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.007628936 valid loss= 0.007953075\n",
      "Epoch: 1000 train loss=0.010992550 valid loss= 0.007720626\n",
      "Epoch: 1500 train loss=0.015871974 valid loss= 0.007063032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:15:27,794]\u001b[0m Trial 29 finished with value: 0.0066917744436250735 and parameters: {'lam': 0.001015196334937472, 'learning_rate': 0.03366248303736346, 'num_epoch': 2000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 train loss=0.010837772 valid loss= 0.007597852\n",
      "Optimization Finished!\n",
      "test loss: 0.00891832821071148, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0066917744436250735\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.015811393 valid loss= 0.009002386\n",
      "Epoch: 1000 train loss=0.005448694 valid loss= 0.008855099\n",
      "Epoch: 1500 train loss=0.009741497 valid loss= 0.008140234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:15:49,178]\u001b[0m Trial 30 finished with value: 0.004434410167364298 and parameters: {'lam': 0.003205859951124777, 'learning_rate': 0.04694533868831051, 'num_epoch': 2000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 train loss=0.011241456 valid loss= 0.007072057\n",
      "Optimization Finished!\n",
      "test loss: 0.0078745037317276, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.004434410167364298\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.010589235 valid loss= 0.011614855\n",
      "Epoch: 1000 train loss=0.021154756 valid loss= 0.011684816\n",
      "Epoch: 1500 train loss=0.011024416 valid loss= 0.010186178\n",
      "Epoch: 2000 train loss=0.006085637 valid loss= 0.007850942\n",
      "Epoch: 2500 train loss=0.013651059 valid loss= 0.007233526\n",
      "Epoch: 3000 train loss=0.005111156 valid loss= 0.006921370\n",
      "Epoch: 3500 train loss=0.008078621 valid loss= 0.006307398\n",
      "Epoch: 4000 train loss=0.004303952 valid loss= 0.006134850\n",
      "Epoch: 4500 train loss=0.003740739 valid loss= 0.006311263\n",
      "Epoch: 5000 train loss=0.005570123 valid loss= 0.006136407\n",
      "Epoch: 5500 train loss=0.004563329 valid loss= 0.006032996\n",
      "Epoch: 6000 train loss=0.009251942 valid loss= 0.006192751\n",
      "Epoch: 6500 train loss=0.005428695 valid loss= 0.006113749\n",
      "Epoch: 7000 train loss=0.008303793 valid loss= 0.006074095\n",
      "Epoch: 7500 train loss=0.007063339 valid loss= 0.005716166\n",
      "Epoch: 8000 train loss=0.003568793 valid loss= 0.005929265\n",
      "Epoch: 8500 train loss=0.005291834 valid loss= 0.005824462\n",
      "Epoch: 9000 train loss=0.003859936 valid loss= 0.005647428\n",
      "Epoch: 9500 train loss=0.003586765 valid loss= 0.005930694\n",
      "Epoch: 10000 train loss=0.006086326 valid loss= 0.005816809\n",
      "Epoch: 10500 train loss=0.003737058 valid loss= 0.005558913\n",
      "Epoch: 11000 train loss=0.003197940 valid loss= 0.005636389\n",
      "Epoch: 11500 train loss=0.003330115 valid loss= 0.005914222\n",
      "Epoch: 12000 train loss=0.007305482 valid loss= 0.006244231\n",
      "Epoch: 12500 train loss=0.004105464 valid loss= 0.005535653\n",
      "Epoch: 13000 train loss=0.003891395 valid loss= 0.005781206\n",
      "Epoch: 13500 train loss=0.003158503 valid loss= 0.005828626\n",
      "Epoch: 14000 train loss=0.004431252 valid loss= 0.005580927\n",
      "Epoch: 14500 train loss=0.003520788 valid loss= 0.005411466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:18:13,908]\u001b[0m Trial 31 finished with value: 0.003081966724907922 and parameters: {'lam': 0.004407530732578871, 'learning_rate': 0.14235148446742651, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.005849209 valid loss= 0.005874604\n",
      "Optimization Finished!\n",
      "test loss: 0.006006978452205658, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.003081966724907922\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.007893599 valid loss= 0.010047792\n",
      "Epoch: 1000 train loss=0.009700712 valid loss= 0.009475157\n",
      "Epoch: 1500 train loss=0.004626623 valid loss= 0.005543958\n",
      "Epoch: 2000 train loss=0.006036119 valid loss= 0.005090661\n",
      "Epoch: 2500 train loss=0.004223286 valid loss= 0.004952504\n",
      "Epoch: 3000 train loss=0.003686262 valid loss= 0.005409451\n",
      "Epoch: 3500 train loss=0.002942384 valid loss= 0.004889025\n",
      "Epoch: 4000 train loss=0.002279427 valid loss= 0.004733989\n",
      "Epoch: 4500 train loss=0.002910715 valid loss= 0.004592702\n",
      "Epoch: 5000 train loss=0.004965853 valid loss= 0.004251890\n",
      "Epoch: 5500 train loss=0.003713930 valid loss= 0.004234246\n",
      "Epoch: 6000 train loss=0.003258039 valid loss= 0.004050041\n",
      "Epoch: 6500 train loss=0.001869064 valid loss= 0.004073724\n",
      "Epoch: 7000 train loss=0.003437043 valid loss= 0.003817792\n",
      "Epoch: 7500 train loss=0.002634093 valid loss= 0.003920845\n",
      "Epoch: 8000 train loss=0.004957380 valid loss= 0.003851747\n",
      "Epoch: 8500 train loss=0.001765104 valid loss= 0.003832403\n",
      "Epoch: 9000 train loss=0.001939543 valid loss= 0.003980512\n",
      "Epoch: 9500 train loss=0.002193885 valid loss= 0.003951747\n",
      "Epoch: 10000 train loss=0.002658641 valid loss= 0.003944039\n",
      "Epoch: 10500 train loss=0.002787913 valid loss= 0.003857078\n",
      "Epoch: 11000 train loss=0.002949896 valid loss= 0.003918355\n",
      "Epoch: 11500 train loss=0.004434556 valid loss= 0.003999766\n",
      "Epoch: 12000 train loss=0.002785182 valid loss= 0.003857940\n",
      "Epoch: 12500 train loss=0.002403083 valid loss= 0.003863638\n",
      "Epoch: 13000 train loss=0.002725820 valid loss= 0.003949819\n",
      "Epoch: 13500 train loss=0.001896475 valid loss= 0.003824089\n",
      "Epoch: 14000 train loss=0.002919751 valid loss= 0.003841989\n",
      "Epoch: 14500 train loss=0.003821580 valid loss= 0.003875420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:20:40,090]\u001b[0m Trial 32 finished with value: 0.002469388251069835 and parameters: {'lam': 0.0026339519001563343, 'learning_rate': 0.10500216562572152, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.002402383 valid loss= 0.004026130\n",
      "Optimization Finished!\n",
      "test loss: 0.0037345942109823227, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002469388251069835\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.005213166 valid loss= 0.007872904\n",
      "Epoch: 1000 train loss=0.003777893 valid loss= 0.004571108\n",
      "Epoch: 1500 train loss=0.005650445 valid loss= 0.004508628\n",
      "Epoch: 2000 train loss=0.003673440 valid loss= 0.004465677\n",
      "Epoch: 2500 train loss=0.001663080 valid loss= 0.003978622\n",
      "Epoch: 3000 train loss=0.003950951 valid loss= 0.004041662\n",
      "Epoch: 3500 train loss=0.002951090 valid loss= 0.003151149\n",
      "Epoch: 4000 train loss=0.005994229 valid loss= 0.003317578\n",
      "Epoch: 4500 train loss=0.002186201 valid loss= 0.003381693\n",
      "Epoch: 5000 train loss=0.007179102 valid loss= 0.003889098\n",
      "Epoch: 5500 train loss=0.003806774 valid loss= 0.003167905\n",
      "Epoch: 6000 train loss=0.001677394 valid loss= 0.003188566\n",
      "Epoch: 6500 train loss=0.004544994 valid loss= 0.003334349\n",
      "Epoch: 7000 train loss=0.002465398 valid loss= 0.002953371\n",
      "Epoch: 7500 train loss=0.004702213 valid loss= 0.003242547\n",
      "Epoch: 8000 train loss=0.004772215 valid loss= 0.002918383\n",
      "Epoch: 8500 train loss=0.003480779 valid loss= 0.002908756\n",
      "Epoch: 9000 train loss=0.002822863 valid loss= 0.003556280\n",
      "Epoch: 9500 train loss=0.003788725 valid loss= 0.003158982\n",
      "Epoch: 10000 train loss=0.002534559 valid loss= 0.003412969\n",
      "Epoch: 10500 train loss=0.007245760 valid loss= 0.003244405\n",
      "Epoch: 11000 train loss=0.002805097 valid loss= 0.003114283\n",
      "Epoch: 11500 train loss=0.001484875 valid loss= 0.003391367\n",
      "Epoch: 12000 train loss=0.003004794 valid loss= 0.003374165\n",
      "Epoch: 12500 train loss=0.002140888 valid loss= 0.003280433\n",
      "Epoch: 13000 train loss=0.001945376 valid loss= 0.003088916\n",
      "Epoch: 13500 train loss=0.003868006 valid loss= 0.003137514\n",
      "Epoch: 14000 train loss=0.002674994 valid loss= 0.003225866\n",
      "Epoch: 14500 train loss=0.001705013 valid loss= 0.003277219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:23:05,426]\u001b[0m Trial 33 finished with value: 0.0018448243265607683 and parameters: {'lam': 0.0017471406360145682, 'learning_rate': 0.15426719800159067, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.002055667 valid loss= 0.003010665\n",
      "Optimization Finished!\n",
      "test loss: 0.0029742198530584574, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0018448243265607683\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.013021744 valid loss= 0.006634592\n",
      "Epoch: 1000 train loss=0.005454311 valid loss= 0.004781383\n",
      "Epoch: 1500 train loss=0.002955123 valid loss= 0.004744596\n",
      "Epoch: 2000 train loss=0.006209900 valid loss= 0.004755818\n",
      "Epoch: 2500 train loss=0.002722207 valid loss= 0.005306396\n",
      "Epoch: 3000 train loss=0.007083802 valid loss= 0.005397701\n",
      "Epoch: 3500 train loss=0.002178715 valid loss= 0.005161074\n",
      "Epoch: 4000 train loss=0.002634447 valid loss= 0.005382789\n",
      "Epoch: 4500 train loss=0.003420833 valid loss= 0.005068064\n",
      "Epoch: 5000 train loss=0.003661124 valid loss= 0.005191293\n",
      "Epoch: 5500 train loss=0.003899339 valid loss= 0.004621822\n",
      "Epoch: 6000 train loss=0.004362685 valid loss= 0.005024678\n",
      "Epoch: 6500 train loss=0.006053170 valid loss= 0.004994896\n",
      "Epoch: 7000 train loss=0.001835695 valid loss= 0.004333192\n",
      "Epoch: 7500 train loss=0.002075632 valid loss= 0.003558686\n",
      "Epoch: 8000 train loss=0.005721973 valid loss= 0.003698461\n",
      "Epoch: 8500 train loss=0.001569934 valid loss= 0.003384167\n",
      "Epoch: 9000 train loss=0.006435801 valid loss= 0.003084483\n",
      "Epoch: 9500 train loss=0.001471409 valid loss= 0.003464413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:24:42,559]\u001b[0m Trial 34 finished with value: 0.0026196713202162043 and parameters: {'lam': 0.0017556546687729636, 'learning_rate': 0.14920393824703015, 'num_epoch': 10000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10000 train loss=0.001833215 valid loss= 0.003678379\n",
      "Optimization Finished!\n",
      "test loss: 0.003309396095573902, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0026196713202162043\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.006849448 valid loss= 0.008420152\n",
      "Epoch: 1000 train loss=0.013593437 valid loss= 0.008136569\n",
      "Epoch: 1500 train loss=0.010370291 valid loss= 0.007329558\n",
      "Epoch: 2000 train loss=0.015155931 valid loss= 0.006946701\n",
      "Epoch: 2500 train loss=0.005780003 valid loss= 0.006872150\n",
      "Epoch: 3000 train loss=0.010343659 valid loss= 0.006997689\n",
      "Epoch: 3500 train loss=0.005666650 valid loss= 0.006312596\n",
      "Epoch: 4000 train loss=0.014901514 valid loss= 0.007032539\n",
      "Epoch: 4500 train loss=0.007813152 valid loss= 0.006644269\n",
      "Epoch: 5000 train loss=0.005950444 valid loss= 0.006027846\n",
      "Epoch: 5500 train loss=0.010738288 valid loss= 0.005999307\n",
      "Epoch: 6000 train loss=0.005498882 valid loss= 0.005425596\n",
      "Epoch: 6500 train loss=0.008018829 valid loss= 0.005189131\n",
      "Epoch: 7000 train loss=0.007211779 valid loss= 0.004877083\n",
      "Epoch: 7500 train loss=0.005876209 valid loss= 0.004484943\n",
      "Epoch: 8000 train loss=0.004274923 valid loss= 0.003933849\n",
      "Epoch: 8500 train loss=0.003285356 valid loss= 0.004135951\n",
      "Epoch: 9000 train loss=0.007064946 valid loss= 0.004036947\n",
      "Epoch: 9500 train loss=0.003056557 valid loss= 0.003825629\n",
      "Epoch: 10000 train loss=0.003087595 valid loss= 0.003877482\n",
      "Epoch: 10500 train loss=0.004984841 valid loss= 0.003804736\n",
      "Epoch: 11000 train loss=0.005278685 valid loss= 0.003565673\n",
      "Epoch: 11500 train loss=0.005391143 valid loss= 0.003709985\n",
      "Epoch: 12000 train loss=0.001910147 valid loss= 0.003537507\n",
      "Epoch: 12500 train loss=0.002280577 valid loss= 0.003804771\n",
      "Epoch: 13000 train loss=0.002426669 valid loss= 0.003475066\n",
      "Epoch: 13500 train loss=0.001923856 valid loss= 0.003542962\n",
      "Epoch: 14000 train loss=0.001383671 valid loss= 0.003653836\n",
      "Epoch: 14500 train loss=0.002125250 valid loss= 0.003726205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:27:08,144]\u001b[0m Trial 35 finished with value: 0.0024549166396218704 and parameters: {'lam': 0.001357045395773397, 'learning_rate': 0.03275747633501483, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.001994097 valid loss= 0.003519308\n",
      "Optimization Finished!\n",
      "test loss: 0.003930684179067612, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0024549166396218704\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.017924149 valid loss= 0.007877795\n",
      "Epoch: 1000 train loss=0.014663812 valid loss= 0.006261642\n",
      "Epoch: 1500 train loss=0.005067640 valid loss= 0.004220570\n",
      "Epoch: 2000 train loss=0.009606488 valid loss= 0.004155038\n",
      "Epoch: 2500 train loss=0.002595449 valid loss= 0.004035358\n",
      "Epoch: 3000 train loss=0.002927489 valid loss= 0.004205240\n",
      "Epoch: 3500 train loss=0.005746202 valid loss= 0.004369373\n",
      "Epoch: 4000 train loss=0.002948371 valid loss= 0.004592422\n",
      "Epoch: 4500 train loss=0.003721616 valid loss= 0.004424941\n",
      "Epoch: 5000 train loss=0.002812127 valid loss= 0.004555452\n",
      "Epoch: 5500 train loss=0.004807163 valid loss= 0.004722724\n",
      "Epoch: 6000 train loss=0.002783731 valid loss= 0.004509056\n",
      "Epoch: 6500 train loss=0.007334653 valid loss= 0.004658293\n",
      "Epoch: 7000 train loss=0.005854218 valid loss= 0.004883746\n",
      "Epoch: 7500 train loss=0.003396302 valid loss= 0.004739339\n",
      "Epoch: 8000 train loss=0.002669744 valid loss= 0.004864332\n",
      "Epoch: 8500 train loss=0.002853984 valid loss= 0.004697837\n",
      "Epoch: 9000 train loss=0.005654490 valid loss= 0.004868395\n",
      "Epoch: 9500 train loss=0.003237354 valid loss= 0.004655058\n",
      "Epoch: 10000 train loss=0.006164410 valid loss= 0.004835938\n",
      "Epoch: 10500 train loss=0.003328410 valid loss= 0.004836463\n",
      "Epoch: 11000 train loss=0.002665743 valid loss= 0.004677721\n",
      "Epoch: 11500 train loss=0.003215585 valid loss= 0.004622608\n",
      "Epoch: 12000 train loss=0.002388694 valid loss= 0.004611391\n",
      "Epoch: 12500 train loss=0.010860647 valid loss= 0.004505182\n",
      "Epoch: 13000 train loss=0.003917652 valid loss= 0.004591127\n",
      "Epoch: 13500 train loss=0.002115526 valid loss= 0.004849726\n",
      "Epoch: 14000 train loss=0.002981613 valid loss= 0.004601922\n",
      "Epoch: 14500 train loss=0.003851795 valid loss= 0.004754866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:29:33,248]\u001b[0m Trial 36 finished with value: 0.0030225942527012206 and parameters: {'lam': 0.0021765411563933086, 'learning_rate': 0.07323703986485094, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.002353789 valid loss= 0.004519858\n",
      "Optimization Finished!\n",
      "test loss: 0.004916359204798937, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0030225942527012206\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.007497773 valid loss= 0.008322340\n",
      "Epoch: 1000 train loss=0.005982928 valid loss= 0.007759021\n",
      "Epoch: 1500 train loss=0.010801452 valid loss= 0.007975627\n",
      "Epoch: 2000 train loss=0.011980163 valid loss= 0.005504754\n",
      "Epoch: 2500 train loss=0.007223813 valid loss= 0.004264408\n",
      "Epoch: 3000 train loss=0.003114247 valid loss= 0.004206066\n",
      "Epoch: 3500 train loss=0.003124867 valid loss= 0.004181112\n",
      "Epoch: 4000 train loss=0.003015239 valid loss= 0.003973703\n",
      "Epoch: 4500 train loss=0.004132085 valid loss= 0.004325611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:30:22,642]\u001b[0m Trial 37 finished with value: 0.003315278945754007 and parameters: {'lam': 0.0017191385002024607, 'learning_rate': 0.09555449353596825, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.007715529 valid loss= 0.004625295\n",
      "Optimization Finished!\n",
      "test loss: 0.0045279767364263535, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.003315278945754007\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.014456810 valid loss= 0.007703324\n",
      "Epoch: 1000 train loss=0.018386671 valid loss= 0.006886587\n",
      "Epoch: 1500 train loss=0.013655172 valid loss= 0.007164861\n",
      "Epoch: 2000 train loss=0.022067299 valid loss= 0.006629315\n",
      "Epoch: 2500 train loss=0.005696778 valid loss= 0.006830679\n",
      "Epoch: 3000 train loss=0.008235014 valid loss= 0.006669077\n",
      "Epoch: 3500 train loss=0.007748945 valid loss= 0.006282439\n",
      "Epoch: 4000 train loss=0.009535342 valid loss= 0.006328899\n",
      "Epoch: 4500 train loss=0.005595324 valid loss= 0.005737313\n",
      "Epoch: 5000 train loss=0.012062239 valid loss= 0.005571841\n",
      "Epoch: 5500 train loss=0.005287821 valid loss= 0.005606648\n",
      "Epoch: 6000 train loss=0.005181834 valid loss= 0.005328514\n",
      "Epoch: 6500 train loss=0.003972578 valid loss= 0.004799832\n",
      "Epoch: 7000 train loss=0.006313623 valid loss= 0.004609584\n",
      "Epoch: 7500 train loss=0.004249576 valid loss= 0.004782336\n",
      "Epoch: 8000 train loss=0.006672330 valid loss= 0.004134873\n",
      "Epoch: 8500 train loss=0.005179667 valid loss= 0.004018139\n",
      "Epoch: 9000 train loss=0.011950449 valid loss= 0.004210089\n",
      "Epoch: 9500 train loss=0.008029820 valid loss= 0.003748670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:32:01,688]\u001b[0m Trial 38 finished with value: 0.0026766570394578834 and parameters: {'lam': 0.001267744716990869, 'learning_rate': 0.02858398381030369, 'num_epoch': 10000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10000 train loss=0.004264617 valid loss= 0.003744535\n",
      "Optimization Finished!\n",
      "test loss: 0.004264966119080782, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0026766570394578834\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.013488506 valid loss= 0.009478193\n",
      "Epoch: 1000 train loss=0.026685394 valid loss= 0.008828673\n",
      "Epoch: 1500 train loss=0.025322216 valid loss= 0.008611329\n",
      "Epoch: 2000 train loss=0.018532772 valid loss= 0.007923762\n",
      "Epoch: 2500 train loss=0.012566654 valid loss= 0.007644537\n",
      "Epoch: 3000 train loss=0.006438002 valid loss= 0.007465512\n",
      "Epoch: 3500 train loss=0.009514209 valid loss= 0.007193908\n",
      "Epoch: 4000 train loss=0.012581044 valid loss= 0.006788456\n",
      "Epoch: 4500 train loss=0.010483973 valid loss= 0.006418698\n",
      "Epoch: 5000 train loss=0.007142226 valid loss= 0.006142834\n",
      "Epoch: 5500 train loss=0.010683955 valid loss= 0.005731753\n",
      "Epoch: 6000 train loss=0.005118461 valid loss= 0.005836200\n",
      "Epoch: 6500 train loss=0.006722604 valid loss= 0.005046379\n",
      "Epoch: 7000 train loss=0.004701177 valid loss= 0.004910544\n",
      "Epoch: 7500 train loss=0.005402724 valid loss= 0.004800563\n",
      "Epoch: 8000 train loss=0.007258860 valid loss= 0.004822865\n",
      "Epoch: 8500 train loss=0.005010943 valid loss= 0.004443500\n",
      "Epoch: 9000 train loss=0.004933162 valid loss= 0.004603078\n",
      "Epoch: 9500 train loss=0.004421880 valid loss= 0.004641620\n",
      "Epoch: 10000 train loss=0.003373530 valid loss= 0.004654623\n",
      "Epoch: 10500 train loss=0.003623555 valid loss= 0.004383669\n",
      "Epoch: 11000 train loss=0.003246223 valid loss= 0.004621406\n",
      "Epoch: 11500 train loss=0.003144707 valid loss= 0.004355802\n",
      "Epoch: 12000 train loss=0.003311262 valid loss= 0.004430253\n",
      "Epoch: 12500 train loss=0.004065768 valid loss= 0.004299540\n",
      "Epoch: 13000 train loss=0.003550247 valid loss= 0.004406002\n",
      "Epoch: 13500 train loss=0.004601012 valid loss= 0.004191435\n",
      "Epoch: 14000 train loss=0.003910621 valid loss= 0.004469825\n",
      "Epoch: 14500 train loss=0.003226745 valid loss= 0.004306213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:34:27,938]\u001b[0m Trial 39 finished with value: 0.0026074097400115696 and parameters: {'lam': 0.0023845325751646845, 'learning_rate': 0.019325259817381507, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.006486155 valid loss= 0.004312655\n",
      "Optimization Finished!\n",
      "test loss: 0.003977583255618811, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0026074097400115696\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.012946052 valid loss= 0.007942867\n",
      "Epoch: 1000 train loss=0.004629719 valid loss= 0.008047061\n",
      "Epoch: 1500 train loss=0.006596347 valid loss= 0.005032710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:34:49,664]\u001b[0m Trial 40 finished with value: 0.0026193732084731983 and parameters: {'lam': 0.002868515652765328, 'learning_rate': 0.12220669638616488, 'num_epoch': 2000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 train loss=0.005607718 valid loss= 0.005105925\n",
      "Optimization Finished!\n",
      "test loss: 0.005822150968015194, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0026193732084731983\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.015778551 valid loss= 0.013110286\n",
      "Epoch: 1000 train loss=0.010245830 valid loss= 0.010963498\n",
      "Epoch: 1500 train loss=0.008743423 valid loss= 0.010148030\n",
      "Epoch: 2000 train loss=0.006155300 valid loss= 0.008847561\n",
      "Epoch: 2500 train loss=0.006387585 valid loss= 0.008123199\n",
      "Epoch: 3000 train loss=0.005908377 valid loss= 0.007898353\n",
      "Epoch: 3500 train loss=0.005634235 valid loss= 0.007680184\n",
      "Epoch: 4000 train loss=0.005913765 valid loss= 0.007655884\n",
      "Epoch: 4500 train loss=0.007820016 valid loss= 0.007396244\n",
      "Epoch: 5000 train loss=0.008586064 valid loss= 0.007467674\n",
      "Epoch: 5500 train loss=0.007867662 valid loss= 0.007381625\n",
      "Epoch: 6000 train loss=0.007179300 valid loss= 0.007269271\n",
      "Epoch: 6500 train loss=0.007050575 valid loss= 0.007360990\n",
      "Epoch: 7000 train loss=0.005467308 valid loss= 0.007209863\n",
      "Epoch: 7500 train loss=0.006254127 valid loss= 0.006974556\n",
      "Epoch: 8000 train loss=0.005615849 valid loss= 0.007750166\n",
      "Epoch: 8500 train loss=0.005630076 valid loss= 0.007340244\n",
      "Epoch: 9000 train loss=0.005476218 valid loss= 0.007104399\n",
      "Epoch: 9500 train loss=0.006127349 valid loss= 0.007500152\n",
      "Epoch: 10000 train loss=0.005556840 valid loss= 0.007284476\n",
      "Epoch: 10500 train loss=0.016006783 valid loss= 0.007290754\n",
      "Epoch: 11000 train loss=0.005985162 valid loss= 0.007374640\n",
      "Epoch: 11500 train loss=0.008458619 valid loss= 0.007207288\n",
      "Epoch: 12000 train loss=0.008211259 valid loss= 0.007188730\n",
      "Epoch: 12500 train loss=0.005769192 valid loss= 0.007498057\n",
      "Epoch: 13000 train loss=0.005697388 valid loss= 0.007359706\n",
      "Epoch: 13500 train loss=0.007963642 valid loss= 0.007507214\n",
      "Epoch: 14000 train loss=0.005428768 valid loss= 0.007014690\n",
      "Epoch: 14500 train loss=0.007863484 valid loss= 0.007171979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:37:15,098]\u001b[0m Trial 41 finished with value: 0.0022647730181837855 and parameters: {'lam': 0.008483738868331607, 'learning_rate': 0.1462354657389409, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.008225873 valid loss= 0.007192951\n",
      "Optimization Finished!\n",
      "test loss: 0.007066588848829269, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0022647730181837855\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.019970410 valid loss= 0.013727374\n",
      "Epoch: 1000 train loss=0.012642169 valid loss= 0.010141970\n",
      "Epoch: 1500 train loss=0.013638794 valid loss= 0.008568846\n",
      "Epoch: 2000 train loss=0.007255496 valid loss= 0.008160969\n",
      "Epoch: 2500 train loss=0.008137520 valid loss= 0.007510724\n",
      "Epoch: 3000 train loss=0.009758566 valid loss= 0.008022849\n",
      "Epoch: 3500 train loss=0.006908744 valid loss= 0.006989552\n",
      "Epoch: 4000 train loss=0.007379430 valid loss= 0.007776597\n",
      "Epoch: 4500 train loss=0.007713201 valid loss= 0.007303930\n",
      "Epoch: 5000 train loss=0.006390291 valid loss= 0.007179729\n",
      "Epoch: 5500 train loss=0.006218342 valid loss= 0.007340918\n",
      "Epoch: 6000 train loss=0.006444758 valid loss= 0.007398357\n",
      "Epoch: 6500 train loss=0.006091156 valid loss= 0.007970821\n",
      "Epoch: 7000 train loss=0.007767828 valid loss= 0.007137704\n",
      "Epoch: 7500 train loss=0.007282899 valid loss= 0.008323488\n",
      "Epoch: 8000 train loss=0.006140138 valid loss= 0.007502533\n",
      "Epoch: 8500 train loss=0.008204853 valid loss= 0.007848158\n",
      "Epoch: 9000 train loss=0.005843510 valid loss= 0.007347273\n",
      "Epoch: 9500 train loss=0.006167018 valid loss= 0.007867171\n",
      "Epoch: 10000 train loss=0.005990733 valid loss= 0.007373333\n",
      "Epoch: 10500 train loss=0.005884994 valid loss= 0.008011209\n",
      "Epoch: 11000 train loss=0.005906212 valid loss= 0.007476470\n",
      "Epoch: 11500 train loss=0.006009520 valid loss= 0.007147701\n",
      "Epoch: 12000 train loss=0.006812868 valid loss= 0.007542762\n",
      "Epoch: 12500 train loss=0.008180332 valid loss= 0.007108198\n",
      "Epoch: 13000 train loss=0.005658784 valid loss= 0.007531979\n",
      "Epoch: 13500 train loss=0.011279921 valid loss= 0.007182818\n",
      "Epoch: 14000 train loss=0.010085268 valid loss= 0.007069272\n",
      "Epoch: 14500 train loss=0.009158755 valid loss= 0.007496707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:39:41,203]\u001b[0m Trial 42 finished with value: 0.0011008228390562336 and parameters: {'lam': 0.009921392308655583, 'learning_rate': 0.1352479773812429, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.008199100 valid loss= 0.006910402\n",
      "Optimization Finished!\n",
      "test loss: 0.00659641157835722, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0011008228390562336\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.021783378 valid loss= 0.013636946\n",
      "Epoch: 1000 train loss=0.013271384 valid loss= 0.013953272\n",
      "Epoch: 1500 train loss=0.015770797 valid loss= 0.013403270\n",
      "Epoch: 2000 train loss=0.020615855 valid loss= 0.014371740\n",
      "Epoch: 2500 train loss=0.017090818 valid loss= 0.012473964\n",
      "Epoch: 3000 train loss=0.010458801 valid loss= 0.010460472\n",
      "Epoch: 3500 train loss=0.011044882 valid loss= 0.009368286\n",
      "Epoch: 4000 train loss=0.012087185 valid loss= 0.009186435\n",
      "Epoch: 4500 train loss=0.009774348 valid loss= 0.008893728\n",
      "Epoch: 5000 train loss=0.013116091 valid loss= 0.009008087\n",
      "Epoch: 5500 train loss=0.006968446 valid loss= 0.009392190\n",
      "Epoch: 6000 train loss=0.008077928 valid loss= 0.008961584\n",
      "Epoch: 6500 train loss=0.007494773 valid loss= 0.009148617\n",
      "Epoch: 7000 train loss=0.006697544 valid loss= 0.008996818\n",
      "Epoch: 7500 train loss=0.008468227 valid loss= 0.008770276\n",
      "Epoch: 8000 train loss=0.008602582 valid loss= 0.008776993\n",
      "Epoch: 8500 train loss=0.011433633 valid loss= 0.008136330\n",
      "Epoch: 9000 train loss=0.006245520 valid loss= 0.007834130\n",
      "Epoch: 9500 train loss=0.005671999 valid loss= 0.007803658\n",
      "Epoch: 10000 train loss=0.005915606 valid loss= 0.007685682\n",
      "Epoch: 10500 train loss=0.019378304 valid loss= 0.007742704\n",
      "Epoch: 11000 train loss=0.011437891 valid loss= 0.007484631\n",
      "Epoch: 11500 train loss=0.010192754 valid loss= 0.007387885\n",
      "Epoch: 12000 train loss=0.011241084 valid loss= 0.007154864\n",
      "Epoch: 12500 train loss=0.008638272 valid loss= 0.007447870\n",
      "Epoch: 13000 train loss=0.008809822 valid loss= 0.007307946\n",
      "Epoch: 13500 train loss=0.005372364 valid loss= 0.007246778\n",
      "Epoch: 14000 train loss=0.005718567 valid loss= 0.007229273\n",
      "Epoch: 14500 train loss=0.006169356 valid loss= 0.007072152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:42:08,332]\u001b[0m Trial 43 finished with value: 0.0022628738589869934 and parameters: {'lam': 0.008571172246865376, 'learning_rate': 0.06625654661822364, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.005759509 valid loss= 0.007064495\n",
      "Optimization Finished!\n",
      "test loss: 0.006904967594891787, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0022628738589869934\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.031787585 valid loss= 0.013821216\n",
      "Epoch: 1000 train loss=0.009905010 valid loss= 0.010350809\n",
      "Epoch: 1500 train loss=0.006695876 valid loss= 0.009856695\n",
      "Epoch: 2000 train loss=0.008723197 valid loss= 0.008722972\n",
      "Epoch: 2500 train loss=0.007173869 valid loss= 0.008194751\n",
      "Epoch: 3000 train loss=0.007415871 valid loss= 0.008375416\n",
      "Epoch: 3500 train loss=0.006438202 valid loss= 0.008006574\n",
      "Epoch: 4000 train loss=0.008180902 valid loss= 0.008441541\n",
      "Epoch: 4500 train loss=0.006284314 valid loss= 0.007654047\n",
      "Epoch: 5000 train loss=0.006913150 valid loss= 0.007867931\n",
      "Epoch: 5500 train loss=0.007328649 valid loss= 0.008038135\n",
      "Epoch: 6000 train loss=0.006594220 valid loss= 0.007932316\n",
      "Epoch: 6500 train loss=0.007381754 valid loss= 0.007872868\n",
      "Epoch: 7000 train loss=0.005544486 valid loss= 0.007975262\n",
      "Epoch: 7500 train loss=0.006201787 valid loss= 0.007318809\n",
      "Epoch: 8000 train loss=0.008037919 valid loss= 0.007545959\n",
      "Epoch: 8500 train loss=0.006612195 valid loss= 0.007626363\n",
      "Epoch: 9000 train loss=0.008083473 valid loss= 0.007368940\n",
      "Epoch: 9500 train loss=0.007000986 valid loss= 0.007079720\n",
      "Epoch: 10000 train loss=0.005940380 valid loss= 0.007244308\n",
      "Epoch: 10500 train loss=0.005889900 valid loss= 0.007031101\n",
      "Epoch: 11000 train loss=0.005278062 valid loss= 0.007167480\n",
      "Epoch: 11500 train loss=0.006636932 valid loss= 0.007063934\n",
      "Epoch: 12000 train loss=0.007651268 valid loss= 0.007348962\n",
      "Epoch: 12500 train loss=0.005778436 valid loss= 0.006800270\n",
      "Epoch: 13000 train loss=0.005373252 valid loss= 0.007487550\n",
      "Epoch: 13500 train loss=0.008529647 valid loss= 0.007266302\n",
      "Epoch: 14000 train loss=0.006839599 valid loss= 0.007336324\n",
      "Epoch: 14500 train loss=0.007401350 valid loss= 0.007092135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:44:33,825]\u001b[0m Trial 44 finished with value: 0.0025576465112892256 and parameters: {'lam': 0.009540365363534834, 'learning_rate': 0.1658392369694828, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.005612239 valid loss= 0.007432520\n",
      "Optimization Finished!\n",
      "test loss: 0.007044208236038685, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0025576465112892256\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.006585124 valid loss= 0.007831648\n",
      "Epoch: 1000 train loss=0.007266392 valid loss= 0.007319142\n",
      "Epoch: 1500 train loss=0.003589981 valid loss= 0.004738924\n",
      "Epoch: 2000 train loss=0.006002361 valid loss= 0.003803311\n",
      "Epoch: 2500 train loss=0.004414754 valid loss= 0.004274029\n",
      "Epoch: 3000 train loss=0.005529798 valid loss= 0.004043234\n",
      "Epoch: 3500 train loss=0.002581503 valid loss= 0.004005878\n",
      "Epoch: 4000 train loss=0.003863783 valid loss= 0.003794523\n",
      "Epoch: 4500 train loss=0.001976908 valid loss= 0.003577973\n",
      "Epoch: 5000 train loss=0.003559425 valid loss= 0.003361959\n",
      "Epoch: 5500 train loss=0.001930917 valid loss= 0.003410761\n",
      "Epoch: 1000 train loss=0.015816895 valid loss= 0.016497470\n",
      "Epoch: 1500 train loss=0.020183418 valid loss= 0.016888734\n",
      "Epoch: 2000 train loss=0.012213031 valid loss= 0.015838495\n",
      "Epoch: 2500 train loss=0.017462861 valid loss= 0.015541997\n",
      "Epoch: 3000 train loss=0.015742915 valid loss= 0.014771529\n",
      "Epoch: 3500 train loss=0.011754172 valid loss= 0.014412692\n",
      "Epoch: 4000 train loss=0.008512671 valid loss= 0.014164329\n",
      "Epoch: 4500 train loss=0.010000220 valid loss= 0.014291647\n",
      "Epoch: 5000 train loss=0.013768231 valid loss= 0.013887160\n",
      "Epoch: 5500 train loss=0.010625321 valid loss= 0.012898240\n",
      "Epoch: 6000 train loss=0.014846323 valid loss= 0.012284786\n",
      "Epoch: 6500 train loss=0.009004406 valid loss= 0.012201076\n",
      "Epoch: 7000 train loss=0.010923896 valid loss= 0.011249086\n",
      "Epoch: 7500 train loss=0.007466038 valid loss= 0.009947182\n",
      "Epoch: 8000 train loss=0.007058688 valid loss= 0.009225654\n",
      "Epoch: 8500 train loss=0.011018805 valid loss= 0.009137763\n",
      "Epoch: 9000 train loss=0.008972690 valid loss= 0.008881970\n",
      "Epoch: 9500 train loss=0.007069825 valid loss= 0.008553182\n",
      "Epoch: 10000 train loss=0.007875088 valid loss= 0.008629777\n",
      "Epoch: 10500 train loss=0.006972618 valid loss= 0.008497205\n",
      "Epoch: 11000 train loss=0.007546538 valid loss= 0.008437238\n",
      "Epoch: 11500 train loss=0.007249365 valid loss= 0.008832177\n",
      "Epoch: 12000 train loss=0.006260477 valid loss= 0.008642633\n",
      "Epoch: 12500 train loss=0.007673535 valid loss= 0.008360329\n",
      "Epoch: 13000 train loss=0.008194737 valid loss= 0.008347417\n",
      "Epoch: 13500 train loss=0.011260551 valid loss= 0.008248243\n",
      "Epoch: 14000 train loss=0.006404426 valid loss= 0.008146765\n",
      "Epoch: 14500 train loss=0.006829170 valid loss= 0.008520726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:49:23,041]\u001b[0m Trial 46 finished with value: 0.0025847759224374104 and parameters: {'lam': 0.009966305032087262, 'learning_rate': 0.053217851824572275, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.008654943 valid loss= 0.008518269\n",
      "Optimization Finished!\n",
      "test loss: 0.008232383988797665, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0025847759224374104\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.009663748 valid loss= 0.006977196\n",
      "Epoch: 1000 train loss=0.008436382 valid loss= 0.006124140\n",
      "Epoch: 1500 train loss=0.004364314 valid loss= 0.003948086\n",
      "Epoch: 2000 train loss=0.003939365 valid loss= 0.003586687\n",
      "Epoch: 2500 train loss=0.002875574 valid loss= 0.003532084\n",
      "Epoch: 3000 train loss=0.002198114 valid loss= 0.003770177\n",
      "Epoch: 3500 train loss=0.006781306 valid loss= 0.003975988\n",
      "Epoch: 4000 train loss=0.003208122 valid loss= 0.004362484\n",
      "Epoch: 4500 train loss=0.005296626 valid loss= 0.003948449\n",
      "Epoch: 5000 train loss=0.003522715 valid loss= 0.003529523\n",
      "Epoch: 5500 train loss=0.003323525 valid loss= 0.003440303\n",
      "Epoch: 6000 train loss=0.004863049 valid loss= 0.003413985\n",
      "Epoch: 6500 train loss=0.004649431 valid loss= 0.003482053\n",
      "Epoch: 7000 train loss=0.002554869 valid loss= 0.003324513\n",
      "Epoch: 7500 train loss=0.002244750 valid loss= 0.003408400\n",
      "Epoch: 8000 train loss=0.007354424 valid loss= 0.003287124\n",
      "Epoch: 8500 train loss=0.006912990 valid loss= 0.003651326\n",
      "Epoch: 9000 train loss=0.002682625 valid loss= 0.003695609\n",
      "Epoch: 9500 train loss=0.001688185 valid loss= 0.003802042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:51:00,996]\u001b[0m Trial 47 finished with value: 0.0023143926449204484 and parameters: {'lam': 0.0019498740190403237, 'learning_rate': 0.08831406659970581, 'num_epoch': 10000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10000 train loss=0.004060100 valid loss= 0.003614092\n",
      "Optimization Finished!\n",
      "test loss: 0.003492704825475812, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0023143926449204484\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.014537715 valid loss= 0.008895438\n",
      "Epoch: 1000 train loss=0.009104900 valid loss= 0.009788316\n",
      "Epoch: 1500 train loss=0.010801183 valid loss= 0.008404520\n",
      "Epoch: 2000 train loss=0.007075557 valid loss= 0.006320396\n",
      "Epoch: 2500 train loss=0.005671722 valid loss= 0.005902480\n",
      "Epoch: 3000 train loss=0.004928995 valid loss= 0.006106116\n",
      "Epoch: 3500 train loss=0.005474402 valid loss= 0.006023851\n",
      "Epoch: 4000 train loss=0.003915029 valid loss= 0.005934329\n",
      "Epoch: 4500 train loss=0.003613113 valid loss= 0.005872438\n",
      "Epoch: 5000 train loss=0.003610715 valid loss= 0.005721216\n",
      "Epoch: 5500 train loss=0.003326781 valid loss= 0.005629698\n",
      "Epoch: 6000 train loss=0.005299417 valid loss= 0.005795838\n",
      "Epoch: 6500 train loss=0.006349131 valid loss= 0.005604878\n",
      "Epoch: 7000 train loss=0.004089228 valid loss= 0.005952709\n",
      "Epoch: 7500 train loss=0.004320178 valid loss= 0.005775750\n",
      "Epoch: 8000 train loss=0.004095973 valid loss= 0.005662273\n",
      "Epoch: 8500 train loss=0.004533680 valid loss= 0.005462733\n",
      "Epoch: 9000 train loss=0.003488660 valid loss= 0.005596436\n",
      "Epoch: 9500 train loss=0.003582173 valid loss= 0.005680904\n",
      "Epoch: 10000 train loss=0.002977766 valid loss= 0.005368490\n",
      "Epoch: 10500 train loss=0.004718264 valid loss= 0.005356562\n",
      "Epoch: 11000 train loss=0.003169794 valid loss= 0.005567904\n",
      "Epoch: 11500 train loss=0.004601472 valid loss= 0.005295579\n",
      "Epoch: 12000 train loss=0.004066096 valid loss= 0.005671621\n",
      "Epoch: 12500 train loss=0.003967938 valid loss= 0.005613581\n",
      "Epoch: 13000 train loss=0.002921963 valid loss= 0.005993423\n",
      "Epoch: 13500 train loss=0.003577530 valid loss= 0.005567184\n",
      "Epoch: 14000 train loss=0.002900361 valid loss= 0.005679435\n",
      "Epoch: 14500 train loss=0.002596597 valid loss= 0.005881938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:53:27,701]\u001b[0m Trial 48 finished with value: 0.0036035969424531122 and parameters: {'lam': 0.003475090036542762, 'learning_rate': 0.13062013666208802, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.004180542 valid loss= 0.005823776\n",
      "Optimization Finished!\n",
      "test loss: 0.006356814876198769, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0036035969424531122\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.013188347 valid loss= 0.008009449\n",
      "Epoch: 1000 train loss=0.008803862 valid loss= 0.006862346\n",
      "Epoch: 1500 train loss=0.012124224 valid loss= 0.006140588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:53:49,241]\u001b[0m Trial 49 finished with value: 0.0037849464364043406 and parameters: {'lam': 0.002218701335019274, 'learning_rate': 0.03745438253849647, 'num_epoch': 2000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 train loss=0.018124042 valid loss= 0.005680516\n",
      "Optimization Finished!\n",
      "test loss: 0.006655707489699125, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0037849464364043406\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.019298825 valid loss= 0.014356786\n",
      "Epoch: 1000 train loss=0.016958676 valid loss= 0.014180198\n",
      "Epoch: 1500 train loss=0.024418898 valid loss= 0.013997057\n",
      "Epoch: 2000 train loss=0.017771954 valid loss= 0.013977209\n",
      "Epoch: 2500 train loss=0.016692311 valid loss= 0.013552033\n",
      "Epoch: 3000 train loss=0.011299752 valid loss= 0.013384956\n",
      "Epoch: 3500 train loss=0.017591771 valid loss= 0.012854105\n",
      "Epoch: 4000 train loss=0.013066348 valid loss= 0.012818551\n",
      "Epoch: 4500 train loss=0.010125216 valid loss= 0.013189135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:54:39,985]\u001b[0m Trial 50 finished with value: 0.006642103952512521 and parameters: {'lam': 0.007945660057792223, 'learning_rate': 0.02329434461291691, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.009184106 valid loss= 0.013299392\n",
      "Optimization Finished!\n",
      "test loss: 0.012602335773408413, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.006642103952512521\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.012336923 valid loss= 0.012134764\n",
      "Epoch: 1000 train loss=0.013139792 valid loss= 0.009736177\n",
      "Epoch: 1500 train loss=0.013115694 valid loss= 0.008375807\n",
      "Epoch: 2000 train loss=0.008204545 valid loss= 0.008148011\n",
      "Epoch: 2500 train loss=0.008433166 valid loss= 0.007946077\n",
      "Epoch: 3000 train loss=0.007669813 valid loss= 0.007885985\n",
      "Epoch: 3500 train loss=0.005359133 valid loss= 0.008041022\n",
      "Epoch: 4000 train loss=0.006239275 valid loss= 0.007828451\n",
      "Epoch: 4500 train loss=0.006420374 valid loss= 0.007874263\n",
      "Epoch: 5000 train loss=0.004973805 valid loss= 0.007896922\n",
      "Epoch: 5500 train loss=0.005153630 valid loss= 0.007900530\n",
      "Epoch: 6000 train loss=0.006362215 valid loss= 0.008027040\n",
      "Epoch: 6500 train loss=0.005116314 valid loss= 0.007680623\n",
      "Epoch: 7000 train loss=0.004823794 valid loss= 0.008129104\n",
      "Epoch: 7500 train loss=0.004433880 valid loss= 0.007611342\n",
      "Epoch: 8000 train loss=0.003805041 valid loss= 0.007946517\n",
      "Epoch: 8500 train loss=0.004458013 valid loss= 0.007943029\n",
      "Epoch: 9000 train loss=0.004425195 valid loss= 0.007875567\n",
      "Epoch: 9500 train loss=0.004258987 valid loss= 0.007637762\n",
      "Epoch: 10000 train loss=0.004592356 valid loss= 0.007990476\n",
      "Epoch: 10500 train loss=0.004492732 valid loss= 0.007808717\n",
      "Epoch: 11000 train loss=0.007145007 valid loss= 0.007905609\n",
      "Epoch: 11500 train loss=0.003595468 valid loss= 0.007865924\n",
      "Epoch: 12000 train loss=0.004317102 valid loss= 0.007663678\n",
      "Epoch: 12500 train loss=0.004312972 valid loss= 0.007775460\n",
      "Epoch: 13000 train loss=0.003579065 valid loss= 0.007768610\n",
      "Epoch: 13500 train loss=0.004822286 valid loss= 0.008031445\n",
      "Epoch: 14000 train loss=0.004362125 valid loss= 0.007982174\n",
      "Epoch: 14500 train loss=0.008027650 valid loss= 0.007715458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:57:05,865]\u001b[0m Trial 51 finished with value: 0.004852880478439555 and parameters: {'lam': 0.006411184225726911, 'learning_rate': 0.1736839091131741, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.003838975 valid loss= 0.007847562\n",
      "Optimization Finished!\n",
      "test loss: 0.007820256054401398, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.004852880478439555\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.010499553 valid loss= 0.011887914\n",
      "Epoch: 1000 train loss=0.007579305 valid loss= 0.010635992\n",
      "Epoch: 1500 train loss=0.008611144 valid loss= 0.008464000\n",
      "Epoch: 2000 train loss=0.008736139 valid loss= 0.008745409\n",
      "Epoch: 2500 train loss=0.008067497 valid loss= 0.008652639\n",
      "Epoch: 3000 train loss=0.008199397 valid loss= 0.008186632\n",
      "Epoch: 3500 train loss=0.006194205 valid loss= 0.007624737\n",
      "Epoch: 4000 train loss=0.005354934 valid loss= 0.007610612\n",
      "Epoch: 4500 train loss=0.005827740 valid loss= 0.007816830\n",
      "Epoch: 5000 train loss=0.007529042 valid loss= 0.007295538\n",
      "Epoch: 5500 train loss=0.006577133 valid loss= 0.007246915\n",
      "Epoch: 6000 train loss=0.006437033 valid loss= 0.007410794\n",
      "Epoch: 6500 train loss=0.005804109 valid loss= 0.007389069\n",
      "Epoch: 7000 train loss=0.005165173 valid loss= 0.007346620\n",
      "Epoch: 7500 train loss=0.006757138 valid loss= 0.007009330\n",
      "Epoch: 8000 train loss=0.005913332 valid loss= 0.007113240\n",
      "Epoch: 8500 train loss=0.006233362 valid loss= 0.006961213\n",
      "Epoch: 9000 train loss=0.005630572 valid loss= 0.006829955\n",
      "Epoch: 9500 train loss=0.010108479 valid loss= 0.007040114\n",
      "Epoch: 10000 train loss=0.005646187 valid loss= 0.006912009\n",
      "Epoch: 10500 train loss=0.008822134 valid loss= 0.006796987\n",
      "Epoch: 11000 train loss=0.005393513 valid loss= 0.006900569\n",
      "Epoch: 11500 train loss=0.008859665 valid loss= 0.007273355\n",
      "Epoch: 12000 train loss=0.011802858 valid loss= 0.006900486\n",
      "Epoch: 12500 train loss=0.005873716 valid loss= 0.006860319\n",
      "Epoch: 13000 train loss=0.005523852 valid loss= 0.007369276\n",
      "Epoch: 13500 train loss=0.005492237 valid loss= 0.006855113\n",
      "Epoch: 14000 train loss=0.005177416 valid loss= 0.007190784\n",
      "Epoch: 14500 train loss=0.005857959 valid loss= 0.006962636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 05:59:32,959]\u001b[0m Trial 52 finished with value: 0.0022013093357894408 and parameters: {'lam': 0.007935854613548579, 'learning_rate': 0.13936367596980656, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.006715523 valid loss= 0.006820310\n",
      "Optimization Finished!\n",
      "test loss: 0.0066819097846746445, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0022013093357894408\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.012847114 valid loss= 0.010772245\n",
      "Epoch: 1000 train loss=0.009955997 valid loss= 0.008330380\n",
      "Epoch: 1500 train loss=0.006557416 valid loss= 0.007027799\n",
      "Epoch: 2000 train loss=0.004812331 valid loss= 0.005770339\n",
      "Epoch: 2500 train loss=0.007574199 valid loss= 0.006523333\n",
      "Epoch: 3000 train loss=0.005868063 valid loss= 0.005385973\n",
      "Epoch: 3500 train loss=0.004203285 valid loss= 0.004564520\n",
      "Epoch: 4000 train loss=0.006214007 valid loss= 0.004469640\n",
      "Epoch: 4500 train loss=0.008735302 valid loss= 0.004111408\n",
      "Epoch: 5000 train loss=0.004744301 valid loss= 0.005045217\n",
      "Epoch: 5500 train loss=0.003712628 valid loss= 0.004569364\n",
      "Epoch: 6000 train loss=0.004352382 valid loss= 0.004733561\n",
      "Epoch: 6500 train loss=0.003693925 valid loss= 0.004186796\n",
      "Epoch: 7000 train loss=0.006344274 valid loss= 0.003773956\n",
      "Epoch: 7500 train loss=0.008550158 valid loss= 0.004006388\n",
      "Epoch: 8000 train loss=0.003246994 valid loss= 0.003871872\n",
      "Epoch: 8500 train loss=0.003281064 valid loss= 0.004525565\n",
      "Epoch: 9000 train loss=0.003074049 valid loss= 0.003960663\n",
      "Epoch: 9500 train loss=0.003923219 valid loss= 0.004332177\n",
      "Epoch: 10000 train loss=0.004093676 valid loss= 0.004666080\n",
      "Epoch: 10500 train loss=0.003286199 valid loss= 0.004721583\n",
      "Epoch: 11000 train loss=0.003271225 valid loss= 0.004685026\n",
      "Epoch: 11500 train loss=0.004718289 valid loss= 0.004278024\n",
      "Epoch: 12000 train loss=0.008707523 valid loss= 0.004520555\n",
      "Epoch: 12500 train loss=0.004210722 valid loss= 0.003904289\n",
      "Epoch: 13000 train loss=0.004010689 valid loss= 0.004094948\n",
      "Epoch: 13500 train loss=0.003198076 valid loss= 0.004346711\n",
      "Epoch: 14000 train loss=0.003270395 valid loss= 0.004443268\n",
      "Epoch: 14500 train loss=0.003803050 valid loss= 0.003839435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:01:57,825]\u001b[0m Trial 53 finished with value: 0.0016332082614414108 and parameters: {'lam': 0.004759534099891216, 'learning_rate': 0.1608821342065827, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.002924858 valid loss= 0.004439321\n",
      "Optimization Finished!\n",
      "test loss: 0.004211264196783304, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0016332082614414108\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.011833101 valid loss= 0.010430590\n",
      "Epoch: 1000 train loss=0.008309884 valid loss= 0.009315942\n",
      "Epoch: 1500 train loss=0.006619344 valid loss= 0.006971376\n",
      "Epoch: 2000 train loss=0.007728448 valid loss= 0.006437256\n",
      "Epoch: 2500 train loss=0.011662031 valid loss= 0.005874507\n",
      "Epoch: 3000 train loss=0.003708945 valid loss= 0.005921574\n",
      "Epoch: 3500 train loss=0.003516829 valid loss= 0.005554733\n",
      "Epoch: 4000 train loss=0.004972392 valid loss= 0.005535622\n",
      "Epoch: 4500 train loss=0.003705917 valid loss= 0.005227943\n",
      "Epoch: 5000 train loss=0.005927274 valid loss= 0.005269194\n",
      "Epoch: 5500 train loss=0.003551014 valid loss= 0.005411316\n",
      "Epoch: 6000 train loss=0.003870667 valid loss= 0.005258708\n",
      "Epoch: 6500 train loss=0.007423462 valid loss= 0.005700499\n",
      "Epoch: 7000 train loss=0.004473100 valid loss= 0.005636780\n",
      "Epoch: 7500 train loss=0.004194976 valid loss= 0.005402472\n",
      "Epoch: 8000 train loss=0.004125225 valid loss= 0.005319224\n",
      "Epoch: 8500 train loss=0.007628017 valid loss= 0.006016006\n",
      "Epoch: 9000 train loss=0.004502614 valid loss= 0.005474885\n",
      "Epoch: 9500 train loss=0.006095794 valid loss= 0.005581921\n",
      "Epoch: 10000 train loss=0.006896393 valid loss= 0.005229756\n",
      "Epoch: 10500 train loss=0.006230497 valid loss= 0.005620550\n",
      "Epoch: 11000 train loss=0.012151847 valid loss= 0.005399044\n",
      "Epoch: 11500 train loss=0.004686673 valid loss= 0.005153396\n",
      "Epoch: 12000 train loss=0.004248083 valid loss= 0.005223382\n",
      "Epoch: 12500 train loss=0.004069422 valid loss= 0.005278440\n",
      "Epoch: 13000 train loss=0.004699043 valid loss= 0.005359079\n",
      "Epoch: 13500 train loss=0.003761882 valid loss= 0.005388574\n",
      "Epoch: 14000 train loss=0.004523968 valid loss= 0.005018568\n",
      "Epoch: 14500 train loss=0.003977865 valid loss= 0.005496758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:04:24,355]\u001b[0m Trial 54 finished with value: 0.0023190259126529725 and parameters: {'lam': 0.005291991012484391, 'learning_rate': 0.1594177129558715, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.003936946 valid loss= 0.005401527\n",
      "Optimization Finished!\n",
      "test loss: 0.005238860845565796, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0023190259126529725\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.017852627 valid loss= 0.014678037\n",
      "Epoch: 1000 train loss=0.012532899 valid loss= 0.010825747\n",
      "Epoch: 1500 train loss=0.011431651 valid loss= 0.008619204\n",
      "Epoch: 2000 train loss=0.007036734 valid loss= 0.007934533\n",
      "Epoch: 2500 train loss=0.010972301 valid loss= 0.007071099\n",
      "Epoch: 3000 train loss=0.006590364 valid loss= 0.006859916\n",
      "Epoch: 3500 train loss=0.007278906 valid loss= 0.007015660\n",
      "Epoch: 4000 train loss=0.006202594 valid loss= 0.007069635\n",
      "Epoch: 4500 train loss=0.006850582 valid loss= 0.006951124\n",
      "Epoch: 5000 train loss=0.009922737 valid loss= 0.006841692\n",
      "Epoch: 5500 train loss=0.008325763 valid loss= 0.007160495\n",
      "Epoch: 6000 train loss=0.006313413 valid loss= 0.006894272\n",
      "Epoch: 6500 train loss=0.005963138 valid loss= 0.007392829\n",
      "Epoch: 7000 train loss=0.005490052 valid loss= 0.007503774\n",
      "Epoch: 7500 train loss=0.005469614 valid loss= 0.006571335\n",
      "Epoch: 8000 train loss=0.005372905 valid loss= 0.006624800\n",
      "Epoch: 8500 train loss=0.006804108 valid loss= 0.006465989\n",
      "Epoch: 9000 train loss=0.007287970 valid loss= 0.006715547\n",
      "Epoch: 9500 train loss=0.005403805 valid loss= 0.006741027\n",
      "Epoch: 10000 train loss=0.006246182 valid loss= 0.006354828\n",
      "Epoch: 10500 train loss=0.005544083 valid loss= 0.007425723\n",
      "Epoch: 11000 train loss=0.007619329 valid loss= 0.006713350\n",
      "Epoch: 11500 train loss=0.005195622 valid loss= 0.007065935\n",
      "Epoch: 12000 train loss=0.007815800 valid loss= 0.006551726\n",
      "Epoch: 12500 train loss=0.005258319 valid loss= 0.007094319\n",
      "Epoch: 13000 train loss=0.006299128 valid loss= 0.006561563\n",
      "Epoch: 13500 train loss=0.008030060 valid loss= 0.006322547\n",
      "Epoch: 14000 train loss=0.017424501 valid loss= 0.007290064\n",
      "Epoch: 14500 train loss=0.010739168 valid loss= 0.007417442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:06:51,109]\u001b[0m Trial 55 finished with value: 0.0017906469519706164 and parameters: {'lam': 0.00906049862419588, 'learning_rate': 0.19628609442164488, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.005640860 valid loss= 0.007086732\n",
      "Optimization Finished!\n",
      "test loss: 0.006757482886314392, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0017906469519706164\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.008593928 valid loss= 0.009288548\n",
      "Epoch: 1000 train loss=0.006763103 valid loss= 0.008043267\n",
      "Epoch: 1500 train loss=0.006371202 valid loss= 0.005616515\n",
      "Epoch: 2000 train loss=0.006197510 valid loss= 0.005445115\n",
      "Epoch: 2500 train loss=0.006690039 valid loss= 0.005341914\n",
      "Epoch: 3000 train loss=0.003395808 valid loss= 0.005268811\n",
      "Epoch: 3500 train loss=0.004803032 valid loss= 0.005339614\n",
      "Epoch: 4000 train loss=0.004288350 valid loss= 0.005251496\n",
      "Epoch: 4500 train loss=0.002826295 valid loss= 0.005224323\n",
      "Epoch: 5000 train loss=0.002866619 valid loss= 0.005169977\n",
      "Epoch: 5500 train loss=0.004034649 valid loss= 0.005115345\n",
      "Epoch: 6000 train loss=0.003041405 valid loss= 0.005019309\n",
      "Epoch: 6500 train loss=0.002388017 valid loss= 0.005023587\n",
      "Epoch: 7000 train loss=0.002434719 valid loss= 0.005070660\n",
      "Epoch: 7500 train loss=0.004191127 valid loss= 0.005604019\n",
      "Epoch: 8000 train loss=0.003286516 valid loss= 0.005542043\n",
      "Epoch: 8500 train loss=0.010575366 valid loss= 0.005526979\n",
      "Epoch: 9000 train loss=0.003171609 valid loss= 0.005104670\n",
      "Epoch: 9500 train loss=0.003122194 valid loss= 0.005473139\n",
      "Epoch: 10000 train loss=0.004800305 valid loss= 0.005356284\n",
      "Epoch: 10500 train loss=0.002418474 valid loss= 0.005054375\n",
      "Epoch: 11000 train loss=0.003014239 valid loss= 0.005612481\n",
      "Epoch: 11500 train loss=0.005044305 valid loss= 0.005568868\n",
      "Epoch: 12000 train loss=0.002912807 valid loss= 0.005596410\n",
      "Epoch: 12500 train loss=0.002339529 valid loss= 0.005934848\n",
      "Epoch: 13000 train loss=0.002488144 valid loss= 0.006105460\n",
      "Epoch: 13500 train loss=0.002670625 valid loss= 0.005924139\n",
      "Epoch: 14000 train loss=0.002587637 valid loss= 0.005757829\n",
      "Epoch: 14500 train loss=0.003889494 valid loss= 0.005411278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:09:18,056]\u001b[0m Trial 56 finished with value: 0.004115921937982804 and parameters: {'lam': 0.0029022708611319793, 'learning_rate': 0.19463742399792716, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.004011695 valid loss= 0.005875499\n",
      "Optimization Finished!\n",
      "test loss: 0.005860443692654371, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.004115921937982804\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.005020007 valid loss= 0.006104433\n",
      "Epoch: 1000 train loss=0.011124523 valid loss= 0.003738612\n",
      "Epoch: 1500 train loss=0.005700880 valid loss= 0.004188222\n",
      "Epoch: 2000 train loss=0.003214743 valid loss= 0.004010074\n",
      "Epoch: 2500 train loss=0.002445320 valid loss= 0.004291291\n",
      "Epoch: 3000 train loss=0.002746669 valid loss= 0.003930588\n",
      "Epoch: 3500 train loss=0.003857914 valid loss= 0.003728169\n",
      "Epoch: 4000 train loss=0.002846691 valid loss= 0.003605891\n",
      "Epoch: 4500 train loss=0.003032209 valid loss= 0.003701260\n",
      "Epoch: 5000 train loss=0.006179206 valid loss= 0.003603157\n",
      "Epoch: 5500 train loss=0.001816500 valid loss= 0.003631961\n",
      "Epoch: 6000 train loss=0.003105429 valid loss= 0.003273060\n",
      "Epoch: 6500 train loss=0.003624980 valid loss= 0.003489236\n",
      "Epoch: 7000 train loss=0.005512918 valid loss= 0.003535176\n",
      "Epoch: 7500 train loss=0.002012114 valid loss= 0.003578658\n",
      "Epoch: 8000 train loss=0.002900141 valid loss= 0.003401072\n",
      "Epoch: 8500 train loss=0.001696421 valid loss= 0.003555183\n",
      "Epoch: 9000 train loss=0.002656661 valid loss= 0.003315708\n",
      "Epoch: 9500 train loss=0.001647538 valid loss= 0.003222455\n",
      "Epoch: 10000 train loss=0.001676358 valid loss= 0.003745167\n",
      "Epoch: 10500 train loss=0.003641991 valid loss= 0.003450033\n",
      "Epoch: 11000 train loss=0.003120544 valid loss= 0.003516036\n",
      "Epoch: 11500 train loss=0.001700483 valid loss= 0.003324615\n",
      "Epoch: 12000 train loss=0.005378079 valid loss= 0.003619378\n",
      "Epoch: 12500 train loss=0.003399510 valid loss= 0.003452300\n",
      "Epoch: 13000 train loss=0.001985653 valid loss= 0.003599279\n",
      "Epoch: 13500 train loss=0.003469731 valid loss= 0.003372333\n",
      "Epoch: 14000 train loss=0.002489377 valid loss= 0.003649094\n",
      "Epoch: 14500 train loss=0.007573802 valid loss= 0.003465060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:11:46,293]\u001b[0m Trial 57 finished with value: 0.0024352586376563375 and parameters: {'lam': 0.0018398525323411981, 'learning_rate': 0.17880490145949954, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.001994793 valid loss= 0.003624636\n",
      "Optimization Finished!\n",
      "test loss: 0.0041616251692175865, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0024352586376563375\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.007959915 valid loss= 0.008916025\n",
      "Epoch: 1000 train loss=0.008470623 valid loss= 0.007694508\n",
      "Epoch: 1500 train loss=0.004734766 valid loss= 0.005762991\n",
      "Epoch: 2000 train loss=0.008800728 valid loss= 0.004997735\n",
      "Epoch: 2500 train loss=0.004693656 valid loss= 0.004927373\n",
      "Epoch: 3000 train loss=0.004716354 valid loss= 0.005466082\n",
      "Epoch: 3500 train loss=0.004972358 valid loss= 0.005115332\n",
      "Epoch: 4000 train loss=0.003610408 valid loss= 0.005067079\n",
      "Epoch: 4500 train loss=0.002739915 valid loss= 0.004522285\n",
      "Epoch: 5000 train loss=0.003166214 valid loss= 0.004689718\n",
      "Epoch: 5500 train loss=0.003010885 valid loss= 0.004569264\n",
      "Epoch: 6000 train loss=0.005453388 valid loss= 0.004792740\n",
      "Epoch: 6500 train loss=0.003774639 valid loss= 0.004999354\n",
      "Epoch: 7000 train loss=0.003465110 valid loss= 0.004484445\n",
      "Epoch: 7500 train loss=0.017121730 valid loss= 0.004128005\n",
      "Epoch: 8000 train loss=0.003582466 valid loss= 0.004809516\n",
      "Epoch: 8500 train loss=0.003017062 valid loss= 0.004354081\n",
      "Epoch: 9000 train loss=0.003241169 valid loss= 0.004767667\n",
      "Epoch: 9500 train loss=0.003920603 valid loss= 0.004441882\n",
      "Epoch: 10000 train loss=0.005449440 valid loss= 0.004400017\n",
      "Epoch: 10500 train loss=0.004057052 valid loss= 0.004226162\n",
      "Epoch: 11000 train loss=0.003109218 valid loss= 0.004547224\n",
      "Epoch: 11500 train loss=0.002852919 valid loss= 0.004313302\n",
      "Epoch: 12000 train loss=0.002294081 valid loss= 0.004386578\n",
      "Epoch: 12500 train loss=0.012079760 valid loss= 0.004395426\n",
      "Epoch: 13000 train loss=0.008770208 valid loss= 0.004321388\n",
      "Epoch: 13500 train loss=0.004025839 valid loss= 0.004223275\n",
      "Epoch: 14000 train loss=0.002869433 valid loss= 0.004566187\n",
      "Epoch: 14500 train loss=0.003607498 valid loss= 0.004189232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:14:14,251]\u001b[0m Trial 58 finished with value: 0.002543943438898931 and parameters: {'lam': 0.003507993404924781, 'learning_rate': 0.19873164649415492, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.004761055 valid loss= 0.004594798\n",
      "Optimization Finished!\n",
      "test loss: 0.004333531484007835, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002543943438898931\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.022190453 valid loss= 0.011146828\n",
      "Epoch: 1000 train loss=0.016985644 valid loss= 0.011732684\n",
      "Epoch: 1500 train loss=0.006559740 valid loss= 0.010679119\n",
      "Epoch: 2000 train loss=0.008322261 valid loss= 0.009505002\n",
      "Epoch: 2500 train loss=0.006691553 valid loss= 0.008843788\n",
      "Epoch: 3000 train loss=0.007297019 valid loss= 0.008312780\n",
      "Epoch: 3500 train loss=0.005727933 valid loss= 0.008014279\n",
      "Epoch: 4000 train loss=0.006240898 valid loss= 0.007999168\n",
      "Epoch: 4500 train loss=0.010202109 valid loss= 0.008116690\n",
      "Epoch: 5000 train loss=0.007248254 valid loss= 0.007842605\n",
      "Epoch: 5500 train loss=0.005749497 valid loss= 0.007919077\n",
      "Epoch: 6000 train loss=0.006359127 valid loss= 0.007823223\n",
      "Epoch: 6500 train loss=0.006308724 valid loss= 0.007896525\n",
      "Epoch: 7000 train loss=0.006504510 valid loss= 0.007895637\n",
      "Epoch: 7500 train loss=0.005163400 valid loss= 0.008152816\n",
      "Epoch: 8000 train loss=0.004765014 valid loss= 0.007991332\n",
      "Epoch: 8500 train loss=0.004785188 valid loss= 0.008135605\n",
      "Epoch: 9000 train loss=0.005981492 valid loss= 0.007882022\n",
      "Epoch: 9500 train loss=0.004790807 valid loss= 0.008082181\n",
      "Epoch: 10000 train loss=0.005074978 valid loss= 0.008109888\n",
      "Epoch: 10500 train loss=0.005464843 valid loss= 0.007931398\n",
      "Epoch: 11000 train loss=0.005903094 valid loss= 0.008099241\n",
      "Epoch: 11500 train loss=0.005140780 valid loss= 0.008065546\n",
      "Epoch: 12000 train loss=0.004174615 valid loss= 0.007956158\n",
      "Epoch: 12500 train loss=0.005493398 valid loss= 0.007919299\n",
      "Epoch: 13000 train loss=0.005617091 valid loss= 0.007938627\n",
      "Epoch: 13500 train loss=0.004637354 valid loss= 0.007857919\n",
      "Epoch: 14000 train loss=0.005163413 valid loss= 0.007772924\n",
      "Epoch: 14500 train loss=0.004082646 valid loss= 0.007878996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:16:40,452]\u001b[0m Trial 59 finished with value: 0.003729766889671658 and parameters: {'lam': 0.006504042220906133, 'learning_rate': 0.11044054228818632, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.009611334 valid loss= 0.007584536\n",
      "Optimization Finished!\n",
      "test loss: 0.007596151903271675, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.003729766889671658\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.021271169 valid loss= 0.010824482\n",
      "Epoch: 1000 train loss=0.022128124 valid loss= 0.008172426\n",
      "Epoch: 1500 train loss=0.007904103 valid loss= 0.008407297\n",
      "Epoch: 2000 train loss=0.007617921 valid loss= 0.008364962\n",
      "Epoch: 2500 train loss=0.010335778 valid loss= 0.008622710\n",
      "Epoch: 3000 train loss=0.007322602 valid loss= 0.008656232\n",
      "Epoch: 3500 train loss=0.006280353 valid loss= 0.008370796\n",
      "Epoch: 4000 train loss=0.005956111 valid loss= 0.007415670\n",
      "Epoch: 4500 train loss=0.013445068 valid loss= 0.007263697\n",
      "Epoch: 5000 train loss=0.006025877 valid loss= 0.007140744\n",
      "Epoch: 5500 train loss=0.005650253 valid loss= 0.006706559\n",
      "Epoch: 6000 train loss=0.005723309 valid loss= 0.006993060\n",
      "Epoch: 6500 train loss=0.007037104 valid loss= 0.006691544\n",
      "Epoch: 7000 train loss=0.006160759 valid loss= 0.006679958\n",
      "Epoch: 7500 train loss=0.005572574 valid loss= 0.006828856\n",
      "Epoch: 8000 train loss=0.007613523 valid loss= 0.006513794\n",
      "Epoch: 8500 train loss=0.004721877 valid loss= 0.006593554\n",
      "Epoch: 9000 train loss=0.005015320 valid loss= 0.006441699\n",
      "Epoch: 9500 train loss=0.004912273 valid loss= 0.006395068\n",
      "Epoch: 10000 train loss=0.004974075 valid loss= 0.006744906\n",
      "Epoch: 10500 train loss=0.004408297 valid loss= 0.006421939\n",
      "Epoch: 11000 train loss=0.007447143 valid loss= 0.006229045\n",
      "Epoch: 11500 train loss=0.004556018 valid loss= 0.006309931\n",
      "Epoch: 12000 train loss=0.004653991 valid loss= 0.006478732\n",
      "Epoch: 12500 train loss=0.006333240 valid loss= 0.006174316\n",
      "Epoch: 13000 train loss=0.005634973 valid loss= 0.006130138\n",
      "Epoch: 13500 train loss=0.006100099 valid loss= 0.005892969\n",
      "Epoch: 14000 train loss=0.008357804 valid loss= 0.006300612\n",
      "Epoch: 14500 train loss=0.004875591 valid loss= 0.006124057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:19:08,054]\u001b[0m Trial 60 finished with value: 0.0020972634025710046 and parameters: {'lam': 0.0075851774556728885, 'learning_rate': 0.12762978699895608, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.008365277 valid loss= 0.006047225\n",
      "Optimization Finished!\n",
      "test loss: 0.005840461701154709, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0020972634025710046\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.017719459 valid loss= 0.013114022\n",
      "Epoch: 1000 train loss=0.010250295 valid loss= 0.010752940\n",
      "Epoch: 1500 train loss=0.006722627 valid loss= 0.008473411\n",
      "Epoch: 2000 train loss=0.006617364 valid loss= 0.008339420\n",
      "Epoch: 2500 train loss=0.007741213 valid loss= 0.007432854\n",
      "Epoch: 3000 train loss=0.010824283 valid loss= 0.006994713\n",
      "Epoch: 3500 train loss=0.007199233 valid loss= 0.007511601\n",
      "Epoch: 4000 train loss=0.007049566 valid loss= 0.007179317\n",
      "Epoch: 4500 train loss=0.006089273 valid loss= 0.007486898\n",
      "Epoch: 5000 train loss=0.005870907 valid loss= 0.006996388\n",
      "Epoch: 5500 train loss=0.006181201 valid loss= 0.007011598\n",
      "Epoch: 6000 train loss=0.006583513 valid loss= 0.006940892\n",
      "Epoch: 6500 train loss=0.011023777 valid loss= 0.007767668\n",
      "Epoch: 7000 train loss=0.006678502 valid loss= 0.007132343\n",
      "Epoch: 7500 train loss=0.006188853 valid loss= 0.007019099\n",
      "Epoch: 8000 train loss=0.008110859 valid loss= 0.007273172\n",
      "Epoch: 8500 train loss=0.006265093 valid loss= 0.007121916\n",
      "Epoch: 9000 train loss=0.009500550 valid loss= 0.007386691\n",
      "Epoch: 9500 train loss=0.007780146 valid loss= 0.006901566\n",
      "Epoch: 10000 train loss=0.008702361 valid loss= 0.006789724\n",
      "Epoch: 10500 train loss=0.005780513 valid loss= 0.006495648\n",
      "Epoch: 11000 train loss=0.006942201 valid loss= 0.007863799\n",
      "Epoch: 11500 train loss=0.005344487 valid loss= 0.007045058\n",
      "Epoch: 12000 train loss=0.005747033 valid loss= 0.007167963\n",
      "Epoch: 12500 train loss=0.008479981 valid loss= 0.007298113\n",
      "Epoch: 13000 train loss=0.007257841 valid loss= 0.006994641\n",
      "Epoch: 13500 train loss=0.005988762 valid loss= 0.006911427\n",
      "Epoch: 14000 train loss=0.011451076 valid loss= 0.006660679\n",
      "Epoch: 14500 train loss=0.005746044 valid loss= 0.007203689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:21:35,192]\u001b[0m Trial 61 finished with value: 0.002309105487047389 and parameters: {'lam': 0.009161817483313388, 'learning_rate': 0.15540875005209126, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.005439405 valid loss= 0.007672534\n",
      "Optimization Finished!\n",
      "test loss: 0.007411602884531021, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002309105487047389\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.016052177 valid loss= 0.015846524\n",
      "Epoch: 1000 train loss=0.013273256 valid loss= 0.015666995\n",
      "Epoch: 1500 train loss=0.013003311 valid loss= 0.011357201\n",
      "Epoch: 2000 train loss=0.010116026 valid loss= 0.010203648\n",
      "Epoch: 2500 train loss=0.007582241 valid loss= 0.009995529\n",
      "Epoch: 3000 train loss=0.008792362 valid loss= 0.009789588\n",
      "Epoch: 3500 train loss=0.008952713 valid loss= 0.008793140\n",
      "Epoch: 4000 train loss=0.011210965 valid loss= 0.008736940\n",
      "Epoch: 4500 train loss=0.008128848 valid loss= 0.008345434\n",
      "Epoch: 5000 train loss=0.006365414 valid loss= 0.008274502\n",
      "Epoch: 5500 train loss=0.014369746 valid loss= 0.007622981\n",
      "Epoch: 6000 train loss=0.006357027 valid loss= 0.007896917\n",
      "Epoch: 6500 train loss=0.008220258 valid loss= 0.007789823\n",
      "Epoch: 7000 train loss=0.007048191 valid loss= 0.007789231\n",
      "Epoch: 7500 train loss=0.007029359 valid loss= 0.007816674\n",
      "Epoch: 8000 train loss=0.007034961 valid loss= 0.008009775\n",
      "Epoch: 8500 train loss=0.009787202 valid loss= 0.008039894\n",
      "Epoch: 9000 train loss=0.007523789 valid loss= 0.008191892\n",
      "Epoch: 9500 train loss=0.006268206 valid loss= 0.007923069\n",
      "Epoch: 10000 train loss=0.006492359 valid loss= 0.008143929\n",
      "Epoch: 10500 train loss=0.007027209 valid loss= 0.007938324\n",
      "Epoch: 11000 train loss=0.009349987 valid loss= 0.007957402\n",
      "Epoch: 11500 train loss=0.006590907 valid loss= 0.007572117\n",
      "Epoch: 12000 train loss=0.006919953 valid loss= 0.008245798\n",
      "Epoch: 12500 train loss=0.006787379 valid loss= 0.007863959\n",
      "Epoch: 13000 train loss=0.011276724 valid loss= 0.007563334\n",
      "Epoch: 13500 train loss=0.006156981 valid loss= 0.007760029\n",
      "Epoch: 14000 train loss=0.008097973 valid loss= 0.007800696\n",
      "Epoch: 14500 train loss=0.006029813 valid loss= 0.007886857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:24:00,945]\u001b[0m Trial 62 finished with value: 0.0025907520825244348 and parameters: {'lam': 0.009306348507367173, 'learning_rate': 0.09926513990092678, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.007780567 valid loss= 0.008021733\n",
      "Optimization Finished!\n",
      "test loss: 0.007674446329474449, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0025907520825244348\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.028281081 valid loss= 0.013463577\n",
      "Epoch: 1000 train loss=0.011476109 valid loss= 0.013138283\n",
      "Epoch: 1500 train loss=0.012358628 valid loss= 0.010117449\n",
      "Epoch: 2000 train loss=0.007489275 valid loss= 0.009285422\n",
      "Epoch: 2500 train loss=0.007290620 valid loss= 0.008895308\n",
      "Epoch: 3000 train loss=0.006887862 valid loss= 0.009196298\n",
      "Epoch: 3500 train loss=0.007335044 valid loss= 0.008507720\n",
      "Epoch: 4000 train loss=0.008552155 valid loss= 0.008442909\n",
      "Epoch: 4500 train loss=0.006212415 valid loss= 0.007970500\n",
      "Epoch: 5000 train loss=0.006640647 valid loss= 0.008257553\n",
      "Epoch: 5500 train loss=0.005520448 valid loss= 0.007722382\n",
      "Epoch: 6000 train loss=0.005755840 valid loss= 0.007491847\n",
      "Epoch: 6500 train loss=0.007263016 valid loss= 0.007655004\n",
      "Epoch: 7000 train loss=0.013142074 valid loss= 0.007623214\n",
      "Epoch: 7500 train loss=0.006001961 valid loss= 0.007576124\n",
      "Epoch: 8000 train loss=0.006797606 valid loss= 0.007877665\n",
      "Epoch: 8500 train loss=0.005472556 valid loss= 0.007319967\n",
      "Epoch: 9000 train loss=0.007414605 valid loss= 0.007114594\n",
      "Epoch: 9500 train loss=0.005572734 valid loss= 0.007512434\n",
      "Epoch: 10000 train loss=0.005645879 valid loss= 0.007430075\n",
      "Epoch: 10500 train loss=0.005787828 valid loss= 0.007381334\n",
      "Epoch: 11000 train loss=0.005725285 valid loss= 0.007517966\n",
      "Epoch: 11500 train loss=0.007043749 valid loss= 0.007453268\n",
      "Epoch: 12000 train loss=0.006235701 valid loss= 0.007433277\n",
      "Epoch: 12500 train loss=0.012018489 valid loss= 0.007271009\n",
      "Epoch: 13000 train loss=0.007170528 valid loss= 0.007356165\n",
      "Epoch: 13500 train loss=0.005423032 valid loss= 0.007633515\n",
      "Epoch: 14000 train loss=0.006579585 valid loss= 0.007170991\n",
      "Epoch: 14500 train loss=0.006183909 valid loss= 0.007199247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:26:29,995]\u001b[0m Trial 63 finished with value: 0.0021803307215131867 and parameters: {'lam': 0.00858071922109798, 'learning_rate': 0.08618125569072595, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.006679349 valid loss= 0.007201729\n",
      "Optimization Finished!\n",
      "test loss: 0.007059092167764902, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0021803307215131867\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.008153142 valid loss= 0.009307720\n",
      "Epoch: 1000 train loss=0.016466390 valid loss= 0.008608447\n",
      "Epoch: 1500 train loss=0.015851840 valid loss= 0.008805583\n",
      "Epoch: 2000 train loss=0.012737073 valid loss= 0.009045330\n",
      "Epoch: 2500 train loss=0.014806692 valid loss= 0.008967046\n",
      "Epoch: 3000 train loss=0.016371332 valid loss= 0.008808807\n",
      "Epoch: 3500 train loss=0.005532020 valid loss= 0.007695467\n",
      "Epoch: 4000 train loss=0.009243022 valid loss= 0.007899215\n",
      "Epoch: 4500 train loss=0.011625768 valid loss= 0.007315299\n",
      "Epoch: 5000 train loss=0.011441635 valid loss= 0.007299415\n",
      "Epoch: 5500 train loss=0.010585317 valid loss= 0.006897583\n",
      "Epoch: 6000 train loss=0.003471250 valid loss= 0.006405320\n",
      "Epoch: 6500 train loss=0.008030580 valid loss= 0.005919532\n",
      "Epoch: 7000 train loss=0.008020772 valid loss= 0.005273510\n",
      "Epoch: 7500 train loss=0.010027036 valid loss= 0.004856524\n",
      "Epoch: 8000 train loss=0.006553884 valid loss= 0.004823590\n",
      "Epoch: 8500 train loss=0.004153545 valid loss= 0.004498738\n",
      "Epoch: 9000 train loss=0.012456332 valid loss= 0.004720423\n",
      "Epoch: 9500 train loss=0.002626161 valid loss= 0.004554524\n",
      "Epoch: 10000 train loss=0.004202001 valid loss= 0.004800598\n",
      "Epoch: 10500 train loss=0.002682107 valid loss= 0.004708969\n",
      "Epoch: 11000 train loss=0.007268383 valid loss= 0.004724649\n",
      "Epoch: 11500 train loss=0.015496270 valid loss= 0.004568903\n",
      "Epoch: 12000 train loss=0.007900785 valid loss= 0.004508865\n",
      "Epoch: 12500 train loss=0.005147839 valid loss= 0.004568650\n",
      "Epoch: 13000 train loss=0.002472858 valid loss= 0.004390989\n",
      "Epoch: 13500 train loss=0.002237491 valid loss= 0.004448230\n",
      "Epoch: 14000 train loss=0.002212035 valid loss= 0.004339025\n",
      "Epoch: 14500 train loss=0.003765624 valid loss= 0.004325401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:28:56,851]\u001b[0m Trial 64 finished with value: 0.0025150895325883786 and parameters: {'lam': 0.002669677251612922, 'learning_rate': 0.027676815905450694, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.002866516 valid loss= 0.004270208\n",
      "Optimization Finished!\n",
      "test loss: 0.004016178660094738, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0025150895325883786\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.013911783 valid loss= 0.009317255\n",
      "Epoch: 1000 train loss=0.006924659 valid loss= 0.007331103\n",
      "Epoch: 1500 train loss=0.009149153 valid loss= 0.006969811\n",
      "Epoch: 2000 train loss=0.007366618 valid loss= 0.006263389\n",
      "Epoch: 2500 train loss=0.004439856 valid loss= 0.005812749\n",
      "Epoch: 3000 train loss=0.004356511 valid loss= 0.006216588\n",
      "Epoch: 3500 train loss=0.005217787 valid loss= 0.005402636\n",
      "Epoch: 4000 train loss=0.004407055 valid loss= 0.005977133\n",
      "Epoch: 4500 train loss=0.004426503 valid loss= 0.005817018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:29:46,684]\u001b[0m Trial 65 finished with value: 0.002562122507825765 and parameters: {'lam': 0.005660690347270349, 'learning_rate': 0.13452751073145722, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.006612600 valid loss= 0.005968382\n",
      "Optimization Finished!\n",
      "test loss: 0.005546634551137686, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002562122507825765\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.022926556 valid loss= 0.009226921\n",
      "Epoch: 1000 train loss=0.013148155 valid loss= 0.008940537\n",
      "Epoch: 1500 train loss=0.014715530 valid loss= 0.008876165\n",
      "Epoch: 2000 train loss=0.007811172 valid loss= 0.008495875\n",
      "Epoch: 2500 train loss=0.012814674 valid loss= 0.008095525\n",
      "Epoch: 3000 train loss=0.011770004 valid loss= 0.008292195\n",
      "Epoch: 3500 train loss=0.018960340 valid loss= 0.007319679\n",
      "Epoch: 4000 train loss=0.011933414 valid loss= 0.007636167\n",
      "Epoch: 4500 train loss=0.013328900 valid loss= 0.007550892\n",
      "Epoch: 5000 train loss=0.022398097 valid loss= 0.007753888\n",
      "Epoch: 5500 train loss=0.004950271 valid loss= 0.007371953\n",
      "Epoch: 6000 train loss=0.007804120 valid loss= 0.007253150\n",
      "Epoch: 6500 train loss=0.012501578 valid loss= 0.006418861\n",
      "Epoch: 7000 train loss=0.009956975 valid loss= 0.006031353\n",
      "Epoch: 7500 train loss=0.005212094 valid loss= 0.005599663\n",
      "Epoch: 8000 train loss=0.005345126 valid loss= 0.005519661\n",
      "Epoch: 8500 train loss=0.005579080 valid loss= 0.005388552\n",
      "Epoch: 9000 train loss=0.005837484 valid loss= 0.004802938\n",
      "Epoch: 9500 train loss=0.010007279 valid loss= 0.005059313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:31:24,046]\u001b[0m Trial 66 finished with value: 0.0028520886947927143 and parameters: {'lam': 0.0024184342636207716, 'learning_rate': 0.019252490701242193, 'num_epoch': 10000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10000 train loss=0.005977392 valid loss= 0.004772227\n",
      "Optimization Finished!\n",
      "test loss: 0.005267011001706123, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0028520886947927143\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.017896948 valid loss= 0.011203704\n",
      "Epoch: 1000 train loss=0.020705322 valid loss= 0.008789377\n",
      "Epoch: 1500 train loss=0.014261790 valid loss= 0.007290273\n",
      "Epoch: 2000 train loss=0.009138713 valid loss= 0.007187149\n",
      "Epoch: 2500 train loss=0.006246655 valid loss= 0.006596906\n",
      "Epoch: 3000 train loss=0.006281821 valid loss= 0.006755188\n",
      "Epoch: 3500 train loss=0.006283708 valid loss= 0.006152736\n",
      "Epoch: 4000 train loss=0.008211545 valid loss= 0.006092016\n",
      "Epoch: 4500 train loss=0.008386146 valid loss= 0.006157215\n",
      "Epoch: 5000 train loss=0.015362266 valid loss= 0.006101089\n",
      "Epoch: 5500 train loss=0.007001745 valid loss= 0.006088202\n",
      "Epoch: 6000 train loss=0.005908714 valid loss= 0.006423960\n",
      "Epoch: 6500 train loss=0.007457800 valid loss= 0.006361455\n",
      "Epoch: 7000 train loss=0.005516584 valid loss= 0.006325446\n",
      "Epoch: 7500 train loss=0.008088995 valid loss= 0.006048828\n",
      "Epoch: 8000 train loss=0.005292496 valid loss= 0.006566637\n",
      "Epoch: 8500 train loss=0.007032275 valid loss= 0.006568375\n",
      "Epoch: 9000 train loss=0.005220112 valid loss= 0.006206105\n",
      "Epoch: 9500 train loss=0.005874373 valid loss= 0.006209787\n",
      "Epoch: 10000 train loss=0.006039766 valid loss= 0.007006375\n",
      "Epoch: 10500 train loss=0.006554068 valid loss= 0.006226836\n",
      "Epoch: 11000 train loss=0.004842827 valid loss= 0.006416014\n",
      "Epoch: 11500 train loss=0.008884229 valid loss= 0.006584228\n",
      "Epoch: 12000 train loss=0.005927109 valid loss= 0.006445865\n",
      "Epoch: 12500 train loss=0.005596732 valid loss= 0.006536853\n",
      "Epoch: 13000 train loss=0.006652385 valid loss= 0.006320301\n",
      "Epoch: 13500 train loss=0.004983358 valid loss= 0.006735903\n",
      "Epoch: 14000 train loss=0.012550337 valid loss= 0.006886141\n",
      "Epoch: 14500 train loss=0.008679772 valid loss= 0.006335170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:33:51,004]\u001b[0m Trial 67 finished with value: 0.0022942579241371054 and parameters: {'lam': 0.007249275939362145, 'learning_rate': 0.11901253098171978, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.005066622 valid loss= 0.006820847\n",
      "Optimization Finished!\n",
      "test loss: 0.008706324733793736, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0022942579241371054\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.016192639 valid loss= 0.006090143\n",
      "Epoch: 1000 train loss=0.008502908 valid loss= 0.003075087\n",
      "Epoch: 1500 train loss=0.004216169 valid loss= 0.003358123\n",
      "Epoch: 2000 train loss=0.003881008 valid loss= 0.003018328\n",
      "Epoch: 2500 train loss=0.001634141 valid loss= 0.002817929\n",
      "Epoch: 3000 train loss=0.003471905 valid loss= 0.002912969\n",
      "Epoch: 3500 train loss=0.003354877 valid loss= 0.003230162\n",
      "Epoch: 4000 train loss=0.005149485 valid loss= 0.002871384\n",
      "Epoch: 4500 train loss=0.003522507 valid loss= 0.002770113\n",
      "Epoch: 5000 train loss=0.002207207 valid loss= 0.002835646\n",
      "Epoch: 5500 train loss=0.002451501 valid loss= 0.002859855\n",
      "Epoch: 6000 train loss=0.002405360 valid loss= 0.002891658\n",
      "Epoch: 6500 train loss=0.005577694 valid loss= 0.002778230\n",
      "Epoch: 7000 train loss=0.003749946 valid loss= 0.002901588\n",
      "Epoch: 7500 train loss=0.001954105 valid loss= 0.002743933\n",
      "Epoch: 8000 train loss=0.003282385 valid loss= 0.002997483\n",
      "Epoch: 8500 train loss=0.002630162 valid loss= 0.003232728\n",
      "Epoch: 9000 train loss=0.003564353 valid loss= 0.002652687\n",
      "Epoch: 9500 train loss=0.001941819 valid loss= 0.003332080\n",
      "Epoch: 10000 train loss=0.002642295 valid loss= 0.003313893\n",
      "Epoch: 10500 train loss=0.003910598 valid loss= 0.003434803\n",
      "Epoch: 11000 train loss=0.002835817 valid loss= 0.003268050\n",
      "Epoch: 11500 train loss=0.002517052 valid loss= 0.003211364\n",
      "Epoch: 12000 train loss=0.002380196 valid loss= 0.003745747\n",
      "Epoch: 12500 train loss=0.003177830 valid loss= 0.003489346\n",
      "Epoch: 13000 train loss=0.001753579 valid loss= 0.003351370\n",
      "Epoch: 13500 train loss=0.001338842 valid loss= 0.003818435\n",
      "Epoch: 14000 train loss=0.001201965 valid loss= 0.003668750\n",
      "Epoch: 14500 train loss=0.001811092 valid loss= 0.003419249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:36:17,796]\u001b[0m Trial 68 finished with value: 0.002528292459417824 and parameters: {'lam': 0.001492493296724608, 'learning_rate': 0.17841633893232572, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.001478652 valid loss= 0.003529658\n",
      "Optimization Finished!\n",
      "test loss: 0.0037451733369380236, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002528292459417824\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.012714854 valid loss= 0.011332547\n",
      "Epoch: 1000 train loss=0.009497771 valid loss= 0.010930715\n",
      "Epoch: 1500 train loss=0.007135063 valid loss= 0.009659388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:36:39,622]\u001b[0m Trial 69 finished with value: 0.00677002742848793 and parameters: {'lam': 0.003870633981044162, 'learning_rate': 0.06879896674362222, 'num_epoch': 2000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 train loss=0.008232269 valid loss= 0.010042416\n",
      "Optimization Finished!\n",
      "test loss: 0.010304606519639492, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.00677002742848793\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.017530128 valid loss= 0.015226407\n",
      "Epoch: 1000 train loss=0.016411684 valid loss= 0.015195949\n",
      "Epoch: 1500 train loss=0.018956281 valid loss= 0.015098879\n",
      "Epoch: 2000 train loss=0.012118751 valid loss= 0.015008150\n",
      "Epoch: 2500 train loss=0.012763947 valid loss= 0.014127407\n",
      "Epoch: 3000 train loss=0.018310625 valid loss= 0.013648224\n",
      "Epoch: 3500 train loss=0.015228597 valid loss= 0.012826203\n",
      "Epoch: 4000 train loss=0.013191590 valid loss= 0.012109038\n",
      "Epoch: 4500 train loss=0.015342040 valid loss= 0.011068670\n",
      "Epoch: 5000 train loss=0.012588450 valid loss= 0.010252999\n",
      "Epoch: 5500 train loss=0.007924241 valid loss= 0.009892960\n",
      "Epoch: 6000 train loss=0.007902164 valid loss= 0.010003922\n",
      "Epoch: 6500 train loss=0.010126101 valid loss= 0.009553906\n",
      "Epoch: 7000 train loss=0.009034333 valid loss= 0.009661191\n",
      "Epoch: 7500 train loss=0.007200973 valid loss= 0.009193080\n",
      "Epoch: 8000 train loss=0.010610103 valid loss= 0.009128058\n",
      "Epoch: 8500 train loss=0.006944901 valid loss= 0.009329986\n",
      "Epoch: 9000 train loss=0.011127507 valid loss= 0.008606412\n",
      "Epoch: 9500 train loss=0.007378588 valid loss= 0.008582050\n",
      "Epoch: 10000 train loss=0.007014037 valid loss= 0.008429509\n",
      "Epoch: 10500 train loss=0.006690681 valid loss= 0.008363578\n",
      "Epoch: 11000 train loss=0.010931456 valid loss= 0.008443378\n",
      "Epoch: 11500 train loss=0.006649756 valid loss= 0.008249581\n",
      "Epoch: 12000 train loss=0.006494934 valid loss= 0.008229711\n",
      "Epoch: 12500 train loss=0.007823220 valid loss= 0.008132203\n",
      "Epoch: 13000 train loss=0.006276309 valid loss= 0.008218638\n",
      "Epoch: 13500 train loss=0.006676790 valid loss= 0.008319855\n",
      "Epoch: 14000 train loss=0.006909230 valid loss= 0.008354776\n",
      "Epoch: 14500 train loss=0.011338855 valid loss= 0.008018268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:39:05,610]\u001b[0m Trial 70 finished with value: 0.0023045156270934696 and parameters: {'lam': 0.009948980625494472, 'learning_rate': 0.042442171752011745, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.005953520 valid loss= 0.007955601\n",
      "Optimization Finished!\n",
      "test loss: 0.007651207968592644, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0023045156270934696\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.014990658 valid loss= 0.008102382\n",
      "Epoch: 1000 train loss=0.011364032 valid loss= 0.009163418\n",
      "Epoch: 1500 train loss=0.007123213 valid loss= 0.007635288\n",
      "Epoch: 2000 train loss=0.006043831 valid loss= 0.007276337\n",
      "Epoch: 2500 train loss=0.005346464 valid loss= 0.007352827\n",
      "Epoch: 3000 train loss=0.004614890 valid loss= 0.006195411\n",
      "Epoch: 3500 train loss=0.005861031 valid loss= 0.006176301\n",
      "Epoch: 4000 train loss=0.004596720 valid loss= 0.006151791\n",
      "Epoch: 4500 train loss=0.010435369 valid loss= 0.005877843\n",
      "Epoch: 5000 train loss=0.008174605 valid loss= 0.005973600\n",
      "Epoch: 5500 train loss=0.003179817 valid loss= 0.006115126\n",
      "Epoch: 6000 train loss=0.004835952 valid loss= 0.005974655\n",
      "Epoch: 6500 train loss=0.003240639 valid loss= 0.005950764\n",
      "Epoch: 7000 train loss=0.006029511 valid loss= 0.005355350\n",
      "Epoch: 7500 train loss=0.002726029 valid loss= 0.005681038\n",
      "Epoch: 8000 train loss=0.002624083 valid loss= 0.005723771\n",
      "Epoch: 8500 train loss=0.004670914 valid loss= 0.005717192\n",
      "Epoch: 9000 train loss=0.002918895 valid loss= 0.005626786\n",
      "Epoch: 9500 train loss=0.004410419 valid loss= 0.005626737\n",
      "Epoch: 10000 train loss=0.002415646 valid loss= 0.005604358\n",
      "Epoch: 10500 train loss=0.007329334 valid loss= 0.005515805\n",
      "Epoch: 11000 train loss=0.003057842 valid loss= 0.005432265\n",
      "Epoch: 11500 train loss=0.003013033 valid loss= 0.005630317\n",
      "Epoch: 12000 train loss=0.003066292 valid loss= 0.005917654\n",
      "Epoch: 12500 train loss=0.002428726 valid loss= 0.005618549\n",
      "Epoch: 13000 train loss=0.005278754 valid loss= 0.006102012\n",
      "Epoch: 13500 train loss=0.002694886 valid loss= 0.006001083\n",
      "Epoch: 14000 train loss=0.002927953 valid loss= 0.005717529\n",
      "Epoch: 14500 train loss=0.004854428 valid loss= 0.005590960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:41:32,587]\u001b[0m Trial 71 finished with value: 0.00377094329415482 and parameters: {'lam': 0.003113905655338407, 'learning_rate': 0.15628107558415455, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.002416746 valid loss= 0.005735714\n",
      "Optimization Finished!\n",
      "test loss: 0.005734103731811047, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.00377094329415482\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.013993676 valid loss= 0.008685607\n",
      "Epoch: 1000 train loss=0.008655480 valid loss= 0.007596827\n",
      "Epoch: 1500 train loss=0.005447652 valid loss= 0.007168880\n",
      "Epoch: 2000 train loss=0.005280974 valid loss= 0.006872816\n",
      "Epoch: 2500 train loss=0.006158776 valid loss= 0.007481049\n",
      "Epoch: 3000 train loss=0.008213142 valid loss= 0.007834082\n",
      "Epoch: 3500 train loss=0.005873521 valid loss= 0.007298534\n",
      "Epoch: 4000 train loss=0.008233283 valid loss= 0.007472454\n",
      "Epoch: 4500 train loss=0.009099851 valid loss= 0.007438249\n",
      "Epoch: 5000 train loss=0.005110736 valid loss= 0.007561750\n",
      "Epoch: 5500 train loss=0.004670858 valid loss= 0.007622855\n",
      "Epoch: 6000 train loss=0.007041394 valid loss= 0.007402103\n",
      "Epoch: 6500 train loss=0.005277929 valid loss= 0.007657235\n",
      "Epoch: 7000 train loss=0.005783853 valid loss= 0.007570364\n",
      "Epoch: 7500 train loss=0.005778063 valid loss= 0.007462658\n",
      "Epoch: 8000 train loss=0.004992612 valid loss= 0.007314488\n",
      "Epoch: 8500 train loss=0.003889612 valid loss= 0.007385081\n",
      "Epoch: 9000 train loss=0.004754737 valid loss= 0.007340227\n",
      "Epoch: 9500 train loss=0.005941744 valid loss= 0.006903435\n",
      "Epoch: 10000 train loss=0.004419872 valid loss= 0.006958888\n",
      "Epoch: 10500 train loss=0.003773550 valid loss= 0.006943665\n",
      "Epoch: 11000 train loss=0.006067657 valid loss= 0.006639692\n",
      "Epoch: 11500 train loss=0.004737482 valid loss= 0.006689413\n",
      "Epoch: 12000 train loss=0.004509645 valid loss= 0.006492808\n",
      "Epoch: 12500 train loss=0.004616028 valid loss= 0.006756651\n",
      "Epoch: 13000 train loss=0.004929688 valid loss= 0.006818541\n",
      "Epoch: 13500 train loss=0.007089073 valid loss= 0.006969191\n",
      "Epoch: 14000 train loss=0.005367571 valid loss= 0.007010473\n",
      "Epoch: 14500 train loss=0.004181765 valid loss= 0.006843141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:43:58,796]\u001b[0m Trial 72 finished with value: 0.0037475188395396904 and parameters: {'lam': 0.005069698577622262, 'learning_rate': 0.1429417037455034, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.004448785 valid loss= 0.006834067\n",
      "Optimization Finished!\n",
      "test loss: 0.00664341077208519, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0037475188395396904\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.016428066 valid loss= 0.009349730\n",
      "Epoch: 1000 train loss=0.010107795 valid loss= 0.006686582\n",
      "Epoch: 1500 train loss=0.008098033 valid loss= 0.005440605\n",
      "Epoch: 2000 train loss=0.004946219 valid loss= 0.005736560\n",
      "Epoch: 2500 train loss=0.004939874 valid loss= 0.005565569\n",
      "Epoch: 3000 train loss=0.004260981 valid loss= 0.004887060\n",
      "Epoch: 3500 train loss=0.003924641 valid loss= 0.005180358\n",
      "Epoch: 4000 train loss=0.004089650 valid loss= 0.005166240\n",
      "Epoch: 4500 train loss=0.005726242 valid loss= 0.005098694\n",
      "Epoch: 5000 train loss=0.004450159 valid loss= 0.005382575\n",
      "Epoch: 5500 train loss=0.006601251 valid loss= 0.005514309\n",
      "Epoch: 6000 train loss=0.007521608 valid loss= 0.005295068\n",
      "Epoch: 6500 train loss=0.004464869 valid loss= 0.004753949\n",
      "Epoch: 7000 train loss=0.003521601 valid loss= 0.004951665\n",
      "Epoch: 7500 train loss=0.003862001 valid loss= 0.004848239\n",
      "Epoch: 8000 train loss=0.005338300 valid loss= 0.004876502\n",
      "Epoch: 8500 train loss=0.005170453 valid loss= 0.004746107\n",
      "Epoch: 9000 train loss=0.006524635 valid loss= 0.004694076\n",
      "Epoch: 9500 train loss=0.004510095 valid loss= 0.004757173\n",
      "Epoch: 10000 train loss=0.003822349 valid loss= 0.004839242\n",
      "Epoch: 10500 train loss=0.007485341 valid loss= 0.004718809\n",
      "Epoch: 11000 train loss=0.003296291 valid loss= 0.004774981\n",
      "Epoch: 11500 train loss=0.003683710 valid loss= 0.004698944\n",
      "Epoch: 12000 train loss=0.004516075 valid loss= 0.004687701\n",
      "Epoch: 12500 train loss=0.005378335 valid loss= 0.004710025\n",
      "Epoch: 13000 train loss=0.003594127 valid loss= 0.004800112\n",
      "Epoch: 13500 train loss=0.003778512 valid loss= 0.004633712\n",
      "Epoch: 14000 train loss=0.003924850 valid loss= 0.004737448\n",
      "Epoch: 14500 train loss=0.004317244 valid loss= 0.004826166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:46:24,529]\u001b[0m Trial 73 finished with value: 0.0019360536492726988 and parameters: {'lam': 0.00459821449121374, 'learning_rate': 0.17199236022691344, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.011246753 valid loss= 0.004773512\n",
      "Optimization Finished!\n",
      "test loss: 0.005176171660423279, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0019360536492726988\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.012364670 valid loss= 0.010257313\n",
      "Epoch: 1000 train loss=0.008606816 valid loss= 0.010726538\n",
      "Epoch: 1500 train loss=0.006805540 valid loss= 0.007766690\n",
      "Epoch: 2000 train loss=0.008122794 valid loss= 0.006875969\n",
      "Epoch: 2500 train loss=0.010943078 valid loss= 0.006013790\n",
      "Epoch: 3000 train loss=0.004371326 valid loss= 0.005745506\n",
      "Epoch: 3500 train loss=0.004960516 valid loss= 0.005899307\n",
      "Epoch: 4000 train loss=0.003562660 valid loss= 0.005592639\n",
      "Epoch: 4500 train loss=0.003535740 valid loss= 0.006026204\n",
      "Epoch: 5000 train loss=0.004385288 valid loss= 0.005847566\n",
      "Epoch: 5500 train loss=0.004591550 valid loss= 0.005532413\n",
      "Epoch: 6000 train loss=0.004851885 valid loss= 0.005570769\n",
      "Epoch: 6500 train loss=0.003103951 valid loss= 0.005869823\n",
      "Epoch: 7000 train loss=0.004619291 valid loss= 0.005617702\n",
      "Epoch: 7500 train loss=0.003131308 valid loss= 0.005660818\n",
      "Epoch: 8000 train loss=0.003823500 valid loss= 0.005949812\n",
      "Epoch: 8500 train loss=0.003739839 valid loss= 0.005936865\n",
      "Epoch: 9000 train loss=0.003535096 valid loss= 0.005650643\n",
      "Epoch: 9500 train loss=0.003490094 valid loss= 0.006171701\n",
      "Epoch: 10000 train loss=0.004778915 valid loss= 0.006127574\n",
      "Epoch: 10500 train loss=0.003354035 valid loss= 0.006118142\n",
      "Epoch: 11000 train loss=0.004836977 valid loss= 0.006070000\n",
      "Epoch: 11500 train loss=0.003439910 valid loss= 0.006010350\n",
      "Epoch: 12000 train loss=0.002963467 valid loss= 0.006278703\n",
      "Epoch: 12500 train loss=0.002864420 valid loss= 0.006282866\n",
      "Epoch: 13000 train loss=0.004360025 valid loss= 0.006630182\n",
      "Epoch: 13500 train loss=0.004027157 valid loss= 0.006422747\n",
      "Epoch: 14000 train loss=0.003151887 valid loss= 0.006623784\n",
      "Epoch: 14500 train loss=0.003074867 valid loss= 0.006863119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:48:51,961]\u001b[0m Trial 74 finished with value: 0.0045936743424858685 and parameters: {'lam': 0.004087433892037042, 'learning_rate': 0.16847629837729114, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.003120555 valid loss= 0.007053792\n",
      "Optimization Finished!\n",
      "test loss: 0.006773123051971197, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0045936743424858685\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.014287893 valid loss= 0.011890272\n",
      "Epoch: 1000 train loss=0.014646312 valid loss= 0.008725660\n",
      "Epoch: 1500 train loss=0.008866487 valid loss= 0.008797166\n",
      "Epoch: 2000 train loss=0.008319267 valid loss= 0.008421532\n",
      "Epoch: 2500 train loss=0.006688491 valid loss= 0.007941610\n",
      "Epoch: 3000 train loss=0.006660962 valid loss= 0.007988230\n",
      "Epoch: 3500 train loss=0.008472171 valid loss= 0.007087130\n",
      "Epoch: 4000 train loss=0.006624198 valid loss= 0.006538533\n",
      "Epoch: 4500 train loss=0.005523827 valid loss= 0.006582275\n",
      "Epoch: 5000 train loss=0.010230629 valid loss= 0.006760924\n",
      "Epoch: 5500 train loss=0.011924223 valid loss= 0.006548650\n",
      "Epoch: 6000 train loss=0.005309615 valid loss= 0.006625284\n",
      "Epoch: 6500 train loss=0.006649842 valid loss= 0.006923187\n",
      "Epoch: 7000 train loss=0.005970008 valid loss= 0.006636498\n",
      "Epoch: 7500 train loss=0.006856187 valid loss= 0.006458576\n",
      "Epoch: 8000 train loss=0.005902632 valid loss= 0.006560769\n",
      "Epoch: 8500 train loss=0.007974441 valid loss= 0.006638922\n",
      "Epoch: 9000 train loss=0.008863702 valid loss= 0.006561901\n",
      "Epoch: 9500 train loss=0.005945348 valid loss= 0.006603510\n",
      "Epoch: 10000 train loss=0.006332841 valid loss= 0.006416888\n",
      "Epoch: 10500 train loss=0.009405512 valid loss= 0.006551592\n",
      "Epoch: 11000 train loss=0.008978868 valid loss= 0.006338536\n",
      "Epoch: 11500 train loss=0.005827399 valid loss= 0.006543742\n",
      "Epoch: 12000 train loss=0.006450436 valid loss= 0.006449670\n",
      "Epoch: 12500 train loss=0.009822886 valid loss= 0.006374580\n",
      "Epoch: 13000 train loss=0.006300434 valid loss= 0.006443182\n",
      "Epoch: 13500 train loss=0.006988141 valid loss= 0.006533330\n",
      "Epoch: 14000 train loss=0.006827815 valid loss= 0.006631757\n",
      "Epoch: 14500 train loss=0.005601351 valid loss= 0.006619926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:51:19,102]\u001b[0m Trial 75 finished with value: 0.0017165983181334104 and parameters: {'lam': 0.008041735331833095, 'learning_rate': 0.18544628234961444, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.007659880 valid loss= 0.006488909\n",
      "Optimization Finished!\n",
      "test loss: 0.006586645729839802, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0017165983181334104\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.010089554 valid loss= 0.012886076\n",
      "Epoch: 1000 train loss=0.008359481 valid loss= 0.009962970\n",
      "Epoch: 1500 train loss=0.007744920 valid loss= 0.009534670\n",
      "Epoch: 2000 train loss=0.006984618 valid loss= 0.009110596\n",
      "Epoch: 2500 train loss=0.009366326 valid loss= 0.009112436\n",
      "Epoch: 3000 train loss=0.006079885 valid loss= 0.008485558\n",
      "Epoch: 3500 train loss=0.006125786 valid loss= 0.007477602\n",
      "Epoch: 4000 train loss=0.006637531 valid loss= 0.007573875\n",
      "Epoch: 4500 train loss=0.008020951 valid loss= 0.007246607\n",
      "Epoch: 5000 train loss=0.006446329 valid loss= 0.007207480\n",
      "Epoch: 5500 train loss=0.005412865 valid loss= 0.006819596\n",
      "Epoch: 6000 train loss=0.005429078 valid loss= 0.007495699\n",
      "Epoch: 6500 train loss=0.005482489 valid loss= 0.006953336\n",
      "Epoch: 7000 train loss=0.006383909 valid loss= 0.007198092\n",
      "Epoch: 7500 train loss=0.007448696 valid loss= 0.007001805\n",
      "Epoch: 8000 train loss=0.005906333 valid loss= 0.006934916\n",
      "Epoch: 8500 train loss=0.006944680 valid loss= 0.007137773\n",
      "Epoch: 9000 train loss=0.006633120 valid loss= 0.007196921\n",
      "Epoch: 9500 train loss=0.006560056 valid loss= 0.006915018\n",
      "Epoch: 10000 train loss=0.005006699 valid loss= 0.007490240\n",
      "Epoch: 10500 train loss=0.005300410 valid loss= 0.006589565\n",
      "Epoch: 11000 train loss=0.005065797 valid loss= 0.006887794\n",
      "Epoch: 11500 train loss=0.005523856 valid loss= 0.007218847\n",
      "Epoch: 12000 train loss=0.006085373 valid loss= 0.006926827\n",
      "Epoch: 12500 train loss=0.005757504 valid loss= 0.007411109\n",
      "Epoch: 13000 train loss=0.005937040 valid loss= 0.007244621\n",
      "Epoch: 13500 train loss=0.007916769 valid loss= 0.007114866\n",
      "Epoch: 14000 train loss=0.007480989 valid loss= 0.007076148\n",
      "Epoch: 14500 train loss=0.005309976 valid loss= 0.006777277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:53:46,626]\u001b[0m Trial 76 finished with value: 0.002009108616572666 and parameters: {'lam': 0.008052035272198369, 'learning_rate': 0.1737764855546178, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.010143436 valid loss= 0.006681126\n",
      "Optimization Finished!\n",
      "test loss: 0.006654908414930105, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002009108616572666\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.009102996 valid loss= 0.010956528\n",
      "Epoch: 1000 train loss=0.006020086 valid loss= 0.006819948\n",
      "Epoch: 1500 train loss=0.005002045 valid loss= 0.006277096\n",
      "Epoch: 2000 train loss=0.005847606 valid loss= 0.005839319\n",
      "Epoch: 2500 train loss=0.005061876 valid loss= 0.006622532\n",
      "Epoch: 3000 train loss=0.004502366 valid loss= 0.005571296\n",
      "Epoch: 3500 train loss=0.006801432 valid loss= 0.005299165\n",
      "Epoch: 4000 train loss=0.004109997 valid loss= 0.005438350\n",
      "Epoch: 4500 train loss=0.004127892 valid loss= 0.005626103\n",
      "Epoch: 5000 train loss=0.007273070 valid loss= 0.005154212\n",
      "Epoch: 5500 train loss=0.004035875 valid loss= 0.005855356\n",
      "Epoch: 6000 train loss=0.004356721 valid loss= 0.005216119\n",
      "Epoch: 6500 train loss=0.005185474 valid loss= 0.005530893\n",
      "Epoch: 7000 train loss=0.005985895 valid loss= 0.006283735\n",
      "Epoch: 7500 train loss=0.004838814 valid loss= 0.005281827\n",
      "Epoch: 8000 train loss=0.006351178 valid loss= 0.005266316\n",
      "Epoch: 8500 train loss=0.004164466 valid loss= 0.005464328\n",
      "Epoch: 9000 train loss=0.004662779 valid loss= 0.005491630\n",
      "Epoch: 9500 train loss=0.004140499 valid loss= 0.005074372\n",
      "Epoch: 10000 train loss=0.004438735 valid loss= 0.006152793\n",
      "Epoch: 10500 train loss=0.008740935 valid loss= 0.005385305\n",
      "Epoch: 11000 train loss=0.005265354 valid loss= 0.006286531\n",
      "Epoch: 11500 train loss=0.007848270 valid loss= 0.005008302\n",
      "Epoch: 12000 train loss=0.012088914 valid loss= 0.005830680\n",
      "Epoch: 12500 train loss=0.004737481 valid loss= 0.005537625\n",
      "Epoch: 13000 train loss=0.016368048 valid loss= 0.006326037\n",
      "Epoch: 13500 train loss=0.004223844 valid loss= 0.005284335\n",
      "Epoch: 14000 train loss=0.004616069 valid loss= 0.005332010\n",
      "Epoch: 14500 train loss=0.003884422 valid loss= 0.006565693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:56:11,970]\u001b[0m Trial 77 finished with value: 0.001311144969326686 and parameters: {'lam': 0.006469225438422608, 'learning_rate': 0.18579954068000776, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.004822883 valid loss= 0.005097144\n",
      "Optimization Finished!\n",
      "test loss: 0.004845074377954006, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.001311144969326686\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.009016514 valid loss= 0.011193033\n",
      "Epoch: 1000 train loss=0.009255188 valid loss= 0.007430058\n",
      "Epoch: 1500 train loss=0.005962753 valid loss= 0.006931855\n",
      "Epoch: 2000 train loss=0.006836588 valid loss= 0.006054955\n",
      "Epoch: 2500 train loss=0.004968702 valid loss= 0.005475091\n",
      "Epoch: 3000 train loss=0.009369660 valid loss= 0.006515900\n",
      "Epoch: 3500 train loss=0.005563194 valid loss= 0.006230609\n",
      "Epoch: 4000 train loss=0.004444228 valid loss= 0.006152436\n",
      "Epoch: 4500 train loss=0.007799672 valid loss= 0.006098323\n",
      "Epoch: 5000 train loss=0.008256506 valid loss= 0.005709892\n",
      "Epoch: 5500 train loss=0.005752674 valid loss= 0.005784085\n",
      "Epoch: 6000 train loss=0.005209105 valid loss= 0.006233834\n",
      "Epoch: 6500 train loss=0.004671133 valid loss= 0.005997965\n",
      "Epoch: 7000 train loss=0.004764304 valid loss= 0.005587153\n",
      "Epoch: 7500 train loss=0.006137809 valid loss= 0.005686165\n",
      "Epoch: 8000 train loss=0.006157232 valid loss= 0.005254859\n",
      "Epoch: 8500 train loss=0.011910762 valid loss= 0.005495394\n",
      "Epoch: 9000 train loss=0.005282508 valid loss= 0.006095631\n",
      "Epoch: 9500 train loss=0.004357887 valid loss= 0.006625267\n",
      "Epoch: 10000 train loss=0.005014691 valid loss= 0.005191077\n",
      "Epoch: 10500 train loss=0.005931022 valid loss= 0.006046918\n",
      "Epoch: 11000 train loss=0.005458636 valid loss= 0.005569978\n",
      "Epoch: 11500 train loss=0.012004003 valid loss= 0.005837695\n",
      "Epoch: 12000 train loss=0.006545110 valid loss= 0.005011059\n",
      "Epoch: 12500 train loss=0.004346235 valid loss= 0.005727877\n",
      "Epoch: 13000 train loss=0.005877497 valid loss= 0.005777375\n",
      "Epoch: 13500 train loss=0.005029373 valid loss= 0.006253578\n",
      "Epoch: 14000 train loss=0.004246544 valid loss= 0.005214578\n",
      "Epoch: 14500 train loss=0.005114096 valid loss= 0.006545824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 06:58:39,769]\u001b[0m Trial 78 finished with value: 0.001790247632766677 and parameters: {'lam': 0.006883993255695307, 'learning_rate': 0.1915310602495659, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.007154793 valid loss= 0.005818576\n",
      "Optimization Finished!\n",
      "test loss: 0.005530236288905144, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.001790247632766677\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.012814909 valid loss= 0.009492902\n",
      "Epoch: 1000 train loss=0.008076138 valid loss= 0.007161858\n",
      "Epoch: 1500 train loss=0.011493265 valid loss= 0.006844910\n",
      "Epoch: 2000 train loss=0.005248721 valid loss= 0.005619081\n",
      "Epoch: 2500 train loss=0.004222032 valid loss= 0.005373376\n",
      "Epoch: 3000 train loss=0.004222865 valid loss= 0.005122451\n",
      "Epoch: 3500 train loss=0.005248928 valid loss= 0.005675558\n",
      "Epoch: 4000 train loss=0.004364383 valid loss= 0.005141123\n",
      "Epoch: 4500 train loss=0.009368707 valid loss= 0.005362964\n",
      "Epoch: 5000 train loss=0.005511069 valid loss= 0.004717541\n",
      "Epoch: 5500 train loss=0.005824245 valid loss= 0.005071433\n",
      "Epoch: 6000 train loss=0.004194777 valid loss= 0.005428100\n",
      "Epoch: 6500 train loss=0.005073786 valid loss= 0.004715762\n",
      "Epoch: 7000 train loss=0.003608475 valid loss= 0.005166833\n",
      "Epoch: 7500 train loss=0.009156520 valid loss= 0.005191746\n",
      "Epoch: 8000 train loss=0.003475303 valid loss= 0.005735263\n",
      "Epoch: 8500 train loss=0.004088874 valid loss= 0.004910837\n",
      "Epoch: 9000 train loss=0.005359599 valid loss= 0.005826099\n",
      "Epoch: 9500 train loss=0.003710143 valid loss= 0.005309795\n",
      "Epoch: 10000 train loss=0.007055874 valid loss= 0.005113021\n",
      "Epoch: 10500 train loss=0.003944580 valid loss= 0.004774010\n",
      "Epoch: 11000 train loss=0.003673471 valid loss= 0.006037105\n",
      "Epoch: 11500 train loss=0.008218680 valid loss= 0.005224258\n",
      "Epoch: 12000 train loss=0.009991883 valid loss= 0.005183632\n",
      "Epoch: 12500 train loss=0.004481338 valid loss= 0.005084079\n",
      "Epoch: 13000 train loss=0.003515159 valid loss= 0.005161789\n",
      "Epoch: 13500 train loss=0.005009141 valid loss= 0.005375200\n",
      "Epoch: 14000 train loss=0.006906924 valid loss= 0.005157318\n",
      "Epoch: 14500 train loss=0.003641932 valid loss= 0.004502986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:01:04,608]\u001b[0m Trial 79 finished with value: 0.0011669220779149674 and parameters: {'lam': 0.005915284745564404, 'learning_rate': 0.19911194409304864, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.004492662 valid loss= 0.004633948\n",
      "Optimization Finished!\n",
      "test loss: 0.004403573460876942, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0011669220779149674\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.012128393 valid loss= 0.009076208\n",
      "Epoch: 1000 train loss=0.006242794 valid loss= 0.006102714\n",
      "Epoch: 1500 train loss=0.005303563 valid loss= 0.006128072\n",
      "Epoch: 2000 train loss=0.008000547 valid loss= 0.005231955\n",
      "Epoch: 2500 train loss=0.005245701 valid loss= 0.005423176\n",
      "Epoch: 3000 train loss=0.005064707 valid loss= 0.005005209\n",
      "Epoch: 3500 train loss=0.004898707 valid loss= 0.005227789\n",
      "Epoch: 4000 train loss=0.003970762 valid loss= 0.005567728\n",
      "Epoch: 4500 train loss=0.004969506 valid loss= 0.005368454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:01:55,738]\u001b[0m Trial 80 finished with value: 0.001379934312377619 and parameters: {'lam': 0.0059325178076581764, 'learning_rate': 0.18434662925395076, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.005624139 valid loss= 0.004929530\n",
      "Optimization Finished!\n",
      "test loss: 0.0046597993932664394, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.001379934312377619\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.011254957 valid loss= 0.010328375\n",
      "Epoch: 1000 train loss=0.008277263 valid loss= 0.007915468\n",
      "Epoch: 1500 train loss=0.009376407 valid loss= 0.006922241\n",
      "Epoch: 2000 train loss=0.004535896 valid loss= 0.006830137\n",
      "Epoch: 2500 train loss=0.007197917 valid loss= 0.006538958\n",
      "Epoch: 3000 train loss=0.004394752 valid loss= 0.006189516\n",
      "Epoch: 3500 train loss=0.004152367 valid loss= 0.006132741\n",
      "Epoch: 4000 train loss=0.006468206 valid loss= 0.006361610\n",
      "Epoch: 4500 train loss=0.003980676 valid loss= 0.005926243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:02:45,483]\u001b[0m Trial 81 finished with value: 0.0022539452567637438 and parameters: {'lam': 0.006053437524142427, 'learning_rate': 0.18530394486341328, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.006324369 valid loss= 0.005847973\n",
      "Optimization Finished!\n",
      "test loss: 0.005717835389077663, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0022539452567637438\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.008818139 valid loss= 0.010619754\n",
      "Epoch: 1000 train loss=0.006746195 valid loss= 0.008367287\n",
      "Epoch: 1500 train loss=0.006105197 valid loss= 0.008573355\n",
      "Epoch: 2000 train loss=0.006201796 valid loss= 0.007419192\n",
      "Epoch: 2500 train loss=0.006894770 valid loss= 0.007096624\n",
      "Epoch: 3000 train loss=0.005001712 valid loss= 0.006487431\n",
      "Epoch: 3500 train loss=0.004526467 valid loss= 0.006592149\n",
      "Epoch: 4000 train loss=0.010455484 valid loss= 0.006606582\n",
      "Epoch: 4500 train loss=0.005440354 valid loss= 0.006672814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:03:35,846]\u001b[0m Trial 82 finished with value: 0.001992314768964587 and parameters: {'lam': 0.006817996364071139, 'learning_rate': 0.19691526558454078, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.006858489 valid loss= 0.006026956\n",
      "Optimization Finished!\n",
      "test loss: 0.005919303745031357, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.001992314768964587\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.016020473 valid loss= 0.009704422\n",
      "Epoch: 1000 train loss=0.007656956 valid loss= 0.007637938\n",
      "Epoch: 1500 train loss=0.014529469 valid loss= 0.007732369\n",
      "Epoch: 2000 train loss=0.005934112 valid loss= 0.007416101\n",
      "Epoch: 2500 train loss=0.006224247 valid loss= 0.007802972\n",
      "Epoch: 3000 train loss=0.005456568 valid loss= 0.007522575\n",
      "Epoch: 3500 train loss=0.004390396 valid loss= 0.006746324\n",
      "Epoch: 4000 train loss=0.004749892 valid loss= 0.006264581\n",
      "Epoch: 4500 train loss=0.004181441 valid loss= 0.005998620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:04:26,456]\u001b[0m Trial 83 finished with value: 0.0023787646364500812 and parameters: {'lam': 0.005735229097178802, 'learning_rate': 0.1611635691122705, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.006215950 valid loss= 0.005893175\n",
      "Optimization Finished!\n",
      "test loss: 0.0056124161928892136, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0023787646364500812\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.014989819 valid loss= 0.011104703\n",
      "Epoch: 1000 train loss=0.014749550 valid loss= 0.010533188\n",
      "Epoch: 1500 train loss=0.008583114 valid loss= 0.007615185\n",
      "Epoch: 2000 train loss=0.008814772 valid loss= 0.008379340\n",
      "Epoch: 2500 train loss=0.007210553 valid loss= 0.007993276\n",
      "Epoch: 3000 train loss=0.005274306 valid loss= 0.008148730\n",
      "Epoch: 3500 train loss=0.006959766 valid loss= 0.008696534\n",
      "Epoch: 4000 train loss=0.005850926 valid loss= 0.008366050\n",
      "Epoch: 4500 train loss=0.005202145 valid loss= 0.008219228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:05:16,276]\u001b[0m Trial 84 finished with value: 0.004347529816711191 and parameters: {'lam': 0.006287119638417833, 'learning_rate': 0.13362592399268308, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.006479503 valid loss= 0.008382214\n",
      "Optimization Finished!\n",
      "test loss: 0.008286599069833755, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.004347529816711191\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.014646858 valid loss= 0.011987144\n",
      "Epoch: 1000 train loss=0.008287027 valid loss= 0.011942456\n",
      "Epoch: 1500 train loss=0.010091328 valid loss= 0.010679947\n",
      "Epoch: 2000 train loss=0.009739218 valid loss= 0.010552345\n",
      "Epoch: 2500 train loss=0.013841488 valid loss= 0.009242238\n",
      "Epoch: 3000 train loss=0.006440469 valid loss= 0.009553386\n",
      "Epoch: 3500 train loss=0.006031392 valid loss= 0.009204663\n",
      "Epoch: 4000 train loss=0.007556177 valid loss= 0.008824460\n",
      "Epoch: 4500 train loss=0.005975901 valid loss= 0.008817403\n",
      "Epoch: 5000 train loss=0.006182550 valid loss= 0.008592260\n",
      "Epoch: 5500 train loss=0.005779938 valid loss= 0.008257226\n",
      "Epoch: 6000 train loss=0.005090932 valid loss= 0.008444475\n",
      "Epoch: 6500 train loss=0.006896635 valid loss= 0.008432632\n",
      "Epoch: 7000 train loss=0.005836369 valid loss= 0.008653000\n",
      "Epoch: 7500 train loss=0.006749622 valid loss= 0.008794446\n",
      "Epoch: 8000 train loss=0.005613070 valid loss= 0.008489111\n",
      "Epoch: 8500 train loss=0.004429078 valid loss= 0.008390287\n",
      "Epoch: 9000 train loss=0.004755682 valid loss= 0.008512422\n",
      "Epoch: 9500 train loss=0.005813519 valid loss= 0.008437990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:06:54,214]\u001b[0m Trial 85 finished with value: 0.004665164215827019 and parameters: {'lam': 0.0069666868645857725, 'learning_rate': 0.14658571749416474, 'num_epoch': 10000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10000 train loss=0.005859616 valid loss= 0.008587519\n",
      "Optimization Finished!\n",
      "test loss: 0.008584218099713326, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.004665164215827019\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.007767190 valid loss= 0.007316678\n",
      "Epoch: 1000 train loss=0.010198090 valid loss= 0.006096181\n",
      "Epoch: 1500 train loss=0.007701915 valid loss= 0.006249130\n",
      "Epoch: 2000 train loss=0.004622062 valid loss= 0.005356880\n",
      "Epoch: 2500 train loss=0.010411445 valid loss= 0.005765031\n",
      "Epoch: 3000 train loss=0.006875029 valid loss= 0.005277532\n",
      "Epoch: 3500 train loss=0.006029812 valid loss= 0.004691410\n",
      "Epoch: 4000 train loss=0.005132696 valid loss= 0.004825619\n",
      "Epoch: 4500 train loss=0.006482591 valid loss= 0.004808303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:07:44,463]\u001b[0m Trial 86 finished with value: 0.001434986977846542 and parameters: {'lam': 0.005135323162760071, 'learning_rate': 0.18394568387543903, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.004348326 valid loss= 0.004606180\n",
      "Optimization Finished!\n",
      "test loss: 0.005167755763977766, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.001434986977846542\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.030676221 valid loss= 0.010505902\n",
      "Epoch: 1000 train loss=0.009766115 valid loss= 0.008588985\n",
      "Epoch: 1500 train loss=0.011325133 valid loss= 0.007086378\n",
      "Epoch: 2000 train loss=0.007295969 valid loss= 0.006949852\n",
      "Epoch: 2500 train loss=0.005888562 valid loss= 0.006774436\n",
      "Epoch: 3000 train loss=0.004057644 valid loss= 0.006685138\n",
      "Epoch: 3500 train loss=0.006715825 valid loss= 0.006730897\n",
      "Epoch: 4000 train loss=0.003978494 valid loss= 0.006434988\n",
      "Epoch: 4500 train loss=0.005406730 valid loss= 0.006570162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:08:34,554]\u001b[0m Trial 87 finished with value: 0.0037596745668642437 and parameters: {'lam': 0.005014896409971338, 'learning_rate': 0.1814940765289153, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.003716247 valid loss= 0.007019817\n",
      "Optimization Finished!\n",
      "test loss: 0.007216384634375572, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0037596745668642437\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.005957491 valid loss= 0.007883917\n",
      "Epoch: 1000 train loss=0.014720267 valid loss= 0.006854056\n",
      "Epoch: 1500 train loss=0.007927459 valid loss= 0.006759821\n",
      "Epoch: 2000 train loss=0.005310502 valid loss= 0.006631494\n",
      "Epoch: 2500 train loss=0.004948631 valid loss= 0.006331029\n",
      "Epoch: 3000 train loss=0.005880046 valid loss= 0.005753187\n",
      "Epoch: 3500 train loss=0.004719664 valid loss= 0.005902620\n",
      "Epoch: 4000 train loss=0.009828640 valid loss= 0.005410349\n",
      "Epoch: 4500 train loss=0.009361291 valid loss= 0.005820189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:09:25,051]\u001b[0m Trial 88 finished with value: 0.001696628384200014 and parameters: {'lam': 0.005574591059718987, 'learning_rate': 0.1214129031022066, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.004395685 valid loss= 0.005368474\n",
      "Optimization Finished!\n",
      "test loss: 0.005639370996505022, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.001696628384200014\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.014891846 valid loss= 0.011650177\n",
      "Epoch: 1000 train loss=0.014396472 valid loss= 0.011859706\n",
      "Epoch: 1500 train loss=0.006712379 valid loss= 0.010390717\n",
      "Epoch: 2000 train loss=0.007544285 valid loss= 0.009449581\n",
      "Epoch: 2500 train loss=0.007981822 valid loss= 0.008393479\n",
      "Epoch: 3000 train loss=0.005817982 valid loss= 0.007939023\n",
      "Epoch: 3500 train loss=0.009930221 valid loss= 0.007272764\n",
      "Epoch: 4000 train loss=0.006902291 valid loss= 0.006949305\n",
      "Epoch: 4500 train loss=0.005753077 valid loss= 0.006905589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:10:15,705]\u001b[0m Trial 89 finished with value: 0.002960702119200952 and parameters: {'lam': 0.005523773297959285, 'learning_rate': 0.11409950185216679, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.005609633 valid loss= 0.006875154\n",
      "Optimization Finished!\n",
      "test loss: 0.00682917470112443, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002960702119200952\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.018331360 valid loss= 0.010581698\n",
      "Epoch: 1000 train loss=0.010190207 valid loss= 0.009747466\n",
      "Epoch: 1500 train loss=0.006661917 valid loss= 0.006399823\n",
      "Epoch: 2000 train loss=0.012761044 valid loss= 0.006608192\n",
      "Epoch: 2500 train loss=0.006181144 valid loss= 0.007273369\n",
      "Epoch: 3000 train loss=0.004432309 valid loss= 0.006797813\n",
      "Epoch: 3500 train loss=0.004053493 valid loss= 0.006421137\n",
      "Epoch: 4000 train loss=0.003911212 valid loss= 0.005906037\n",
      "Epoch: 4500 train loss=0.007380096 valid loss= 0.005882925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:11:06,225]\u001b[0m Trial 90 finished with value: 0.002792202221877653 and parameters: {'lam': 0.0047401496831165225, 'learning_rate': 0.10086173315688049, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.003610853 valid loss= 0.005769498\n",
      "Optimization Finished!\n",
      "test loss: 0.005462110973894596, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002792202221877653\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.014301128 valid loss= 0.010345394\n",
      "Epoch: 1000 train loss=0.009279536 valid loss= 0.009698898\n",
      "Epoch: 1500 train loss=0.006668920 valid loss= 0.007514711\n",
      "Epoch: 2000 train loss=0.005761732 valid loss= 0.006401455\n",
      "Epoch: 2500 train loss=0.005816021 valid loss= 0.005759495\n",
      "Epoch: 3000 train loss=0.004187134 valid loss= 0.005523742\n",
      "Epoch: 3500 train loss=0.004485791 valid loss= 0.005607564\n",
      "Epoch: 4000 train loss=0.006391225 valid loss= 0.004691453\n",
      "Epoch: 4500 train loss=0.004580920 valid loss= 0.004791900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:11:56,508]\u001b[0m Trial 91 finished with value: 0.0023369716934083856 and parameters: {'lam': 0.0043299755407204266, 'learning_rate': 0.1284692782238994, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.003522913 valid loss= 0.005088365\n",
      "Optimization Finished!\n",
      "test loss: 0.0047304145991802216, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0023369716934083856\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.009003422 valid loss= 0.009459529\n",
      "Epoch: 1000 train loss=0.010256272 valid loss= 0.007291341\n",
      "Epoch: 1500 train loss=0.006417177 valid loss= 0.007354380\n",
      "Epoch: 2000 train loss=0.020006523 valid loss= 0.007805180\n",
      "Epoch: 2500 train loss=0.007783829 valid loss= 0.007637044\n",
      "Epoch: 3000 train loss=0.005356150 valid loss= 0.008768627\n",
      "Epoch: 3500 train loss=0.009096688 valid loss= 0.008109991\n",
      "Epoch: 4000 train loss=0.004794618 valid loss= 0.007792443\n",
      "Epoch: 4500 train loss=0.005864921 valid loss= 0.007351617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:12:47,004]\u001b[0m Trial 92 finished with value: 0.002981048218133418 and parameters: {'lam': 0.005894442644317936, 'learning_rate': 0.08234914293065924, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.006343362 valid loss= 0.006786878\n",
      "Optimization Finished!\n",
      "test loss: 0.006611413322389126, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002981048218133418\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.009565308 valid loss= 0.009086479\n",
      "Epoch: 1000 train loss=0.009503540 valid loss= 0.006800973\n",
      "Epoch: 1500 train loss=0.004569907 valid loss= 0.006833146\n",
      "Epoch: 2000 train loss=0.007244909 valid loss= 0.006151321\n",
      "Epoch: 2500 train loss=0.004451380 valid loss= 0.005610978\n",
      "Epoch: 3000 train loss=0.004972053 valid loss= 0.005571763\n",
      "Epoch: 3500 train loss=0.012544756 valid loss= 0.005828460\n",
      "Epoch: 4000 train loss=0.003862265 valid loss= 0.005000358\n",
      "Epoch: 4500 train loss=0.003447522 valid loss= 0.005360388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:13:36,821]\u001b[0m Trial 93 finished with value: 0.0017723769366719696 and parameters: {'lam': 0.005384559925621399, 'learning_rate': 0.16490413170037846, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.004928570 valid loss= 0.005019670\n",
      "Optimization Finished!\n",
      "test loss: 0.004741974174976349, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0017723769366719696\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.017656101 valid loss= 0.010508139\n",
      "Epoch: 1000 train loss=0.010948459 valid loss= 0.009347891\n",
      "Epoch: 1500 train loss=0.008794337 valid loss= 0.007983236\n",
      "Epoch: 2000 train loss=0.012453229 valid loss= 0.007384034\n",
      "Epoch: 2500 train loss=0.005698647 valid loss= 0.006981124\n",
      "Epoch: 3000 train loss=0.004387898 valid loss= 0.005801679\n",
      "Epoch: 3500 train loss=0.004864720 valid loss= 0.005958812\n",
      "Epoch: 4000 train loss=0.005648240 valid loss= 0.006015568\n",
      "Epoch: 4500 train loss=0.007285274 valid loss= 0.006260590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:14:27,269]\u001b[0m Trial 94 finished with value: 0.0024071091295949604 and parameters: {'lam': 0.005058153376180427, 'learning_rate': 0.15106973214007616, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.004912621 valid loss= 0.005837676\n",
      "Optimization Finished!\n",
      "test loss: 0.006174442358314991, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0024071091295949604\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.008390646 valid loss= 0.010638781\n",
      "Epoch: 1000 train loss=0.012398355 valid loss= 0.007643314\n",
      "Epoch: 1500 train loss=0.006984657 valid loss= 0.007590965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:14:48,610]\u001b[0m Trial 95 finished with value: 0.0036401929826487293 and parameters: {'lam': 0.006105439513005255, 'learning_rate': 0.18334433052886512, 'num_epoch': 2000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 train loss=0.007603064 valid loss= 0.007831826\n",
      "Optimization Finished!\n",
      "test loss: 0.007962355390191078, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0036401929826487293\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.012684477 valid loss= 0.011710388\n",
      "Epoch: 1000 train loss=0.012540082 valid loss= 0.009707819\n",
      "Epoch: 1500 train loss=0.010950323 valid loss= 0.008230237\n",
      "Epoch: 2000 train loss=0.006553858 valid loss= 0.008734883\n",
      "Epoch: 2500 train loss=0.009903961 valid loss= 0.008926261\n",
      "Epoch: 3000 train loss=0.007098150 valid loss= 0.009265535\n",
      "Epoch: 3500 train loss=0.007352842 valid loss= 0.009346302\n",
      "Epoch: 4000 train loss=0.007911263 valid loss= 0.009575550\n",
      "Epoch: 4500 train loss=0.007620945 valid loss= 0.009061941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:15:39,337]\u001b[0m Trial 96 finished with value: 0.004207901457693794 and parameters: {'lam': 0.007525323997460335, 'learning_rate': 0.1223977553663327, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.006527204 valid loss= 0.009043511\n",
      "Optimization Finished!\n",
      "test loss: 0.009185123257339, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.004207901457693794\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.020956237 valid loss= 0.011181543\n",
      "Epoch: 1000 train loss=0.016140454 valid loss= 0.011292567\n",
      "Epoch: 1500 train loss=0.011282347 valid loss= 0.009096956\n",
      "Epoch: 2000 train loss=0.006978329 valid loss= 0.008202051\n",
      "Epoch: 2500 train loss=0.007512579 valid loss= 0.007814507\n",
      "Epoch: 3000 train loss=0.013021190 valid loss= 0.007171492\n",
      "Epoch: 3500 train loss=0.007811570 valid loss= 0.007538255\n",
      "Epoch: 4000 train loss=0.012361119 valid loss= 0.007400236\n",
      "Epoch: 4500 train loss=0.007848646 valid loss= 0.007293096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:16:29,432]\u001b[0m Trial 97 finished with value: 0.002192423305957612 and parameters: {'lam': 0.006641577678437236, 'learning_rate': 0.09212691425452713, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.007069642 valid loss= 0.006992965\n",
      "Optimization Finished!\n",
      "test loss: 0.007214725017547607, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.002192423305957612\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.016970264 valid loss= 0.011354893\n",
      "Epoch: 1000 train loss=0.008994584 valid loss= 0.008751148\n",
      "Epoch: 1500 train loss=0.006855155 valid loss= 0.008547335\n",
      "Epoch: 2000 train loss=0.007180161 valid loss= 0.007538665\n",
      "Epoch: 2500 train loss=0.007068157 valid loss= 0.006443588\n",
      "Epoch: 3000 train loss=0.010586872 valid loss= 0.006528873\n",
      "Epoch: 3500 train loss=0.007805925 valid loss= 0.006432569\n",
      "Epoch: 4000 train loss=0.006557568 valid loss= 0.006344850\n",
      "Epoch: 4500 train loss=0.006050133 valid loss= 0.006265748\n",
      "Epoch: 5000 train loss=0.005786521 valid loss= 0.006958898\n",
      "Epoch: 5500 train loss=0.012165094 valid loss= 0.006091699\n",
      "Epoch: 6000 train loss=0.006863004 valid loss= 0.006379384\n",
      "Epoch: 6500 train loss=0.006210754 valid loss= 0.006041808\n",
      "Epoch: 7000 train loss=0.007336610 valid loss= 0.006007978\n",
      "Epoch: 7500 train loss=0.005500030 valid loss= 0.006600279\n",
      "Epoch: 8000 train loss=0.006692689 valid loss= 0.006328636\n",
      "Epoch: 8500 train loss=0.004918803 valid loss= 0.007102432\n",
      "Epoch: 9000 train loss=0.006966460 valid loss= 0.006514408\n",
      "Epoch: 9500 train loss=0.008512254 valid loss= 0.006509521\n",
      "Epoch: 10000 train loss=0.005078856 valid loss= 0.007239670\n",
      "Epoch: 10500 train loss=0.004935008 valid loss= 0.006731325\n",
      "Epoch: 11000 train loss=0.009677297 valid loss= 0.007029382\n",
      "Epoch: 11500 train loss=0.004847495 valid loss= 0.006094645\n",
      "Epoch: 12000 train loss=0.007038584 valid loss= 0.006795278\n",
      "Epoch: 12500 train loss=0.006626995 valid loss= 0.006693141\n",
      "Epoch: 13000 train loss=0.005144386 valid loss= 0.006380375\n",
      "Epoch: 13500 train loss=0.008339898 valid loss= 0.006603878\n",
      "Epoch: 14000 train loss=0.005716809 valid loss= 0.006506077\n",
      "Epoch: 14500 train loss=0.004822562 valid loss= 0.006641172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:18:54,929]\u001b[0m Trial 98 finished with value: 0.0022816646640611065 and parameters: {'lam': 0.008250630830642171, 'learning_rate': 0.13861933396737622, 'num_epoch': 15000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15000 train loss=0.005804750 valid loss= 0.007121581\n",
      "Optimization Finished!\n",
      "test loss: 0.006892329081892967, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0022816646640611065\n",
      "num_samples : 10\n",
      "Epoch: 500 train loss=0.011449064 valid loss= 0.010548988\n",
      "Epoch: 1000 train loss=0.009004927 valid loss= 0.009869389\n",
      "Epoch: 1500 train loss=0.014692801 valid loss= 0.009769997\n",
      "Epoch: 2000 train loss=0.008399087 valid loss= 0.009499892\n",
      "Epoch: 2500 train loss=0.006676926 valid loss= 0.009334725\n",
      "Epoch: 3000 train loss=0.007943718 valid loss= 0.009306790\n",
      "Epoch: 3500 train loss=0.009524919 valid loss= 0.009000801\n",
      "Epoch: 4000 train loss=0.006687374 valid loss= 0.009081498\n",
      "Epoch: 4500 train loss=0.006743739 valid loss= 0.009090488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-27 07:19:45,941]\u001b[0m Trial 99 finished with value: 0.0042613618650357505 and parameters: {'lam': 0.00762622844568, 'learning_rate': 0.15879109814136286, 'num_epoch': 5000}. Best is trial 20 with value: 0.0002924818646676488.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 train loss=0.006509603 valid loss= 0.008909993\n",
      "Optimization Finished!\n",
      "test loss: 0.00870593637228012, test acc: 1.0\n",
      "In trial:---------------------\n",
      "validation mse: 0.0042613618650357505\n"
     ]
    }
   ],
   "source": [
    "# optimize the model via Optuna and obtain the best model with smallest validation mse\n",
    "best_model = None\n",
    "model = None\n",
    "study = optuna.create_study(pruner=None)\n",
    "study.optimize(llspin_objective, n_trials=100, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training gate matrix\n",
    "gate_mat_train = best_model.get_prob_alpha(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = study.best_params['learning_rate']\n",
    "best_epoch = study.best_params['num_epoch']\n",
    "best_lam = study.best_params['lam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial Finished*************\n",
      "Best model's lambda: 0.002292293155230627\n",
      "Best model's learning rate: 0.09214842664633631\n",
      "Best model's num of epochs: 15000\n",
      "Test mse : 0.0002557289058882526\n",
      "Test r2 : 0.9938291863184201\n"
     ]
    }
   ],
   "source": [
    "# test the best model\n",
    "y_pred_llspin = best_model.test(X_test)[0]\n",
    "            \n",
    "print(\"Trial Finished*************\")\n",
    "print(\"Best model's lambda: {}\".format(best_lam))\n",
    "print(\"Best model's learning rate: {}\".format(best_lr))\n",
    "print(\"Best model's num of epochs: {}\".format(best_epoch))\n",
    "print(\"Test mse : {}\".format(mean_squared_error(y_test.reshape(-1),y_pred_llspin.reshape(-1))))\n",
    "print(\"Test r2 : {}\".format(r2_score(y_test.reshape(-1),y_pred_llspin.reshape(-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the training gates to the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = cm.Blues\n",
    "bounds=[0,0.5,1]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "title_size = 30\n",
    "xtick_size = 20\n",
    "ytick_size = 20\n",
    "xlabel_size = 35\n",
    "ylabel_size = 35\n",
    "colorbar_tick_size = 20\n",
    "title_pad = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2,sharex=False, sharey=True,figsize=(8, 6))\n",
    "\n",
    "sorted_order = np.concatenate((np.where(train_label == 1)[0],np.where(train_label == 2)[0]))\n",
    "\n",
    "im1 = axes[0].imshow(ref_feat_mat_train[sorted_order,:].astype(int),aspect='auto',cmap=cmap, norm=norm)\n",
    "axes[0].set_title(\"Ground Truth\",fontsize=title_size,fontweight=\"bold\",pad=title_pad)\n",
    "axes[0].set_ylabel(\"Sample Index\",fontsize=ylabel_size)\n",
    "axes[0].set_yticks([1,3,5,7,9])\n",
    "axes[0].set_yticklabels([2,4,6,8,10],fontsize=ytick_size)\n",
    "axes[0].set_xticks(list(range(5)))\n",
    "axes[0].set_xticklabels(list(range(1,6)),fontsize=xtick_size)\n",
    "axes[0].set_xlabel(\"Feature Index\",fontsize=xlabel_size,labelpad=-5)\n",
    "\n",
    "cbar = fig.colorbar(im1,ax=axes[0], cmap=cmap, norm=norm, boundaries=bounds, ticks=[0, 1])\n",
    "cbar.ax.tick_params(labelsize=colorbar_tick_size)\n",
    "\n",
    "im2 = axes[1].imshow(gate_mat_train[sorted_order,:],aspect='auto',cmap=cmap)\n",
    "axes[1].set_title(\"LLSPIN Gates\",fontsize=title_size,fontweight=\"bold\",pad=title_pad)\n",
    "axes[1].set_yticks([1,3,5,7,9])\n",
    "axes[1].set_yticklabels([2,4,6,8,10],fontsize=ytick_size)\n",
    "axes[1].set_xticks(list(range(5)))\n",
    "axes[1].set_xticklabels(list(range(1,6)),fontsize=xtick_size)\n",
    "axes[1].set_xlabel(\"Feature Index\",fontsize=xlabel_size,labelpad=-5)\n",
    "\n",
    "cbar = fig.colorbar(im2,ax=axes[1])\n",
    "cbar.ax.tick_params(labelsize=colorbar_tick_size)\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
